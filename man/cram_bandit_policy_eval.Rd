% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/armed_bandit_delta.R
\name{cram_bandit_policy_eval}
\alias{cram_bandit_policy_eval}
\title{Cramming Policy Evaluation for Multi-Armed Bandit}
\usage{
cram_bandit_policy_eval(policy_diff, reward, T)
}
\arguments{
\item{policy_diff}{A matrix where each entry represents the difference in policies
between iterations, i.e., \eqn{\pi_t(X, a) - \pi_{t-1}(X, a)} for each t.}

\item{reward}{A matrix of observed rewards corresponding to each action and time step.}

\item{T}{The total number of iterations in the bandit process.}
}
\value{
The estimated policy value difference \eqn{\Delta(\pi_T; \pi_0)}.
}
\description{
This function implements the armed bandit policy evaluation formula for
estimating \eqn{\Delta(\pi_T; \pi_0)} as given in the user-provided formula.
}
\examples{
# Example usage
set.seed(123)
T <- 10
K <- 5  # Number of actions

# Simulate policy differences and rewards
policy_diff <- matrix(runif(T * K, -0.5, 0.5), nrow = T, ncol = K)
reward <- matrix(rnorm(T * K), nrow = T, ncol = K)

cram_bandit_policy_eval(policy_diff, reward, T)
}

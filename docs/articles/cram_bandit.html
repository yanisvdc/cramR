<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Cram Bandit • cramR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Cram Bandit">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<!-- Load Inter font --><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&amp;display=swap" rel="stylesheet">
<style>
  :root {
    --primary-gradient: linear-gradient(135deg, #2c3e50, #3498db);
    --hover-shadow: 0 4px 15px rgba(0,0,0,0.2);
  }

  .navbar {
    background-color: rgba(255, 255, 255, 0.95); /* Light background with slight transparency */
    box-shadow: 0 2px 10px rgba(0,0,0,0.1); /* Optional shadow for depth */
  }

  body {
    padding-top: 80px;
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  }

  h1, h2, h3, h4, h5, h6 {
    font-family: 'Inter', sans-serif;
  }

  code, pre, kbd {
    font-family: 'JetBrains Mono', monospace;
    background-color: #f5f5f5;
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-size: 90%;
  }

  pre {
    overflow-x: auto;
  }

  pre code {
    padding: 1em;
    display: block;
  }

  .navbar-brand img {
    height: 40px;
    transition: transform 0.3s ease;
  }

  .navbar-brand:hover img {
    transform: scale(1.05);
  }

  .hero-banner {
    background: var(--primary-gradient);
    padding: 6rem 1rem;
    color: white;
    text-shadow: 0 2px 4px rgba(0,0,0,0.2);
  }

  .feature-card {
    background: white;
    border-radius: 12px;
    padding: 2rem;
    margin: 1rem;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    transition: all 0.3s ease;
  }

  .feature-card:hover {
    transform: translateY(-5px);
    box-shadow: var(--hover-shadow);
  }

  /* Add hover effect to make the navbar more readable */
  .navbar:hover {
    background-color: rgba(255, 255, 255, 1); /* Fully visible on hover */
  }

  /* Add space below the navbar to prevent cutting off content */
  main {
    padding-top: 1rem;
  }
</style>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cramR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/cram_policy_part_1.html">Introduction &amp; Cram Policy part 1</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html">Quick Start</a></li>
    <li><a class="dropdown-item" href="../articles/cram_policy_part_2.html">Cram Policy part 2</a></li>
    <li><a class="dropdown-item" href="../articles/cram_ml.html">Cram ML</a></li>
    <li><a class="dropdown-item" href="../articles/cram_bandit.html">Cram Bandit</a></li>
    <li><a class="dropdown-item" href="../articles/cram_bandit_helpers.html">Cram Bandit Helpers</a></li>
    <li><a class="dropdown-item" href="../articles/cram_policy_simulation.html">Cram Policy Simulation</a></li>
    <li><a class="dropdown-item" href="../articles/cram_bandit_simulation.html">Cram Bandit Simulation</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-resources" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Resources</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-resources">
<li><a class="dropdown-item" href="../reference/index.html">Function Reference</a></li>
    <li><a class="dropdown-item" href="../news/index.html">Release Notes</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/yanisvdc/cramR" aria-label="GitHub repository"><span class="fa fa-brands fa-github"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf" aria-label="Research paper"><span class="fa fa-solid fa-file-pdf"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<link href="cram_bandit_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="cram_bandit_files/htmlwidgets-1.6.4/htmlwidgets.js"></script><link href="cram_bandit_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="cram_bandit_files/datatables-binding-0.33/datatables.js"></script><link href="cram_bandit_files/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="cram_bandit_files/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="cram_bandit_files/dt-core-1.13.6/js/jquery.dataTables.min.js"></script><link href="cram_bandit_files/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="cram_bandit_files/crosstalk-1.2.1/js/crosstalk.min.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Cram Bandit</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/yanisvdc/cramR/blob/HEAD/vignettes/cram_bandit.Rmd" class="external-link"><code>vignettes/cram_bandit.Rmd</code></a></small>
      <div class="d-none name"><code>cram_bandit.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="what-is-cram_bandit">What is <code>cram_bandit()</code>?<a class="anchor" aria-label="anchor" href="#what-is-cram_bandit"></a>
</h2>
<p>The <code><a href="../reference/cram_bandit.html">cram_bandit()</a></code> function implements the Cram
methodology for <strong>on-policy statistical evaluation</strong> of
contextual bandit algorithms. Unlike traditional off-policy approaches,
Cram uses the <em>same adaptively collected data</em> for both learning
and evaluation, delivering more <strong>efficient, consistent</strong>,
and <strong>asymptotically normal</strong> policy value estimates.</p>
</div>
<div class="section level2">
<h2 id="introduction-bandits-policies-and-cram">Introduction: Bandits, Policies, and Cram<a class="anchor" aria-label="anchor" href="#introduction-bandits-policies-and-cram"></a>
</h2>
<p>In many machine learning settings, decisions must be made
sequentially under uncertainty — for instance, recommending content,
personalizing treatments, or allocating resources. These problems are
often modeled as <strong>contextual bandits</strong>, where an
agent:</p>
<ol style="list-style-type: decimal">
<li>Observes context (features of the situation)</li>
<li>Chooses an action (e.g., recommend an article)</li>
<li>Observes a reward (e.g., targeted user clicks on the article or
not)</li>
</ol>
<p>A <strong>policy</strong> is a function that maps context to a
probability distribution over actions, with the goal of
<strong>maximizing expected cumulative reward over time</strong>.
Learning an optimal policy and evaluating its performance using the same
data is difficult due to the adaptive nature of the data collection.</p>
<p>This challenge becomes evident when comparing to supervised learning:
in supervised learning, the outcome or label
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is observed for every input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>,
allowing direct minimization of prediction error. In contrast, in a
bandit setting, the outcome (reward) is only observed for the single
action chosen by the agent. The agent must therefore select an action in
order to reveal the reward associated with it, making data collection
and learning inherently intertwined.</p>
<p>The <strong>Cram</strong> method addresses this as being a general
statistical framework for evaluating the final learned policy from a
multi-armed contextual bandit algorithm, using the dataset generated by
the same bandit algorithm. Notably, Cram is able to leverage this
setting to return an estimate of how well the learned policy would
perform <em>if deployed on the entire population</em> (policy value),
along with a confidence interval at desired significance level.</p>
</div>
<div class="section level2">
<h2 id="understanding-the-inputs">Understanding the inputs<a class="anchor" aria-label="anchor" href="#understanding-the-inputs"></a>
</h2>
<p>Many contextual bandit algorithms update their policies every few
rounds instead of at every step — this is known as the <strong>batched
setting</strong>. For example, if the batch size is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">B = 5</annotation></semantics></math>,
the algorithm collects 5 new samples before updating its policy. This
results in a sequence of policies
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>π</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover><mi>π</mi><mo accent="true">̂</mo></mover><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mover><mi>π</mi><mo accent="true">̂</mo></mover><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\pi}_1, \hat{\pi}_2, ..., \hat{\pi}_T</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
is the number of batches.</p>
<p>In total, we observe
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">T \times B</annotation></semantics></math>
data points, each consisting of:</p>
<ul>
<li><p>A context</p></li>
<li><p>An action selected by the policy active at the time</p></li>
<li><p>A reward</p></li>
</ul>
<p>Cram supports the batched setting of bandit algorithms to allow for
flexible use. Note that one can still set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">B = 1</annotation></semantics></math>
if performing policy updates after each observation.</p>
<p>Thus, Cram bandit takes as inputs:</p>
<ul>
<li>
<p><code>pi</code>: An array of shape <strong>(T × B, T, K)</strong>
or <strong>(T × B, T)</strong>, where:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
is the number of learning steps (or policy updates)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
is the batch size</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
is the number of arms</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">T \times B</annotation></semantics></math>
is the total number of contexts</p></li>
<li><p>In the natural 3D version, <code>pi[j, t, a]</code> gives the
probability that the policy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>π</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\hat{\pi}_t</annotation></semantics></math>
assigns arm <code>a</code> to context
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math></p></li>
<li><p>Users may still use the 2D version as internally, we actually
only need the probabilities assigned to the <strong>chosen arm</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>j</mi></msub><annotation encoding="application/x-tex">A_j</annotation></semantics></math>
for each context
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>
in the historical data - and not the probabilities for all of the arms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
under each context
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>,
which allows us to remove the last dimension (“arm dimension”) of the 3D
array. In other words, this compact form omits the full distribution
over arms and assumes you are only providing the realized action
probabilities.</p></li>
</ul>
<blockquote>
<p>🛠️ If you need to compute this probability array from a trained
policy or historical data, the <code>cramR</code> package provides
helper utilities in the <code>cramR:::</code> namespace (see “Bandit
Helpers” vignette). Note that the exact method may depend on how your
bandit logs and models are structured.</p>
</blockquote>
</li>
<li><p><code>arm</code>: A vector of length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">T \times B</annotation></semantics></math>
indicating which arm was selected in each context.</p></li>
<li><p><code>reward</code>: A vector of observed rewards of length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">T \times B</annotation></semantics></math>.</p></li>
<li><p><code>batch</code>: (optional) Integer batch size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
Default is 1.</p></li>
<li><p><code>alpha</code>: Significance level for confidence
intervals.</p></li>
</ul>
<p>Cram bandit returns:</p>
<ul>
<li><p>Estimated policy value</p></li>
<li><p>Estimated standard error</p></li>
<li><p>Confidence interval at level alpha</p></li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="example-use-cram_bandit-on-simulated-data-with-batch-size-of-1">Example: Use <code>cram_bandit()</code> on simulated data with batch
size of 1<a class="anchor" aria-label="anchor" href="#example-use-cram_bandit-on-simulated-data-with-batch-size-of-1"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set random seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define parameters</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">100</span>  <span class="co"># Number of timesteps</span></span>
<span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fl">4</span>    <span class="co"># Number of arms</span></span>
<span></span>
<span><span class="co"># Simulate a 3D array `pi` of shape (T, T, K)</span></span>
<span><span class="co"># - First dimension: Individuals (context Xj)</span></span>
<span><span class="co"># - Second dimension: Time steps (pi_t)</span></span>
<span><span class="co"># - Third dimension: Arms (depth)</span></span>
<span><span class="va">pi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">array</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="cn">T</span> <span class="op">*</span> <span class="cn">T</span> <span class="op">*</span> <span class="va">K</span>, <span class="fl">0.1</span>, <span class="fl">1</span><span class="op">)</span>, dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">T</span>, <span class="cn">T</span>, <span class="va">K</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Normalize probabilities so that each row sums to 1 across arms</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="cn">T</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="cn">T</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">t</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">t</span>, <span class="op">]</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">pi</span><span class="op">[</span><span class="va">j</span>, <span class="va">t</span>, <span class="op">]</span><span class="op">)</span>  </span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Simulate arm selections (randomly choosing an arm)</span></span>
<span><span class="va">arm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">K</span>, <span class="cn">T</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate rewards (assume normally distributed rewards)</span></span>
<span><span class="va">reward</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="cn">T</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span></code></pre></div>
<hr>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cram_bandit.html">cram_bandit</a></span><span class="op">(</span><span class="va">pi</span>, <span class="va">arm</span>, <span class="va">reward</span>, batch<span class="op">=</span><span class="fl">1</span>, alpha<span class="op">=</span><span class="fl">0.05</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span><span class="op">$</span><span class="va">raw_results</span></span>
<span><span class="co">#&gt;                        Metric   Value</span></span>
<span><span class="co">#&gt; 1       Policy Value Estimate 0.67621</span></span>
<span><span class="co">#&gt; 2 Policy Value Standard Error 0.04394</span></span>
<span><span class="co">#&gt; 3       Policy Value CI Lower 0.59008</span></span>
<span><span class="co">#&gt; 4       Policy Value CI Upper 0.76234</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span><span class="op">$</span><span class="va">interactive_table</span></span></code></pre></div>
<div class="datatables html-widget html-fill-item" id="htmlwidget-ac96cb3ee4656e2e9ec3" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-ac96cb3ee4656e2e9ec3">{"x":{"filter":"none","vertical":false,"caption":"<caption>CRAM Bandit Policy Evaluation Results<\/caption>","data":[["1","2","3","4"],["Policy Value Estimate","Policy Value Standard Error","Policy Value CI Lower","Policy Value CI Upper"],[0.67621,0.04394,0.59008,0.76234]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Metric<\/th>\n      <th>Value<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0},{"name":" ","targets":0},{"name":"Metric","targets":1},{"name":"Value","targets":2}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script><hr>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li>Jia, Z., Imai, K., &amp; Li, M. L. (2024). The Cram Method for
Efficient Simultaneous Learning and Evaluation. arXiv preprint
arXiv:2403.07031.</li>
<li>Zhan et al. (2021). <em>Off-policy evaluation via adaptive weighting
with data from contextual bandits.</em> KDD.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Yanis Vandecasteele, Michael Lingzhi Li, Kosuke Imai, Zeyang Jia, Longlin Wang.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>

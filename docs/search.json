[{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"what-is-cram_bandit","dir":"Articles","previous_headings":"","what":"What is cram_bandit()?","title":"Cram Bandit","text":"cram_bandit() function implements Cram methodology -policy statistical evaluation contextual bandit algorithms. Unlike traditional -policy approaches, Cram uses adaptively collected data learning evaluation, delivering efficient, consistent, asymptotically normal policy value estimates.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"introduction-bandits-policies-and-cram","dir":"Articles","previous_headings":"","what":"Introduction: Bandits, Policies, and Cram","title":"Cram Bandit","text":"many machine learning settings, decisions must made sequentially uncertainty ‚Äî instance, recommending content, personalizing treatments, allocating resources. problems often modeled contextual bandits, agent: Observes context (features situation) Chooses action (e.g., recommend article) Observes reward (e.g., targeted user clicks article ) policy function maps context probability distribution actions, goal maximizing expected cumulative reward time. Learning optimal policy evaluating performance using data difficult due adaptive nature data collection. challenge becomes evident comparing supervised learning: supervised learning, outcome label yy observed every input xx, allowing direct minimization prediction error. contrast, bandit setting, outcome (reward) observed single action chosen agent. agent must therefore select action order reveal reward associated , making data collection learning inherently intertwined. Cram method addresses general statistical framework evaluating final learned policy multi-armed contextual bandit algorithm, using dataset generated bandit algorithm. Notably, Cram able leverage setting return estimate well learned policy perform deployed entire population (policy value), along confidence interval desired significance level.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"understanding-the-inputs","dir":"Articles","previous_headings":"","what":"Understanding the inputs","title":"Cram Bandit","text":"Many contextual bandit algorithms update policies every rounds instead every step ‚Äî known batched setting. example, batch size B=5B = 5, algorithm collects 5 new samples updating policy. results sequence policies œÄÃÇ1,œÄÃÇ2,...,œÄÃÇT\\hat{\\pi}_1, \\hat{\\pi}_2, ..., \\hat{\\pi}_T, TT number batches. total, observe T√óBT \\times B data points, consisting : context action selected policy active time reward Cram supports batched setting bandit algorithms allow flexible use. Note one can still set B=1B = 1 performing policy updates observation. Thus, Cram bandit takes inputs: pi: array shape (T √ó B, T, K) (T √ó B, T), : TT number learning steps (policy updates) BB batch size KK number arms T√óBT \\times B total number contexts natural 3D version, pi[j, t, ] gives probability policy œÄÃÇt\\hat{\\pi}_t assigns arm context XjX_j Users may still use 2D version internally, actually need probabilities assigned chosen arm AjA_j context XjX_j historical data - probabilities arms aa context XjX_j, allows us remove last dimension (‚Äúarm dimension‚Äù) 3D array. words, compact form omits full distribution arms assumes providing realized action probabilities. üõ†Ô∏è need compute probability array trained policy historical data, cramR package provides helper utilities cramR::: namespace (see ‚ÄúBandit Helpers‚Äù vignette). Note exact method may depend bandit logs models structured. arm: vector length T√óBT \\times B indicating arm selected context. reward: vector observed rewards length T√óBT \\times B. batch: (optional) Integer batch size BB. Default 1. alpha: Significance level confidence intervals. Cram bandit returns: Estimated policy value Estimated standard error Confidence interval level alpha","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"example-use-cram_bandit-on-simulated-data-with-batch-size-of-1","dir":"Articles","previous_headings":"","what":"Example: Use cram_bandit() on simulated data with batch size of 1","title":"Cram Bandit","text":"","code":"# Set random seed for reproducibility set.seed(42)  # Define parameters T <- 100  # Number of timesteps K <- 4    # Number of arms  # Simulate a 3D array `pi` of shape (T, T, K) # - First dimension: Individuals (context Xj) # - Second dimension: Time steps (pi_t) # - Third dimension: Arms (depth) pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K))  # Normalize probabilities so that each row sums to 1 across arms for (t in 1:T) {   for (j in 1:T) {     pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])     } }  # Simulate arm selections (randomly choosing an arm) arm <- sample(1:K, T, replace = TRUE)  # Simulate rewards (assume normally distributed rewards) reward <- rnorm(T, mean = 1, sd = 0.5) result <- cram_bandit(pi, arm, reward, batch=1, alpha=0.05) result$raw_results #>                        Metric   Value #> 1       Policy Value Estimate 0.67621 #> 2 Policy Value Standard Error 0.04394 #> 3       Policy Value CI Lower 0.59008 #> 4       Policy Value CI Upper 0.76234 result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Cram Bandit","text":"Jia, Z., Imai, K., & Li, M. L. (2024). Cram Method Efficient Simultaneous Learning Evaluation. arXiv preprint arXiv:2403.07031. Zhan et al.¬†(2021). -policy evaluation via adaptive weighting data contextual bandits. KDD.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"what-is-cram_bandit_sim","dir":"Articles","previous_headings":"","what":"üéØ What is cram_bandit_sim()?","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"function cram_bandit_sim() evaluates contextual bandit algorithms using CRAM method. built top contextual package, several extensions batch-aware evaluation high-precision probability tracking.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"how-it-works","dir":"Articles","previous_headings":"","what":"üß© How it works","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"CRAM contextual bandits relies accurately estimating action probabilities œÄt(Xt,)\\pi_t(X_t, a_t). essential computing unbiased -policy estimators: VÃÇœÄ=1T‚àët=1Trt‚ãÖùüô=œÄt(Xt)œÄt(Xt,) \\hat{V}_\\pi = \\frac{1}{T} \\sum_{t=1}^T \\frac{r_t \\cdot \\mathbb{1}_{a_t = \\pi_t(X_t)}}{\\pi_t(X_t, a_t)} , : Use custom subclasses contextual::Policy contextual::Bandit Store update model parameters (, b, etc.) Reconstruct probability selected arm chosen","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"custom-bandit-contextuallinearbandit","dir":"Articles","previous_headings":"","what":"üß† Custom Bandit: ContextualLinearBandit","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"extend standard linear bandit explicitly expose underlying beta matrix used reward generation. allows us compute true expected rewards compare estimated values.","code":"bandit <- ContextualLinearBandit$new(k = 3, d = 5)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"batch-aware-policies","dir":"Articles","previous_headings":"","what":"üèóÔ∏è Batch-Aware Policies","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"allow CRAM‚Äôs learning-evaluation cycles, implement batch versions common policies: batch policy updates internal parameters every batch_size steps, enabling stable, evaluable policies.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"how-probabilities-are-computed","dir":"Articles","previous_headings":"","what":"üî¢ How Probabilities Are Computed","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"use custom logic reconstruct selection probabilities œÄt(Xt,)\\pi_t(X_t, a_t) algorithm internal model state:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"epsilon-greedy","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ Epsilon-Greedy","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"exploration probability Œµ, probability selecting greedy arm : P(|Xt)=(1‚àíœµ)‚ãÖ1#greedy arms+œµK P(a_t | X_t) = \\left(1 - \\epsilon\\right) \\cdot \\frac{1}{\\#\\text{greedy arms}} + \\frac{\\epsilon}{K} detect arms greedy computing Œ∏k=Ak‚àí1bk\\theta_k = A_k^{-1} b_k evaluating expected rewards.","code":"get_proba_c_eps_greedy(eps, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"linucb-disjoint","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ LinUCB Disjoint","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"LinUCB selects arms adding exploration bonus based uncertainty: P(|Xt)=(1‚àíœµ)‚ãÖ1#best arms+œµK P(a_t | X_t) = \\left(1 - \\epsilon \\right) \\cdot \\frac{1}{\\#\\text{best arms}} + \\frac{\\epsilon}{K} accounts confidence intervals using Œºk+Œ±‚ãÖœÉk\\mu_k + \\alpha \\cdot \\sigma_k. CRAM tracks precisely using:","code":"get_proba_ucb_disjoint(alpha, eps, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"thompson-sampling","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ Thompson Sampling","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"subtle: integrate probability chosen arm outperforms others given posterior uncertainty: P(|Xt)=‚Ñô[Œ∏atTXt>Œ∏kTXt‚àÄk‚â†] P(a_t | X_t) = \\mathbb{P}\\left[\\theta_{a_t}^T X_t > \\theta_k^T X_t \\quad \\forall k \\neq a_t\\right] perform numerical integration multivariate Gaussians using: computationally expensive precise.","code":"get_proba_thompson(sigma, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"estimand-calculation","dir":"Articles","previous_headings":"","what":"üß™ Estimand Calculation","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"policies reconstructed, compute true expected value using independent contexts known reward function: essential evaluate bias coverage CRAM estimators.","code":"compute_estimand(data_group, list_betas, policy, policy_name, batch_size, bandit)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"optimizations","dir":"Articles","previous_headings":"","what":"üî• Optimizations","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"Probabilities vectorized across timesteps batches use Sherman-Morrison updates fast matrix inverses function extract_2d_from_3d() performs efficient 3D slicing isolate selected arm probabilities Confidence intervals calculated using asymptotic variance estimates","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"example","dir":"Articles","previous_headings":"","what":"üì¶ Example","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"","code":"# Set up bandit and policy bandit <- ContextualLinearBandit$new(k = 3, d = 5) policy <- BatchLinUCBDisjointPolicyEpsilon$new(alpha = 1.0, epsilon = 0.1, batch_size = 10)  # Run CRAM Bandit simulation results <- cram_bandit_sim(   horizon = 100,   simulations = 5,   bandit = bandit,   policy = policy,   alpha = 0.05 )  head(results)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"contextual: Bandit simulation framework cram_bandit() ‚Äî compute value/CI one policy trajectory compute_probas() ‚Äî probability matrix reconstruction engine","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"what-is-cram_bandit_sim","dir":"Articles","previous_headings":"","what":"What is cram_bandit_sim()?","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"cram_bandit_sim() function runs -policy simulation contextual bandit algorithms using CRAM method. evaluates statistical properties policy value estimates : Prediction error Variance estimation error Empirical coverage confidence intervals useful benchmarking bandit policies controlled, simulated environments.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"requirements","dir":"Articles","previous_headings":"","what":"Requirements","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"need provide: bandit: contextual bandit environment object (e.g.¬†ContextualLinearBandit) policy: policy object (e.g.¬†BatchContextualLinTSPolicy) horizon: number time steps simulations: number repeated simulations Optional: alpha, seed, do_parallel","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"example-cram-simulation-with-lints-policy","dir":"Articles","previous_headings":"","what":"Example: CRAM Simulation with LinTS Policy","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"","code":"# Setup library(contextual)  # Define a bandit bandit <- ContextualLinearBandit$new(k = 3, d = 5, sigma = 0.1)  # Define a policy policy <- BatchContextualLinTSPolicy$new(v = 0.2, batch_size = 5)  # Run simulation result <- cram_bandit_sim(   horizon = 100,   simulations = 10,   bandit = bandit,   policy = policy,   alpha = 0.05,   do_parallel = FALSE,   seed = 123 )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"bandit-simulation","dir":"Articles","previous_headings":"Example: CRAM Simulation with LinTS Policy","what":"Bandit Simulation","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"","code":"horizon     <- 500L simulations <- 100L k <- 4 d <- 3 list_betas <- cramR:::get_betas(simulations, d, k) bandit     <- cramR:::ContextualLinearBandit$new(k = k, d = d, list_betas = list_betas, sigma = 0.3) policy     <- cramR:::BatchContextualEpsilonGreedyPolicy$new(epsilon = 0.1, batch_size = 5)  sim <- cram_bandit_sim(   horizon, simulations,   bandit, policy,   alpha = 0.05, do_parallel = FALSE ) #> Simulation horizon: 500 #> Number of simulations: 101 #> Number of batches: 1 #> Starting main loop. #> Finished main loop. #> Completed simulation in 0:00:05.240 #> Computing statistics. print(sim) #> $estimates #>        sim   estimate variance_est  estimand prediction_error est_rel_error #>      <int>      <num>        <num>     <num>            <num>         <num> #>   1:     1  0.5017577 0.0089906657 0.5445425    -0.0427847832 -0.0785701446 #>   2:     2  0.6189290 0.0007709980 0.5981170     0.0208120529  0.0347959584 #>   3:     3  0.3455183 0.0574641827 0.5528602    -0.2073419542 -0.3750350368 #>   4:     4  0.5837325 0.0010253244 0.5998785    -0.0161459620 -0.0269153869 #>   5:     5  0.3333935 0.0183438945 0.5228533    -0.1894598043 -0.3623574960 #>   6:     6  0.5137367 0.0013894431 0.4651693     0.0485673660  0.1044079358 #>   7:     7  0.6620854 0.0045512621 0.5694665     0.0926188229  0.1626413806 #>   8:     8  0.6704123 0.0018381022 0.7159943    -0.0455819026 -0.0636623863 #>   9:     9  0.5326382 0.0010390982 0.5572175    -0.0245792402 -0.0441106764 #>  10:    10  0.4698764 0.0013506076 0.5061488    -0.0362724215 -0.0716635533 #>  11:    11  0.6390434 0.0041663962 0.5721729     0.0668704579  0.1168710693 #>  12:    12  0.5954521 0.0010635500 0.5413280     0.0541241073  0.0999839391 #>  13:    13  0.4607073 0.0050745461 0.4598595     0.0008478800  0.0018437808 #>  14:    14  0.4648931 0.0016996746 0.4089102     0.0559828894  0.1369075274 #>  15:    15  0.2174046 0.0243192919 0.4197489    -0.2023442877 -0.4820603573 #>  16:    16  0.4904415 0.0010992578 0.4781373     0.0123042466  0.0257337122 #>  17:    17  0.6654798 0.0014336626 0.5170318     0.1484480006  0.2871158025 #>  18:    18  0.6748404 0.0021942552 0.6778199    -0.0029794314 -0.0043956093 #>  19:    19  0.5387263 0.0076842782 0.5064235     0.0323027620  0.0637860627 #>  20:    20  0.6395227 0.0021974001 0.5643850     0.0751377214  0.1331320401 #>  21:    21  0.5722331 0.0017006120 0.5083031     0.0639300095  0.1257714319 #>  22:    22  0.5370528 0.0037995951 0.5991923    -0.0621395465 -0.1037055134 #>  23:    23  0.6004996 0.0021958896 0.5800736     0.0204259345  0.0352126577 #>  24:    24  0.6083162 0.0050001302 0.6183482    -0.0100319263 -0.0162237508 #>  25:    25  0.5740335 0.0010634455 0.5910484    -0.0170148288 -0.0287875406 #>  26:    26  0.4860106 0.0248606203 0.6403480    -0.1543374015 -0.2410211487 #>  27:    27  0.5536516 0.0023158683 0.5988126    -0.0451609679 -0.0754175342 #>  28:    28  0.5169974 0.0049163531 0.5258604    -0.0088630483 -0.0168543747 #>  29:    29 -2.1191887 9.0771030217 0.5462983    -2.6654870535 -4.8791786797 #>  30:    30  0.7475257 0.0015568325 0.6642620     0.0832637246  0.1253477143 #>  31:    31  0.5767502 0.0013215883 0.5838684    -0.0071182656 -0.0121915573 #>  32:    32  0.4896574 0.0008165182 0.4822683     0.0073890887  0.0153215315 #>  33:    33  0.6719951 0.0014106350 0.6758467    -0.0038516056 -0.0056989338 #>  34:    34  0.4764243 0.0246272110 0.6249639    -0.1485396256 -0.2376771336 #>  35:    35  0.6029144 0.0025771860 0.5554696     0.0474448465  0.0854139412 #>  36:    36  0.6561539 0.0055411531 0.6807954    -0.0246415405 -0.0361952208 #>  37:    37  0.6089547 0.0035024706 0.5712634     0.0376913232  0.0659788851 #>  38:    38  0.5676137 0.0011158692 0.5990288    -0.0314150631 -0.0524433307 #>  39:    39  0.3694683 0.0019769898 0.3754663    -0.0059979884 -0.0159747714 #>  40:    40  0.6417252 0.0043880159 0.6006261     0.0410991455  0.0684271758 #>  41:    41  0.5792268 0.0011235993 0.5839465    -0.0047197134 -0.0080824412 #>  42:    42  0.5586279 0.0055275451 0.5823086    -0.0236807449 -0.0406669988 #>  43:    43  0.3582330 0.0024488026 0.3276013     0.0306317114  0.0935030181 #>  44:    44  0.5068864 0.0006430995 0.5064009     0.0004855044  0.0009587353 #>  45:    45  0.7417242 0.0014031729 0.6872763     0.0544478250  0.0792226082 #>  46:    46  0.4910630 0.0016724234 0.4979208    -0.0068577976 -0.0137728685 #>  47:    47  0.6256094 0.0017294602 0.5937348     0.0318746307  0.0536849638 #>  48:    48  0.5598116 0.0020497925 0.5316100     0.0282015760  0.0530493723 #>  49:    49  0.7516421 0.0017009423 0.6690764     0.0825656773  0.1234024621 #>  50:    50  0.5996121 0.0019008599 0.5281070     0.0715050382  0.1353987581 #>  51:    51  0.6295201 0.0020805334 0.6470065    -0.0174863879 -0.0270266051 #>  52:    52  0.5724893 0.0009691105 0.5240569     0.0484323761  0.0924181593 #>  53:    53  0.6486673 0.0016762675 0.6432101     0.0054572318  0.0084843695 #>  54:    54  0.4782135 0.0015887938 0.5162136    -0.0380001635 -0.0736132506 #>  55:    55  0.5003852 0.0056962562 0.4091676     0.0912175513  0.2229344165 #>  56:    56  0.5034387 0.0019197685 0.4667933     0.0366453694  0.0785044822 #>  57:    57  0.6982961 0.0030676937 0.6610340     0.0372620958  0.0563694073 #>  58:    58  0.6053892 0.0140214238 0.5422465     0.0631427675  0.1164466182 #>  59:    59  0.4441367 0.0026254866 0.4213550     0.0227817447  0.0540678163 #>  60:    60  0.7526088 0.0076305113 0.6014569     0.1511518983  0.2513095929 #>  61:    61  0.4896763 0.0056383277 0.4275514     0.0621248788  0.1453038851 #>  62:    62  0.5705590 0.0074579340 0.6049983    -0.0344392644 -0.0569245675 #>  63:    63  0.5264744 0.0014694889 0.4810370     0.0454373886  0.0944571667 #>  64:    64  0.6837173 0.0047937465 0.7089580    -0.0252407107 -0.0356025491 #>  65:    65  0.4297868 0.0064204076 0.3844496     0.0453371438  0.1179273972 #>  66:    66  0.5694287 0.0023131787 0.4789663     0.0904623931  0.1888700737 #>  67:    67  0.5042332 0.0123607863 0.5487671    -0.0445338767 -0.0811526026 #>  68:    68  0.3843057 0.0088062639 0.4485597    -0.0642540162 -0.1432451744 #>  69:    69  0.6766113 0.0042314777 0.5980372     0.0785740620  0.1313865702 #>  70:    70  0.5867536 0.0038092869 0.5797907     0.0069629340  0.0120093925 #>  71:    71  0.6978945 0.0011959045 0.6324704     0.0654241258  0.1034421905 #>  72:    72  0.5683481 0.0048758125 0.5249744     0.0433737042  0.0826206148 #>  73:    73  0.6370002 0.0030962141 0.6372674    -0.0002671567 -0.0004192224 #>  74:    74  0.4127460 0.0038287596 0.4383629    -0.0256169094 -0.0584376775 #>  75:    75  0.4780651 0.0108196762 0.4653262     0.0127389104  0.0273763040 #>  76:    76  0.6509041 0.0033485445 0.6506095     0.0002945800  0.0004527753 #>  77:    77  0.6292353 0.0012967632 0.5831126     0.0461226971  0.0790974102 #>  78:    78  0.5437774 0.0007938156 0.4829140     0.0608633744  0.1260335726 #>  79:    79  0.7427696 0.0031663304 0.7065272     0.0362424642  0.0512966315 #>  80:    80  0.5425855 0.0019641894 0.5481508    -0.0055653021 -0.0101528660 #>  81:    81  0.5197554 0.0048798479 0.5827652    -0.0630098026 -0.1081221130 #>  82:    82  0.5426813 0.0012636952 0.4906194     0.0520619425  0.1061147344 #>  83:    83  0.7853279 0.0018017217 0.7149549     0.0703730421  0.0984300452 #>  84:    84  0.7188162 0.0011805727 0.7086060     0.0102102161  0.0144088769 #>  85:    85  0.4960613 0.0007445642 0.5642763    -0.0682149809 -0.1208893306 #>  86:    86  0.4138832 0.0026729463 0.4343485    -0.0204653503 -0.0471173487 #>  87:    87  0.4860988 0.0020924787 0.5084793    -0.0223805567 -0.0440146826 #>  88:    88  0.5920957 0.0011606226 0.5885485     0.0035471761  0.0060269903 #>  89:    89  0.6240917 0.0028470052 0.6036379     0.0204538048  0.0338842303 #>  90:    90  0.6757469 0.0017367887 0.6248632     0.0508837461  0.0814318225 #>  91:    91  0.4905933 0.0019050290 0.4847009     0.0058924735  0.0121569280 #>  92:    92  0.5115639 0.0034775579 0.5500221    -0.0384582232 -0.0699212326 #>  93:    93  0.6265034 0.0018818107 0.6151870     0.0113163606  0.0183949921 #>  94:    94  0.4956177 0.0036720461 0.4219392     0.0736784969  0.1746187486 #>  95:    95  0.6340703 0.0171949356 0.7612616    -0.1271913039 -0.1670796236 #>  96:    96  0.6125654 0.0008937470 0.6630469    -0.0504815702 -0.0761357411 #>  97:    97  0.5658907 0.0009155062 0.5719100    -0.0060192899 -0.0105248899 #>  98:    98  0.5704144 0.0009436609 0.5829141    -0.0124997243 -0.0214435091 #>  99:    99  0.3937577 0.0057148836 0.4232778    -0.0295200978 -0.0697416613 #> 100:   100  0.5309513 0.0014341470 0.5001337     0.0308176348  0.0616187973 #>        sim   estimate variance_est  estimand prediction_error est_rel_error #>      variance_prediction_error  std_error    ci_lower  ci_upper #>                          <num>      <num>       <num>     <num> #>   1:                -0.8807182 0.09481912  0.31591566 0.6875998 #>   2:                -0.9897709 0.02776685  0.56450699 0.6733510 #>   3:                -0.2376059 0.23971688 -0.12431818 0.8153547 #>   4:                -0.9863967 0.03202069  0.52097315 0.6464919 #>   5:                -0.7566262 0.13543963  0.06793667 0.5988503 #>   6:                -0.9815659 0.03727523  0.44067855 0.5867948 #>   7:                -0.9396171 0.06746304  0.52986022 0.7943105 #>   8:                -0.9756134 0.04287309  0.58638263 0.7544421 #>   9:                -0.9862140 0.03223505  0.46945872 0.5958178 #>  10:                -0.9820811 0.03675061  0.39784650 0.5419063 #>  11:                -0.9447232 0.06454763  0.51253233 0.7655544 #>  12:                -0.9858896 0.03261211  0.53153355 0.6593707 #>  13:                -0.9326745 0.07123585  0.32108763 0.6003270 #>  14:                -0.9774499 0.04122711  0.38408947 0.5456968 #>  15:                -0.6773488 0.15594644 -0.08824483 0.5230540 #>  16:                -0.9854158 0.03315506  0.42545879 0.5554242 #>  17:                -0.9809792 0.03786374  0.59126824 0.7396914 #>  18:                -0.9708882 0.04684288  0.58303009 0.7666508 #>  19:                -0.8980504 0.08766001  0.36691580 0.7105367 #>  20:                -0.9708464 0.04687643  0.54764656 0.7313988 #>  21:                -0.9774375 0.04123848  0.49140718 0.6530590 #>  22:                -0.9495897 0.06164086  0.41623891 0.6578666 #>  23:                -0.9708665 0.04686032  0.50865503 0.6923441 #>  24:                -0.9336618 0.07071160  0.46972404 0.7469084 #>  25:                -0.9858910 0.03261051  0.51011810 0.6379490 #>  26:                -0.6701669 0.15767251  0.17697812 0.7950430 #>  27:                -0.9692747 0.04812347  0.45933134 0.6479719 #>  28:                -0.9347733 0.07011671  0.37957113 0.6544236 #>  29:               119.4285843 3.01282310 -8.02421351 3.7858360 #>  30:                -0.9793450 0.03945672  0.67019199 0.8248595 #>  31:                -0.9824661 0.03635366  0.50549833 0.6480020 #>  32:                -0.9891670 0.02857478  0.43365183 0.5456629 #>  33:                -0.9812847 0.03755842  0.59838194 0.7456082 #>  34:                -0.6732636 0.15693059  0.16884595 0.7840026 #>  35:                -0.9658077 0.05076599  0.50341491 0.7024139 #>  36:                -0.9264839 0.07443892  0.51025627 0.8020515 #>  37:                -0.9535317 0.05918167  0.49296079 0.7249487 #>  38:                -0.9851954 0.03340463  0.50214182 0.6330856 #>  39:                -0.9737707 0.04446335  0.28232174 0.4566149 #>  40:                -0.9417829 0.06624210  0.51189308 0.7715573 #>  41:                -0.9850929 0.03352013  0.51352856 0.6449251 #>  42:                -0.9266644 0.07434746  0.41290955 0.7043462 #>  43:                -0.9675110 0.04948538  0.26124347 0.4552226 #>  44:                -0.9914678 0.02535941  0.45718287 0.5565899 #>  45:                -0.9813837 0.03745895  0.66830598 0.8151424 #>  46:                -0.9778115 0.04089527  0.41090973 0.5712163 #>  47:                -0.9770547 0.04158678  0.54410083 0.7071180 #>  48:                -0.9728048 0.04527463  0.47107491 0.6485482 #>  49:                -0.9774331 0.04124248  0.67080832 0.8324759 #>  50:                -0.9747807 0.04359885  0.51415991 0.6850643 #>  51:                -0.9723970 0.04561286  0.54012049 0.7189196 #>  52:                -0.9871425 0.03113054  0.51147456 0.6335040 #>  53:                -0.9777605 0.04094225  0.56842196 0.7289126 #>  54:                -0.9789210 0.03985968  0.40008994 0.5563370 #>  55:                -0.9244261 0.07547355  0.35245977 0.6483106 #>  56:                -0.9745299 0.04381516  0.41756257 0.5893148 #>  57:                -0.9593000 0.05538676  0.58974005 0.8068522 #>  58:                -0.8139737 0.11841209  0.37330580 0.8374727 #>  59:                -0.9651669 0.05123950  0.34370916 0.5445643 #>  60:                -0.8987638 0.08735280  0.58140050 0.9238172 #>  61:                -0.9251947 0.07508880  0.34250493 0.6368476 #>  62:                -0.9010534 0.08635933  0.40129782 0.7398202 #>  63:                -0.9805039 0.03833391  0.45134126 0.6016074 #>  64:                -0.9364000 0.06923689  0.54801545 0.8194191 #>  65:                -0.9148186 0.08012745  0.27273986 0.5868337 #>  66:                -0.9693104 0.04809552  0.47516317 0.6636941 #>  67:                -0.8360058 0.11117907  0.28632622 0.7221402 #>  68:                -0.8831647 0.09384170  0.20037937 0.5682321 #>  69:                -0.9438597 0.06504981  0.54911602 0.8041066 #>  70:                -0.9494611 0.06171942  0.46578579 0.7077215 #>  71:                -0.9841336 0.03458185  0.63011536 0.7656737 #>  72:                -0.9353112 0.06982702  0.43148962 0.7052065 #>  73:                -0.9589216 0.05564363  0.52794072 0.7460598 #>  74:                -0.9492027 0.06187697  0.29146935 0.5340226 #>  75:                -0.8564522 0.10401767  0.27419418 0.6819360 #>  76:                -0.9555739 0.05786661  0.53748765 0.7643206 #>  77:                -0.9827955 0.03601060  0.55865583 0.6998148 #>  78:                -0.9894682 0.02817473  0.48855589 0.5989988 #>  79:                -0.9579914 0.05627016  0.63248216 0.8530571 #>  80:                -0.9739405 0.04431918  0.45572155 0.6294495 #>  81:                -0.9352576 0.06985591  0.38284031 0.6566704 #>  82:                -0.9832342 0.03554849  0.47300753 0.6123551 #>  83:                -0.9760960 0.04244669  0.70213394 0.8685219 #>  84:                -0.9843370 0.03435946  0.65147286 0.7861595 #>  85:                -0.9901216 0.02728670  0.44258034 0.5495422 #>  86:                -0.9645372 0.05170054  0.31255196 0.5152144 #>  87:                -0.9722385 0.04574362  0.39644294 0.5757546 #>  88:                -0.9846017 0.03406791  0.52532380 0.6588676 #>  89:                -0.9622279 0.05335734  0.51951323 0.7286701 #>  90:                -0.9769575 0.04167480  0.59406581 0.7574280 #>  91:                -0.9747254 0.04364664  0.40504750 0.5761392 #>  92:                -0.9538622 0.05897082  0.39598319 0.6271446 #>  93:                -0.9750335 0.04337984  0.54148047 0.7115263 #>  94:                -0.9512819 0.06059741  0.37684897 0.6143864 #>  95:                -0.7718698 0.13112946  0.37706129 0.8910793 #>  96:                -0.9881424 0.02989560  0.55397107 0.6711597 #>  97:                -0.9878537 0.03025733  0.50658744 0.6251940 #>  98:                -0.9874802 0.03071906  0.51020614 0.6306227 #>  99:                -0.9241790 0.07559685  0.24559061 0.5419248 #> 100:                -0.9809728 0.03787013  0.45672720 0.6051754 #>      variance_prediction_error  std_error    ci_lower  ci_upper #>  #> $summary_table #>                                       Metric    Value #> 1             Empirical Bias on Policy Value -0.02093 #> 2     Average relative error on Policy Value -0.03638 #> 3             RMSE of errors on Policy Value  0.50223 #> 4 Empirical Coverage of Confidence Intervals  0.95000 #>  #> $interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"what-does-it-return","dir":"Articles","previous_headings":"","what":"What Does It Return?","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"output data.table one row per simulation, includes: estimate: estimated policy value variance_est: estimated variance estimand: true policy value (computed held-context data) prediction_error: estimate - estimand est_rel_error: relative error estimate variance_prediction_error: relative error variance ci_lower, ci_upper: bounds confidence interval std_error: standard error Plus summary metrics like average prediction error empirical coverage","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"example-output-preview","dir":"Articles","previous_headings":"","what":"Example Output Preview","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"Expected columns: sim, estimate, variance_est, estimand, prediction_error, est_rel_error, variance_prediction_error, std_error, ci_lower, ci_upper","code":"head(result)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"list_betas updated internally track true parameters per simulation first simulation discarded design (due writing issues contextual) Approximately 20% simulations excluded final error summaries (robustness outliers)","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"recommended-use-cases","dir":"Articles","previous_headings":"","what":"Recommended Use Cases","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"Validate bandit policies repeated experiments Compare bias variance different policy types Analyze empirical coverage confidence intervals Stress-test policies different batch sizes, sigma levels, dimensions","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"simulation builds : Contextual bandits (contextual package) -policy CRAM estimation Influence-function-based CI construction See also: cram_policy() -policy CRAM cram_bandit() single-run evaluation BatchContextualLinTSPolicy, LinUCBDisjointPolicyEpsilon, etc.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"what-is-cram_learning","dir":"Articles","previous_headings":"","what":"üîç What is cram_learning()?","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"cram_learning() function runs core learning routine CRAM framework. learns batch-wise policies using cumulative data splits supports: Different model types (causal_forest, s_learner, m_learner) Different learners (ridge, fnn) Flexible batching (sequential parallel) Support custom models (custom_fit, custom_predict) function typically called internally wrappers like cram_policy(), cram_simulation(), cram_ml() ‚Äî can also used standalone advanced use.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"when-to-use-it","dir":"Articles","previous_headings":"","what":"üß† When to use it?","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Use cram_learning() directly want: - control model training batch handling - debug visualize policy learning phase - inject custom models outside built-ones","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"example-running-cram_learning-with-causal-forest","dir":"Articles","previous_headings":"","what":"üìò Example: Running cram_learning() with Causal Forest","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"","code":"# Simulated data X <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D <- sample(c(0, 1), 100, replace = TRUE) Y <- rnorm(100) # Parameters batch <- 20 model_type <- 'causal_forest'      # causal_forest, s_learner, or m_learner learner_type <- NULL               # NULL for causal_forest baseline_policy <- as.list(rep(0, nrow(X)))  # or random: as.list(sample(c(0, 1), nrow(X), TRUE)) parallelize_batch <- FALSE         # Set to TRUE for parallelized learning model_params <- NULL               # e.g., list(num.trees = 100) for causal_forest # Run cram_learning learning_result <- cram_learning(   X = X,   D = D,   Y = Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"output","dir":"Articles","previous_headings":"","what":"üì¶ Output","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Returns list :","code":"str(learning_result) #> List of 3 #>  $ final_policy_model:List of 28 #>   ..$ _ci_group_size          : num 2 #>   ..$ _num_variables          : num 5 #>   ..$ _num_trees              : num 100 #>   ..$ _root_nodes             :List of 100 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. .. [list output truncated] #>   ..$ _child_nodes            :List of 100 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. .. [list output truncated] #>   ..$ _leaf_samples           :List of 100 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 91 33 98 72 13 81 29 9 36 75 ... #>   .. .. ..$ : num [1:13] 66 25 67 1 43 92 38 8 70 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 7 91 72 43 64 16 38 56 42 #>   .. .. ..$ : num [1:16] 36 68 54 67 31 90 57 25 27 71 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 3 17 11 56 63 29 43 38 77 55 ... #>   .. .. ..$ : num [1:9] 8 92 13 94 93 47 83 4 5 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 43 2 76 39 24 86 56 8 83 1 ... #>   .. .. ..$ : num [1:8] 35 94 0 22 21 85 3 52 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 65 16 46 72 54 42 80 94 88 28 ... #>   .. .. ..$ : num [1:13] 45 85 67 38 3 74 95 73 4 30 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 76 88 74 15 29 30 #>   .. .. ..$ : num [1:19] 21 26 41 85 27 77 17 0 87 73 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 41 75 62 80 40 82 #>   .. .. ..$ : num [1:19] 46 23 53 4 38 63 89 95 39 47 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:20] 53 89 24 7 66 54 92 10 40 15 ... #>   .. .. ..$ : num [1:5] 68 80 41 83 51 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 41 45 53 64 16 82 72 32 96 17 ... #>   .. .. ..$ : num [1:12] 8 83 66 50 31 77 94 76 62 1 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 64 66 94 57 14 39 84 23 78 67 ... #>   .. .. ..$ : num [1:13] 72 59 49 10 17 69 31 9 83 96 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 24 95 43 82 20 72 53 80 48 92 ... #>   .. .. ..$ : num [1:12] 90 3 52 4 16 31 34 12 11 97 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 52 82 35 33 97 84 12 23 #>   .. .. ..$ : num [1:17] 96 17 31 40 90 0 8 18 61 89 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 73 38 58 49 9 88 63 25 30 6 ... #>   .. .. ..$ : num [1:11] 12 3 68 94 98 85 57 44 21 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 6 74 39 15 44 25 66 32 38 24 ... #>   .. .. ..$ : num [1:8] 98 36 55 51 69 68 85 42 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 30 43 74 24 99 58 31 52 36 73 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 32 91 62 42 77 76 40 67 94 64 ... #>   .. .. ..$ : num [1:9] 79 53 87 7 46 13 99 56 74 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 76 88 15 57 44 54 74 64 70 46 #>   .. .. ..$ : num [1:15] 96 83 26 72 73 47 87 45 69 1 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 71 27 13 64 72 79 47 69 42 44 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 33 80 69 40 27 78 22 13 94 36 ... #>   .. .. ..$ : num [1:11] 6 4 32 14 58 48 52 19 49 92 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 45 83 92 78 86 27 64 5 22 77 ... #>   .. .. ..$ : num [1:11] 21 62 48 38 74 37 63 60 4 80 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 59 45 67 29 6 69 57 55 94 91 ... #>   .. .. ..$ : num [1:12] 13 30 3 89 1 84 38 33 43 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 33 67 74 57 35 84 79 3 94 70 ... #>   .. .. ..$ : num [1:14] 91 53 32 60 26 54 5 61 83 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 19 15 12 4 6 97 55 48 20 #>   .. .. ..$ : num [1:16] 49 96 92 34 62 8 32 60 26 69 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 62 96 52 15 53 27 77 98 59 13 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 15 73 76 36 39 94 16 63 93 66 ... #>   .. .. ..$ : num [1:13] 69 65 75 38 28 13 83 21 47 45 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 76 11 71 97 81 37 82 28 16 83 ... #>   .. .. ..$ : num [1:9] 89 10 44 38 63 47 34 52 66 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 20 26 56 69 32 60 76 75 13 #>   .. .. ..$ : num [1:16] 45 38 68 25 19 2 67 96 95 47 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:15] 9 0 61 81 49 86 67 23 5 88 ... #>   .. .. ..$ : num [1:10] 74 87 38 30 3 19 60 13 58 63 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:4] 34 81 44 57 #>   .. .. ..$ : num [1:21] 40 23 46 95 48 22 16 1 25 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 55 98 60 91 81 57 #>   .. .. ..$ : num [1:19] 64 39 25 77 22 11 74 37 10 38 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 39 44 63 11 70 79 50 8 92 58 ... #>   .. .. ..$ : num [1:9] 36 51 0 3 5 26 16 41 57 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 8 92 80 28 32 76 70 50 73 30 ... #>   .. .. ..$ : num [1:14] 51 10 78 23 68 16 64 44 31 66 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 31 72 0 88 41 83 34 94 39 24 #>   .. .. ..$ : num [1:15] 37 7 12 56 53 69 89 71 99 28 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 71 69 76 24 11 36 50 43 98 94 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 36 81 14 39 91 88 86 29 #>   .. .. ..$ : num [1:17] 7 78 84 85 54 75 40 99 97 28 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 32 81 20 36 51 77 14 39 22 41 ... #>   .. .. ..$ : num [1:9] 99 79 52 28 65 12 10 48 90 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 93 74 70 71 20 85 72 91 82 14 ... #>   .. .. ..$ : num [1:9] 38 65 81 90 61 9 49 8 86 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 66 91 43 42 20 85 90 35 70 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 95 92 44 70 14 82 91 81 42 2 ... #>   .. .. ..$ : num [1:12] 25 35 74 52 43 84 21 20 71 89 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:18] 22 95 61 92 18 35 24 67 69 42 ... #>   .. .. ..$ : num [1:7] 84 56 68 16 63 99 48 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 7 41 13 22 72 82 60 35 55 64 ... #>   .. .. ..$ : num [1:9] 48 95 67 43 61 1 38 63 21 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 41 82 88 26 28 55 60 14 17 7 ... #>   .. .. ..$ : num [1:14] 35 13 51 77 22 36 93 71 57 92 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 55 78 26 86 11 88 93 5 #>   .. .. ..$ : num [1:17] 4 32 63 87 49 97 70 85 18 14 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:21] 60 73 12 44 13 35 88 55 40 53 ... #>   .. .. ..$ : num [1:4] 26 0 81 28 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 2 85 48 93 23 6 19 88 74 91 ... #>   .. .. ..$ : num [1:12] 25 26 0 61 41 96 21 45 62 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 78 93 58 57 48 14 91 16 6 15 ... #>   .. .. ..$ : num [1:12] 43 96 41 0 73 77 75 65 72 99 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 59 5 21 6 49 8 15 54 88 0 ... #>   .. .. ..$ : num [1:9] 50 68 25 1 89 79 11 52 48 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 70 4 59 93 54 23 11 95 79 48 ... #>   .. .. ..$ : num [1:8] 68 90 0 13 96 21 56 45 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 8 11 87 94 0 57 68 47 40 78 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 4 95 78 70 23 82 57 16 67 15 ... #>   .. .. ..$ : num [1:14] 34 37 77 45 62 0 11 27 47 43 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 40 80 15 6 65 46 69 72 76 78 ... #>   .. .. ..$ : num [1:8] 53 13 43 4 37 48 89 75 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 17 76 83 69 55 34 22 82 62 95 ... #>   .. .. ..$ : num [1:12] 25 89 28 13 11 10 48 56 84 37 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 48 4 52 23 39 97 36 84 76 54 #>   .. .. ..$ : num [1:15] 90 71 32 62 86 85 27 89 81 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 73 47 25 32 92 83 15 19 56 37 ... #>   .. .. ..$ : num [1:11] 61 22 7 85 98 23 27 42 52 78 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 91 24 31 57 56 82 36 20 42 27 ... #>   .. .. ..$ : num [1:11] 87 53 51 84 61 95 99 10 77 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 95 84 48 3 66 12 79 20 #>   .. .. ..$ : num [1:17] 90 75 56 71 73 80 10 43 41 8 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 58 32 64 20 6 95 48 85 23 40 ... #>   .. .. ..$ : num [1:11] 37 26 81 71 96 49 0 73 1 87 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 95 85 14 91 16 15 76 36 57 #>   .. .. ..$ : num [1:16] 71 87 89 49 81 26 99 25 96 83 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:21] 15 1 2 67 88 13 97 53 71 3 ... #>   .. .. ..$ : num [1:4] 38 10 8 65 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 81 37 1 22 68 12 44 63 89 14 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 45 76 54 72 32 90 42 49 12 63 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:19] 23 64 36 12 6 32 58 19 34 54 ... #>   .. .. ..$ : num [1:6] 18 75 22 98 86 90 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 2 76 4 15 66 38 42 17 8 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 9 72 80 20 93 71 76 #>   .. .. ..$ : num [1:18] 54 11 3 66 69 84 26 44 0 96 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 86 17 55 93 60 26 29 76 #>   .. .. ..$ : num [1:17] 53 48 50 52 14 38 18 21 64 5 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 46 64 29 58 52 33 93 66 3 6 ... #>   .. .. ..$ : num [1:14] 99 37 27 25 86 49 90 0 71 60 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 40 14 31 77 88 45 59 39 92 49 ... #>   .. .. ..$ : num [1:14] 84 30 75 50 4 52 37 56 80 19 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 72 64 6 19 14 42 17 53 3 60 ... #>   .. .. ..$ : num [1:11] 54 50 11 84 13 79 77 92 1 67 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 58 6 32 36 19 42 66 74 76 5 ... #>   .. .. ..$ : num [1:9] 45 69 37 83 28 22 11 99 77 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 40 17 30 25 65 88 78 43 28 19 ... #>   .. .. ..$ : num [1:14] 1 67 95 11 13 81 58 36 37 79 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 83 5 78 84 61 42 35 28 44 7 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 43 96 10 49 66 44 19 82 89 40 ... #>   .. .. ..$ : num [1:11] 0 5 69 52 21 51 98 55 27 42 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 12 75 79 69 24 55 29 41 #>   .. .. ..$ : num [1:17] 90 44 52 65 39 49 0 37 2 70 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 34 12 88 71 54 32 70 42 29 79 ... #>   .. .. ..$ : num [1:8] 68 0 90 87 83 69 65 80 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 91 20 29 14 76 64 34 44 16 39 ... #>   .. .. ..$ : num [1:14] 45 89 13 99 72 75 8 47 83 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 20 30 45 60 26 17 55 34 16 64 ... #>   .. .. ..$ : num [1:13] 69 89 36 57 76 27 85 0 98 24 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 9 61 88 5 93 45 42 81 39 73 ... #>   .. .. ..$ : num [1:12] 98 48 65 12 55 53 68 28 13 80 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 26 30 81 61 79 90 68 55 83 64 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 11 94 62 43 39 56 85 28 53 64 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 62 87 85 30 32 53 96 88 70 33 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 48 95 49 81 19 88 53 32 87 65 ... #>   .. .. ..$ : num [1:8] 75 3 69 11 42 22 12 31 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 48 14 19 83 81 71 63 49 89 53 #>   .. .. ..$ : num [1:15] 74 87 77 42 96 75 3 26 11 12 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 24 15 44 16 6 42 2 19 58 5 ... #>   .. .. ..$ : num [1:14] 10 87 90 81 83 45 62 47 17 43 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 42 3 19 66 6 67 24 5 54 14 ... #>   .. .. ..$ : num [1:13] 26 83 69 21 96 99 89 87 65 72 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 26 56 78 21 45 86 17 2 #>   .. .. ..$ : num [1:17] 76 36 37 41 57 47 50 0 90 23 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 90 27 15 66 31 8 50 80 5 83 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 35 33 14 92 39 50 70 58 74 15 ... #>   .. .. ..$ : num [1:14] 49 98 51 18 53 10 86 38 8 77 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 40 1 54 74 4 60 91 33 94 15 ... #>   .. .. ..$ : num [1:8] 98 0 51 18 86 77 13 81 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:19] 57 92 42 95 88 24 58 36 40 14 ... #>   .. .. ..$ : num [1:6] 49 56 81 83 31 28 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 91 56 1 36 35 70 19 74 75 34 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 53 93 82 49 15 30 65 63 72 95 ... #>   .. .. ..$ : num [1:14] 85 3 41 94 8 46 99 24 43 98 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 27 47 0 91 82 18 8 64 9 #>   .. .. ..$ : num [1:16] 3 53 65 85 19 37 38 15 46 33 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 13 45 79 72 48 34 39 95 71 47 ... #>   .. .. ..$ : num [1:11] 31 46 26 64 66 27 59 61 51 97 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:20] 51 57 55 27 71 39 29 69 45 92 ... #>   .. .. ..$ : num [1:5] 66 1 48 4 50 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 50 71 6 13 44 35 97 9 42 79 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 64 12 58 24 97 42 63 35 76 33 ... #>   .. .. ..$ : num [1:11] 26 38 60 28 51 13 8 27 9 18 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 20 94 35 57 30 79 46 78 70 48 ... #>   .. .. ..$ : num [1:11] 1 18 49 26 81 47 8 41 28 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 41 15 78 8 49 94 2 #>   .. .. ..$ : num [1:18] 60 52 25 3 84 99 48 23 43 65 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:15] 82 57 19 91 40 72 64 5 3 58 ... #>   .. .. ..$ : num [1:10] 56 83 87 99 75 96 65 69 77 38 #>   .. .. [list output truncated] #>   ..$ _split_vars             :List of 100 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. .. [list output truncated] #>   ..$ _split_values           :List of 100 #>   .. ..$ : num [1:3] -0.118 -1 -1 #>   .. ..$ : num [1:3] -0.479 -1 -1 #>   .. ..$ : num [1:3] 0.0645 -1 -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] 0.0609 -1 -1 #>   .. ..$ : num [1:3] -0.247 -1 -1 #>   .. ..$ : num [1:3] -0.752 -1 -1 #>   .. ..$ : num [1:3] 0.556 -1 -1 #>   .. ..$ : num [1:3] -0.368 -1 -1 #>   .. ..$ : num [1:3] 0.112 -1 -1 #>   .. ..$ : num [1:3] 0.072 -1 -1 #>   .. ..$ : num [1:3] -0.156 -1 -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] 0.559 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.0192 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.0709 -1 -1 #>   .. ..$ : num [1:3] 0.108 -1 -1 #>   .. ..$ : num [1:3] 0.518 -1 -1 #>   .. ..$ : num [1:3] -0.244 -1 -1 #>   .. ..$ : num [1:3] -0.00557 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.363 -1 -1 #>   .. ..$ : num [1:3] 0.0655 -1 -1 #>   .. ..$ : num [1:3] 0.00789 -1 -1 #>   .. ..$ : num [1:3] 0.449 -1 -1 #>   .. ..$ : num [1:3] -0.474 -1 -1 #>   .. ..$ : num [1:3] -0.308 -1 -1 #>   .. ..$ : num [1:3] 0.393 -1 -1 #>   .. ..$ : num [1:3] 0.0208 -1 -1 #>   .. ..$ : num [1:3] -0.103 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.356 -1 -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.363 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.133 -1 -1 #>   .. ..$ : num [1:3] 0.259 -1 -1 #>   .. ..$ : num [1:3] 0.112 -1 -1 #>   .. ..$ : num [1:3] -0.368 -1 -1 #>   .. ..$ : num [1:3] -0.0103 -1 -1 #>   .. ..$ : num [1:3] 0.524 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.434 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.604 -1 -1 #>   .. ..$ : num [1:3] 0.429 -1 -1 #>   .. ..$ : num [1:3] -0.016 -1 -1 #>   .. ..$ : num [1:3] 0.199 -1 -1 #>   .. ..$ : num [1:3] -0.0779 -1 -1 #>   .. ..$ : num [1:3] -0.0526 -1 -1 #>   .. ..$ : num [1:3] 0.118 -1 -1 #>   .. ..$ : num [1:3] 0.0465 -1 -1 #>   .. ..$ : num [1:3] 0.468 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.344 -1 -1 #>   .. ..$ : num [1:3] -0.341 -1 -1 #>   .. ..$ : num [1:3] -0.00557 -1 -1 #>   .. ..$ : num [1:3] 0.192 -1 -1 #>   .. ..$ : num [1:3] -0.264 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] -0.262 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.408 -1 -1 #>   .. ..$ : num [1:3] -0.341 -1 -1 #>   .. ..$ : num [1:3] 0.424 -1 -1 #>   .. ..$ : num [1:3] 0.255 -1 -1 #>   .. ..$ : num [1:3] -0.323 -1 -1 #>   .. ..$ : num [1:3] 0.122 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] -0.105 -1 -1 #>   .. ..$ : num [1:3] 0.0465 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] -0.489 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.172 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.105 -1 -1 #>   .. ..$ : num [1:3] -0.163 -1 -1 #>   .. ..$ : num [1:3] 0.344 -1 -1 #>   .. ..$ : num [1:3] 0.376 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] 0.0673 -1 -1 #>   .. ..$ : num [1:3] 0.424 -1 -1 #>   .. .. [list output truncated] #>   ..$ _drawn_samples          :List of 100 #>   .. ..$ : num [1:50] 38 35 31 12 81 33 43 6 9 72 ... #>   .. ..$ : num [1:50] 98 44 11 56 16 43 33 38 81 93 ... #>   .. ..$ : num [1:50] 40 29 92 94 63 13 37 76 93 47 ... #>   .. ..$ : num [1:50] 17 1 47 21 52 56 8 51 18 24 ... #>   .. ..$ : num [1:50] 21 52 56 67 55 4 60 44 66 6 ... #>   .. ..$ : num [1:50] 77 79 80 47 21 6 85 87 22 88 ... #>   .. ..$ : num [1:50] 92 96 20 84 45 54 79 15 89 42 ... #>   .. ..$ : num [1:50] 98 42 39 45 40 3 21 4 13 82 ... #>   .. ..$ : num [1:50] 29 96 59 94 61 56 75 17 38 41 ... #>   .. ..$ : num [1:50] 26 49 50 33 71 75 48 76 67 66 ... #>   .. ..$ : num [1:50] 52 0 18 42 3 23 21 75 96 43 ... #>   .. ..$ : num [1:50] 80 23 0 18 61 98 33 35 24 11 ... #>   .. ..$ : num [1:50] 21 63 24 38 70 67 61 74 75 58 ... #>   .. ..$ : num [1:50] 9 85 39 74 88 24 99 51 38 58 ... #>   .. ..$ : num [1:50] 64 12 94 87 59 1 68 0 52 74 ... #>   .. ..$ : num [1:50] 56 1 43 24 94 69 31 96 0 32 ... #>   .. ..$ : num [1:50] 57 27 56 82 41 77 45 21 91 72 ... #>   .. ..$ : num [1:50] 73 78 45 8 33 82 88 13 65 66 ... #>   .. ..$ : num [1:50] 14 66 45 7 80 64 40 21 6 8 ... #>   .. ..$ : num [1:50] 22 52 49 77 35 38 83 92 27 66 ... #>   .. ..$ : num [1:50] 53 87 45 1 21 32 30 56 50 16 ... #>   .. ..$ : num [1:50] 37 74 53 88 58 35 9 55 56 43 ... #>   .. ..$ : num [1:50] 45 61 13 11 80 95 36 4 19 56 ... #>   .. ..$ : num [1:50] 27 49 5 20 62 97 51 45 34 80 ... #>   .. ..$ : num [1:50] 66 13 61 57 45 41 15 51 83 73 ... #>   .. ..$ : num [1:50] 36 39 28 34 8 51 78 45 81 13 ... #>   .. ..$ : num [1:50] 38 5 86 6 13 47 95 25 88 59 ... #>   .. ..$ : num [1:50] 58 60 29 23 75 9 54 63 3 11 ... #>   .. ..$ : num [1:50] 95 57 52 19 1 55 87 37 34 80 ... #>   .. ..$ : num [1:50] 77 10 51 4 84 16 40 39 86 19 ... #>   .. ..$ : num [1:50] 23 36 11 54 58 70 39 3 49 41 ... #>   .. ..$ : num [1:50] 68 55 39 59 78 23 80 65 31 11 ... #>   .. ..$ : num [1:50] 43 15 85 12 59 11 52 81 55 0 ... #>   .. ..$ : num [1:50] 99 34 56 94 43 7 2 37 6 70 ... #>   .. ..$ : num [1:50] 48 32 54 14 96 31 52 51 20 39 ... #>   .. ..$ : num [1:50] 32 86 2 81 12 64 48 90 93 29 ... #>   .. ..$ : num [1:50] 48 90 17 38 70 14 91 71 8 58 ... #>   .. ..$ : num [1:50] 32 86 91 11 65 13 40 20 49 21 ... #>   .. ..$ : num [1:50] 52 54 95 29 21 61 80 66 42 67 ... #>   .. ..$ : num [1:50] 91 25 89 9 61 99 54 14 80 42 ... #>   .. ..$ : num [1:50] 13 93 52 41 19 89 15 83 17 61 ... #>   .. ..$ : num [1:50] 36 61 15 1 64 95 7 9 46 16 ... #>   .. ..$ : num [1:50] 18 65 23 53 28 40 99 87 68 44 ... #>   .. ..$ : num [1:50] 97 81 32 86 56 35 44 60 5 26 ... #>   .. ..$ : num [1:50] 59 89 61 47 45 14 51 2 3 70 ... #>   .. ..$ : num [1:50] 91 73 89 68 2 70 79 48 76 26 ... #>   .. ..$ : num [1:50] 88 10 23 8 21 77 6 70 13 2 ... #>   .. ..$ : num [1:50] 12 11 77 63 95 1 4 13 62 93 ... #>   .. ..$ : num [1:50] 87 11 68 69 90 24 3 93 8 78 ... #>   .. ..$ : num [1:50] 69 16 77 62 93 47 40 4 87 24 ... #>   .. ..$ : num [1:50] 4 13 43 7 15 65 18 22 62 99 ... #>   .. ..$ : num [1:50] 62 29 97 90 50 17 92 76 45 47 ... #>   .. ..$ : num [1:50] 62 66 35 78 94 4 18 96 27 90 ... #>   .. ..$ : num [1:50] 35 73 79 3 92 76 89 11 66 29 ... #>   .. ..$ : num [1:50] 80 81 36 21 48 8 51 54 16 75 ... #>   .. ..$ : num [1:50] 99 77 73 3 40 64 95 9 0 74 ... #>   .. ..$ : num [1:50] 6 75 41 20 72 17 95 65 85 73 ... #>   .. ..$ : num [1:50] 95 6 16 81 29 48 69 87 30 75 ... #>   .. ..$ : num [1:50] 71 8 90 15 93 95 2 34 76 24 ... #>   .. ..$ : num [1:50] 31 81 90 68 95 70 29 65 2 77 ... #>   .. ..$ : num [1:50] 65 15 93 54 53 61 77 25 6 36 ... #>   .. ..$ : num [1:50] 41 74 60 65 35 46 77 73 25 36 ... #>   .. ..$ : num [1:50] 38 20 19 0 14 2 59 75 26 70 ... #>   .. ..$ : num [1:50] 66 23 57 2 72 17 8 50 21 77 ... #>   .. ..$ : num [1:50] 53 26 17 86 48 73 52 93 18 89 ... #>   .. ..$ : num [1:50] 37 96 53 91 11 87 46 0 90 33 ... #>   .. ..$ : num [1:50] 17 9 75 39 56 52 99 59 71 66 ... #>   .. ..$ : num [1:50] 52 79 75 12 88 17 50 66 59 45 ... #>   .. ..$ : num [1:50] 37 30 44 42 45 1 76 87 77 78 ... #>   .. ..$ : num [1:50] 11 30 63 58 24 37 69 22 19 52 ... #>   .. ..$ : num [1:50] 46 72 40 51 27 83 21 56 17 19 ... #>   .. ..$ : num [1:50] 46 40 43 5 7 37 49 12 27 81 ... #>   .. ..$ : num [1:50] 9 83 35 78 75 24 31 33 79 71 ... #>   .. ..$ : num [1:50] 3 44 32 39 42 69 29 55 78 57 ... #>   .. ..$ : num [1:50] 4 18 64 51 56 98 20 75 31 47 ... #>   .. ..$ : num [1:50] 55 99 64 29 58 14 72 45 89 27 ... #>   .. ..$ : num [1:50] 54 25 26 76 52 36 65 89 47 46 ... #>   .. ..$ : num [1:50] 5 91 76 93 74 61 36 71 54 13 ... #>   .. ..$ : num [1:50] 17 95 15 66 38 12 44 99 9 85 ... #>   .. ..$ : num [1:50] 95 79 90 12 28 69 56 54 93 3 ... #>   .. ..$ : num [1:50] 14 48 87 31 75 54 28 15 33 7 ... #>   .. ..$ : num [1:50] 89 15 88 44 48 96 77 7 28 9 ... #>   .. ..$ : num [1:50] 52 16 22 47 20 3 92 98 57 28 ... #>   .. ..$ : num [1:50] 0 54 44 89 74 10 39 65 24 84 ... #>   .. ..$ : num [1:50] 21 49 42 31 60 54 95 29 46 30 ... #>   .. ..$ : num [1:50] 49 5 56 37 21 54 42 26 14 76 ... #>   .. ..$ : num [1:50] 8 51 61 39 32 15 98 40 97 49 ... #>   .. ..$ : num [1:50] 18 21 14 13 59 33 77 49 79 98 ... #>   .. ..$ : num [1:50] 99 87 59 96 77 49 74 31 66 70 ... #>   .. ..$ : num [1:50] 5 57 6 59 24 56 92 15 19 9 ... #>   .. ..$ : num [1:50] 21 82 40 18 9 98 46 63 0 83 ... #>   .. ..$ : num [1:50] 3 93 15 7 94 88 9 22 19 49 ... #>   .. ..$ : num [1:50] 92 50 79 69 46 68 58 39 59 32 ... #>   .. ..$ : num [1:50] 43 90 47 20 34 32 11 26 9 71 ... #>   .. ..$ : num [1:50] 12 50 35 34 93 53 67 8 95 33 ... #>   .. ..$ : num [1:50] 36 99 96 28 8 67 62 18 16 27 ... #>   .. ..$ : num [1:50] 20 47 63 41 17 48 57 34 18 2 ... #>   .. ..$ : num [1:50] 34 15 49 65 38 22 82 51 78 99 ... #>   .. ..$ : num [1:50] 39 86 75 87 62 23 13 96 8 30 ... #>   .. .. [list output truncated] #>   ..$ _send_missing_left      :List of 100 #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. .. [list output truncated] #>   ..$ _pv_values              :List of 100 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.1434 -0.2106 -0.2106 -0.0856 0.2587 ... #>   .. .. ..$ : num [1:7] 0.1616 0.0373 0.0373 0.2264 0.2705 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.6 -0.175 -0.175 0.226 0.298 ... #>   .. .. ..$ : num [1:7] 0.0253 -0.1244 -0.1244 0.0712 0.2635 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.4359 -0.0281 -0.0281 -0.0255 0.2743 ... #>   .. .. ..$ : num [1:7] -0.4072 0.0124 0.0124 -0.0373 0.2478 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.093 0.086 0.086 0.0512 0.2665 ... #>   .. .. ..$ : num [1:7] -0.131 0.369 0.369 -0.359 0.287 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.283 0.154 0.154 0.187 0.281 ... #>   .. .. ..$ : num [1:7] 0.1205 0.0123 0.0123 -0.1004 0.2563 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.137 0.137 0.137 -0.234 0.208 ... #>   .. .. ..$ : num [1:7] 0.0188 0.0851 0.0851 0.0074 0.2602 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5369 -0.146 -0.146 -0.0117 0.2712 ... #>   .. .. ..$ : num [1:7] -0.157 -0.147 -0.147 0.112 0.25 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2483 -0.1215 -0.1215 -0.0178 0.2653 ... #>   .. .. ..$ : num [1:7] -0.163 0.132 0.132 0.044 0.281 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0735 -0.0312 -0.0312 -0.1274 0.2736 ... #>   .. .. ..$ : num [1:7] -0.0507 0.0802 0.0802 0.0785 0.2524 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.1 0.143 0.143 -0.13 0.261 ... #>   .. .. ..$ : num [1:7] -0.227 -0.0169 -0.0169 0.1358 0.2472 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.00532 -0.18847 -0.18847 -0.01481 0.23083 ... #>   .. .. ..$ : num [1:7] -0.4153 -0.2061 -0.2061 -0.0864 0.2705 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.5007 -0.1714 -0.1714 0.0625 0.259 ... #>   .. .. ..$ : num [1:7] -0.088 -0.1313 -0.1313 -0.0434 0.248 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0566 0.155 0.155 0.0704 0.236 ... #>   .. .. ..$ : num [1:7] -0.3628 0.0456 0.0456 -0.0648 0.3231 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0635 0.132 0.132 0.0597 0.2513 ... #>   .. .. ..$ : num [1:7] 0.312 0.164 0.164 0.274 0.297 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.0987 -0.052 -0.052 0.1134 0.261 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5017 0.0587 0.0587 0.0437 0.2645 ... #>   .. .. ..$ : num [1:7] 0.27 0.141 0.141 0.174 0.226 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3428 0.0475 0.0475 -0.2269 0.2681 ... #>   .. .. ..$ : num [1:7] -0.386624 -0.038608 -0.038608 0.000421 0.271994 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.33512 -0.07353 -0.07353 -0.00189 0.26369 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.362 -0.213 -0.213 -0.192 0.262 ... #>   .. .. ..$ : num [1:7] 0.0422 0.2217 0.2217 -0.0312 0.2391 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.256 0.0651 0.0651 0.0559 0.2701 ... #>   .. .. ..$ : num [1:7] 0.1232 0.0491 0.0491 -0.0965 0.2532 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0122 0.0532 0.0532 -0.1579 0.2809 ... #>   .. .. ..$ : num [1:7] -0.071 -0.257 -0.257 0.148 0.28 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.285 0.013 0.013 -0.36 0.261 ... #>   .. .. ..$ : num [1:7] 0.18211 0.00624 0.00624 -0.09823 0.25469 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1411 -0.0856 -0.0856 0.2372 0.2556 ... #>   .. .. ..$ : num [1:7] -0.11206 0.13994 0.13994 0.00914 0.2775 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.18888 0.13044 0.13044 0.00613 0.24482 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.07789 -0.12131 -0.12131 -0.00407 0.27233 ... #>   .. .. ..$ : num [1:7] -0.5053 -0.0324 -0.0324 0.0621 0.2785 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0164 -0.0126 -0.0126 -0.0951 0.241 ... #>   .. .. ..$ : num [1:7] -0.3475 -0.0799 -0.0799 0.1586 0.2471 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.00292 -0.1469 -0.1469 0.1695 0.23999 ... #>   .. .. ..$ : num [1:7] 0.069782 -0.000126 -0.000126 0.043476 0.247289 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.13051 0.08542 0.08542 -0.00868 0.24723 ... #>   .. .. ..$ : num [1:7] 0.1217 -0.2042 -0.2042 0.0107 0.2377 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2682 -0.0303 -0.0303 -0.0124 0.2998 ... #>   .. .. ..$ : num [1:7] -0.0847 -0.1186 -0.1186 0.0755 0.2418 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.35 0.177 0.177 0.135 0.287 ... #>   .. .. ..$ : num [1:7] -0.1774 -0.0477 -0.0477 -0.0665 0.2469 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.2086 -0.0352 -0.0352 -0.099 0.2604 ... #>   .. .. ..$ : num [1:7] 0.1494 -0.1744 -0.1744 -0.0491 0.2932 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0937 0.1195 0.1195 -0.0953 0.2453 ... #>   .. .. ..$ : num [1:7] -0.2555 -0.0744 -0.0744 0.0236 0.2843 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1891 0.0861 0.0861 -0.1655 0.2727 ... #>   .. .. ..$ : num [1:7] -0.1163 -0.0871 -0.0871 0.2171 0.264 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.297 0.1708 0.1708 -0.0269 0.2354 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.319 -0.0463 -0.0463 -0.2839 0.2803 ... #>   .. .. ..$ : num [1:7] -0.3788 -0.0457 -0.0457 -0.0509 0.2643 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0811 0.0789 0.0789 0.0211 0.2738 ... #>   .. .. ..$ : num [1:7] -0.9136 -0.0681 -0.0681 0.2144 0.2568 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.07215 -0.00358 -0.00358 -0.06436 0.27236 ... #>   .. .. ..$ : num [1:7] 0.38 0.173 0.173 0.323 0.275 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.1496 -0.0498 -0.0498 0.1231 0.2756 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3115 0.0369 0.0369 0.0424 0.2886 ... #>   .. .. ..$ : num [1:7] -0.6055 0.0834 0.0834 -0.1047 0.2191 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.320212 0.040369 0.040369 0.000657 0.264658 ... #>   .. .. ..$ : num [1:7] -0.0359 -0.0429 -0.0429 0.1945 0.2513 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2933 -0.0579 -0.0579 -0.0142 0.2544 ... #>   .. .. ..$ : num [1:7] -0.0794 -0.1119 -0.1119 0.0164 0.2599 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2977 -0.0183 -0.0183 -0.1142 0.26 ... #>   .. .. ..$ : num [1:7] -0.11953 -0.09832 -0.09832 0.00601 0.2564 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.181 0.124 0.124 -0.016 0.256 ... #>   .. .. ..$ : num [1:7] -0.1017 0.2074 0.2074 0.0553 0.2534 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.2453 -0.0477 -0.0477 0.088 0.2628 ... #>   .. .. ..$ : num [1:7] -0.1571 0.2505 0.2505 -0.0195 0.2327 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.162 0.0194 0.0194 -0.2 0.2557 ... #>   .. .. ..$ : num [1:7] -0.127 0.0355 0.0355 0.1246 0.2692 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0557 -0.0681 -0.0681 -0.2782 0.2812 ... #>   .. .. ..$ : num [1:7] -0.579 0.105 0.105 0.082 0.253 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0236 0.0534 0.0534 -0.0865 0.2659 ... #>   .. .. ..$ : num [1:7] -0.541 0.167 0.167 -0.184 0.232 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.05813 -0.00118 -0.00118 -0.31238 0.25002 ... #>   .. .. ..$ : num [1:7] -0.259 0.1575 0.1575 0.0419 0.2883 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0723 0.1622 0.1622 -0.1906 0.2577 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.147 -0.147 -0.147 -0.206 0.244 ... #>   .. .. ..$ : num [1:7] -0.0584 0.0075 0.0075 -0.0885 0.2528 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.21198 0.00653 0.00653 0.04426 0.24045 ... #>   .. .. ..$ : num [1:7] -0.2251 -0.2601 -0.2601 0.0559 0.2331 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.684014 0.000903 0.000903 -0.135834 0.261857 ... #>   .. .. ..$ : num [1:7] -0.3293 -0.0824 -0.0824 0.1466 0.2537 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.047 -0.155 -0.155 -0.198 0.27 ... #>   .. .. ..$ : num [1:7] -0.00826 0.24196 0.24196 0.10557 0.26012 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0023 0.0543 0.0543 -0.0516 0.2451 ... #>   .. .. ..$ : num [1:7] -0.0877 0.0492 0.0492 0.0299 0.2692 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.4313 -0.1403 -0.1403 -0.0738 0.2713 ... #>   .. .. ..$ : num [1:7] -0.3991 -0.0413 -0.0413 0.1585 0.2259 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2092 -0.1641 -0.1641 -0.0101 0.2645 ... #>   .. .. ..$ : num [1:7] -0.101 -0.07 -0.07 0.142 0.274 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2424 -0.0774 -0.0774 -0.247 0.2512 ... #>   .. .. ..$ : num [1:7] 0.1411 0.2438 0.2438 0.0474 0.2794 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.061 0.0438 0.0438 -0.123 0.2773 ... #>   .. .. ..$ : num [1:7] 0.2601 -0.0357 -0.0357 0.1061 0.2293 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0174 -0.2038 -0.2038 -0.1081 0.2913 ... #>   .. .. ..$ : num [1:7] -0.3657 -0.0115 -0.0115 0.6287 0.3105 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.151 -0.0247 -0.0247 0.0997 0.2755 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.039 -0.0311 -0.0311 0.2737 0.2709 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3079 -0.2607 -0.2607 0.0915 0.2807 ... #>   .. .. ..$ : num [1:7] -0.281 0.193 0.193 0.103 0.273 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0407 0.0955 0.0955 -0.0265 0.2506 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0891 -0.24 -0.24 0.1725 0.2584 ... #>   .. .. ..$ : num [1:7] -0.0293 0.0214 0.0214 -0.205 0.2618 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5816 -0.1006 -0.1006 0.0784 0.2591 ... #>   .. .. ..$ : num [1:7] 0.00782 0.22312 0.22312 -0.10291 0.26679 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5206 -0.2007 -0.2007 -0.0741 0.2757 ... #>   .. .. ..$ : num [1:7] 0.1234 -0.0596 -0.0596 0.1997 0.2517 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2418 -0.0606 -0.0606 -0.1878 0.268 ... #>   .. .. ..$ : num [1:7] -0.1779 -0.0247 -0.0247 -0.2202 0.2433 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.481 -0.1452 -0.1452 0.0851 0.2667 ... #>   .. .. ..$ : num [1:7] -0.35 0.11 0.11 -0.162 0.241 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.295 0.118 0.118 0.157 0.25 ... #>   .. .. ..$ : num [1:7] -0.2211 -0.045 -0.045 -0.0855 0.2421 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1829 -0.0979 -0.0979 -0.1224 0.2157 ... #>   .. .. ..$ : num [1:7] 0.0791 -0.1033 -0.1033 -0.0849 0.255 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.19424 0.13646 0.13646 0.00721 0.24229 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0913 -0.1505 -0.1505 0.2721 0.2664 ... #>   .. .. ..$ : num [1:7] -0.12944 0.17027 0.17027 0.00202 0.2774 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.14 0.00348 0.00348 0.24093 0.29438 ... #>   .. .. ..$ : num [1:7] 0.2361 0.0165 0.0165 -0.1712 0.2849 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.153 -0.186 -0.186 -0.198 0.291 ... #>   .. .. ..$ : num [1:7] 0.0948 0.1405 0.1405 -0.0393 0.2441 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.428 -0.348 -0.348 -0.248 0.287 ... #>   .. .. ..$ : num [1:7] -0.627 -0.221 -0.221 0.131 0.254 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.276 -0.3847 -0.3847 0.0224 0.2445 ... #>   .. .. ..$ : num [1:7] 0.0934 0.1072 0.1072 -0.0625 0.2662 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2431 -0.058 -0.058 -0.0527 0.2623 ... #>   .. .. ..$ : num [1:7] -0.0646 0.0923 0.0923 0.1512 0.2782 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.1905 0.0987 0.0987 0.0433 0.2455 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.4114 0.0431 0.0431 -0.1235 0.2758 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.171 0.0472 0.0472 0.0164 0.2587 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.237 0.143 0.143 -0.018 0.257 ... #>   .. .. ..$ : num [1:7] 0.174 -0.14 -0.14 0.249 0.294 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0508 0.0221 0.0221 0.1389 0.2508 ... #>   .. .. ..$ : num [1:7] 0.0127 0.0227 0.0227 0.0598 0.2685 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0945 0.029 0.029 -0.0364 0.2586 ... #>   .. .. ..$ : num [1:7] -0.179 0.0673 0.0673 0.2349 0.231 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5913 -0.0178 -0.0178 -0.0182 0.2644 ... #>   .. .. ..$ : num [1:7] -0.36618 0.132671 0.132671 -0.000512 0.255963 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.177 0.17 0.17 -0.133 0.265 ... #>   .. .. ..$ : num [1:7] -0.0474 -0.104 -0.104 0.013 0.2617 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0844 -0.1482 -0.1482 -0.1062 0.2587 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.17712 -0.00182 -0.00182 -0.36857 0.24232 ... #>   .. .. ..$ : num [1:7] -0.2335 0.0897 0.0897 0.1864 0.2765 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0761 0.0472 0.0472 -0.2607 0.252 ... #>   .. .. ..$ : num [1:7] -0.3197 0.1377 0.1377 0.0382 0.2496 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0514 0.0258 0.0258 -0.1524 0.254 ... #>   .. .. ..$ : num [1:7] 0.442 0.358 0.358 0.154 0.263 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.098 -0.1401 -0.1401 -0.0548 0.2536 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0509 -0.0355 -0.0355 0.173 0.2508 ... #>   .. .. ..$ : num [1:7] 0.00194 0.13453 0.13453 0.02499 0.27838 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0342 -0.0847 -0.0847 -0.2556 0.2816 ... #>   .. .. ..$ : num [1:7] 0.28422 0.00692 0.00692 -0.05958 0.25885 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.138 -0.2345 -0.2345 0.0462 0.2486 ... #>   .. .. ..$ : num [1:7] 0.1353 -0.2393 -0.2393 -0.0199 0.2392 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0258 -0.1609 -0.1609 -0.026 0.2696 ... #>   .. .. ..$ : num [1:7] -0.0309 0.265 0.265 -0.1258 0.2168 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.02149 0.00541 0.00541 0.03591 0.26366 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.112 -0.184 -0.184 0.157 0.283 ... #>   .. .. ..$ : num [1:7] -0.2495 -0.0508 -0.0508 0.1312 0.2648 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.02079 -0.00497 -0.00497 -0.1303 0.24454 ... #>   .. .. ..$ : num [1:7] -0.227 0.162 0.162 0.196 0.281 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1716 0.367 0.367 0.0641 0.2971 ... #>   .. .. ..$ : num [1:7] -0.1606 -0.0407 -0.0407 0.1448 0.2398 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.612 -0.237 -0.237 -0.199 0.295 ... #>   .. .. ..$ : num [1:7] 0.0457 0.1203 0.1203 0.205 0.2378 ... #>   .. .. [list output truncated] #>   ..$ _pv_num_types           : num 7 #>   ..$ predictions             : num [1:100, 1] 0.1017 0.0629 -0.0916 -0.1134 0.0676 ... #>   ..$ variance.estimates      : num[0 , 0 ]  #>   ..$ debiased.error          : num [1:100, 1] 0.0521 0.0654 0.6136 3.7177 1.6982 ... #>   ..$ excess.error            : num [1:100, 1] 0.00211 0.00225 0.00213 0.00569 0.00172 ... #>   ..$ seed                    : num 9.8e+08 #>   ..$ num.threads             : num 0 #>   ..$ ci.group.size           : num 2 #>   ..$ X.orig                  : num [1:100, 1:5] 0.9354 0.2134 -0.0381 -0.9358 -0.1557 ... #>   ..$ Y.orig                  : num [1:100] -0.527 0.234 -0.771 1.807 -1.437 ... #>   ..$ W.orig                  : num [1:100] 1 1 1 0 1 0 1 0 0 1 ... #>   ..$ Y.hat                   : num [1:100] -0.3849 -0.0265 -0.0267 -0.1986 -0.2333 ... #>   ..$ W.hat                   : num [1:100] 0.509 0.435 0.478 0.639 0.621 ... #>   ..$ clusters                : num(0)  #>   ..$ equalize.cluster.weights: logi FALSE #>   ..$ tunable.params          :List of 7 #>   .. ..$ sample.fraction     : num 0.5 #>   .. ..$ mtry                : num 5 #>   .. ..$ min.node.size       : num 5 #>   .. ..$ honesty.fraction    : num 0.5 #>   .. ..$ honesty.prune.leaves: logi TRUE #>   .. ..$ alpha               : num 0.05 #>   .. ..$ imbalance.penalty   : num 0 #>   ..$ has.missing.values      : logi FALSE #>   ..- attr(*, \"class\")= chr [1:2] \"causal_forest\" \"grf\" #>  $ policies          : num [1:100, 1:21] 0 0 0 0 0 0 0 0 0 0 ... #>  $ batch_indices     :List of 20 #>   ..$ 1 : int [1:5] 31 60 94 27 88 #>   ..$ 2 : int [1:5] 4 71 80 83 12 #>   ..$ 3 : int [1:5] 15 22 62 45 18 #>   ..$ 4 : int [1:5] 57 64 25 2 43 #>   ..$ 5 : int [1:5] 97 13 69 17 87 #>   ..$ 6 : int [1:5] 99 40 92 75 14 #>   ..$ 7 : int [1:5] 44 58 47 90 35 #>   ..$ 8 : int [1:5] 30 7 91 95 78 #>   ..$ 9 : int [1:5] 41 93 54 86 29 #>   ..$ 10: int [1:5] 6 36 61 100 67 #>   ..$ 11: int [1:5] 70 63 50 74 52 #>   ..$ 12: int [1:5] 9 73 66 39 33 #>   ..$ 13: int [1:5] 32 77 19 10 3 #>   ..$ 14: int [1:5] 48 85 42 46 96 #>   ..$ 15: int [1:5] 38 68 72 79 16 #>   ..$ 16: int [1:5] 56 11 81 28 59 #>   ..$ 17: int [1:5] 51 34 37 82 20 #>   ..$ 18: int [1:5] 55 5 84 65 89 #>   ..$ 19: int [1:5] 98 53 49 1 8 #>   ..$ 20: int [1:5] 26 24 23 21 76"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"advanced-configuration","dir":"Articles","previous_headings":"","what":"üõ†Ô∏è Advanced Configuration","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"can plug custom training/prediction functions: can also provide advanced model_params like: neural networks:","code":"custom_fit <- function(X, Y, D) { ... } custom_predict <- function(model, X, D) { ... } list(num.trees = 200)  # for grf::causal_forest fnn_params <- list(   input_layer = list(units = 64, activation = 'relu', input_shape = c(ncol(X))),   layers = list(list(units = 32, activation = 'relu')),   output_layer = list(units = 1, activation = 'linear'),   compile_args = list(optimizer = 'adam', loss = 'mse'),   fit_params = list(epochs = 5, batch_size = 32, verbose = 0) )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"‚ö° Parallelization","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Set parallelize_batch = TRUE enable foreach-based parallel training (e.g.¬†across 4 cores):","code":"learning_result <- cram_learning(   X, D, Y, batch,   model_type = \"s_learner\",   learner_type = \"ridge\",   parallelize_batch = TRUE,   n_cores = 4 )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"cram-ml","dir":"Articles","previous_headings":"","what":"Cram ML","title":"Cram ML","text":"article ‚ÄúIntroduction & Cram Policy Part 1‚Äù, introduced Cram method, enables simultaneous learning evaluation binary policy. section, extend framework machine learning tasks cram_ml() function.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"output-of-cram-ml","dir":"Articles","previous_headings":"Cram ML","what":"Output of Cram ML","title":"Cram ML","text":"Cram ML outputs Expected Loss Estimate, refers following statistical quantity:R(œÄÃÇ)=ùîºXÃÉ‚àºD[L(XÃÉ,œÄÃÇ)], R(\\hat{\\pi}) = \\mathbb{E}_{\\tilde{X} \\sim D} \\left[ L(\\tilde{X}, \\hat{\\pi}) \\right],  Expected Loss Estimate represents average loss incurred model, trained given data sample, deployed across entire population. Cram framework, corresponds estimating learned model generalizes unseen data‚Äî.e., performs new observations XÃÉ\\tilde{X} drawn true data-generating distribution DD, independently training data. expected loss serves population-level performance metric (analogous policy value policy learning), Cram provides consistent, low-bias estimate quantity combining models trained sequential batches evaluating held-observations.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"built-in-model","dir":"Articles","previous_headings":"Cram ML","what":"Built-in Model","title":"Cram ML","text":"illustrate use cram_ml(), begin generating synthetic dataset regression task. data consists three independent covariates continuous outcome. section illustrates use cram_ml() built-modeling options available cramR package. function integrates caret framework, allowing users specify learning algorithm, loss function, batching strategy evaluate model performance. Beyond caret, cram_ml() also supports fully custom model training, prediction, loss functions, making suitable virtually machine learning task ‚Äî including regression classification. cram_ml() function offers extensive flexibility loss_name caret_params arguments.","code":"set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"loss_name-argument","dir":"Articles","previous_headings":"Cram ML > Built-in Model","what":"loss_name argument","title":"Cram ML","text":"loss_name argument specifies performance metric used evaluate model batch. Note Cram needs calculate individual losses (.e.¬†map data point prediction loss value) internally averaged across batches observations form Expected Loss Estimate. Depending task, losses interpreted follows: denote xix_i data point œÄÃÇk\\hat{\\pi}_k model trained first k batches data illustrate individual losses computed using built-loss names package.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"regression-losses","dir":"Articles","previous_headings":"Cram ML > Built-in Model > loss_name argument","what":"Regression Losses","title":"Cram ML","text":"Squared Error (\"se\"):L(xi,œÄÃÇk)=(yÃÇ‚àíyi)2 L(x_i, \\hat{\\pi}_k) = (\\hat{y}_i - y_i)^2  Measures squared difference predicted actual outcomes. Absolute Error (\"ae\"):L(xi,œÄÃÇk)=|yÃÇ‚àíyi| L(x_i, \\hat{\\pi}_k) = |\\hat{y}_i - y_i|  Captures magnitude prediction error, regardless direction.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"classification-losses","dir":"Articles","previous_headings":"Cram ML > Built-in Model > loss_name argument","what":"Classification Losses","title":"Cram ML","text":"Accuracy (\"accuracy\"):L(xi,œÄÃÇk)=1{yÃÇ=yi} L(x_i, \\hat{\\pi}_k) = 1\\{\\hat{y}_i = y_i\\}  performance metric loss - Cram allows define performance metric want estimate accuracy built-example. metric 1 correct predictions, 0 incorrect ones. Logarithmic Loss (\"logloss\"): \"logloss\" loss function measures well predicted class probabilities align true class labels. applies binary multiclass classification tasks. given observation ii, let: yi‚àà{c1,c2,‚Ä¶,cK}y_i \\\\{c_1, c_2, \\dots, c_K\\} true class label, pÃÇk(,c)\\hat{p}_k(, c) predicted probability assigned class cc model. individual log loss computed : L(xi,œÄÃÇk)=‚àílog(pÃÇk(,yi)) L(x_i, \\hat{\\pi}_k) = -\\log\\left( \\hat{p}_k(, y_i) \\right)  , take negative log probability assigned true class.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"custom-loss-functions","dir":"Articles","previous_headings":"Cram ML > Built-in Model > loss_name argument","what":"Custom Loss Functions","title":"Cram ML","text":"Users can also define custom loss function providing custom_loss(predictions, data) function returns vector individual losses. allows evaluation complex models domain-specific metrics (details Custom Model part )","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"caret_params-argument","dir":"Articles","previous_headings":"Cram ML > Built-in Model","what":"caret_params argument","title":"Cram ML","text":"caret_params list defines model trained using caret package. can include argument supported caret::train(), allowing full control model specification tuning. Common components include: method: machine learning algorithm (e.g., \"lm\" linear regression, \"rf\" random forest, \"xgbTree\" XGBoost, \"svmLinear\" support vector machines) trControl: resampling strategy (e.g., trainControl(method = \"cv\", number = 5) 5-fold cross-validation, \"none\" training without resampling) tuneGrid: grid hyperparameters tuning (e.g., expand.grid(mtry = c(2, 3, 4))) metric: model selection metric used tuning (e.g., \"RMSE\" \"Accuracy\") preProcess: optional preprocessing steps (e.g., centering, scaling) importance: logical flag compute variable importance (useful tree-based models) Refer full documentation caret model training tuning complete list supported arguments options.","code":"caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"se\",   caret_params = caret_params_lm ) print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.86429 #> 2 Expected Loss Standard Error  0.73665 #> 3       Expected Loss CI Lower -0.57952 #> 4       Expected Loss CI Upper  2.30809 #>  #> $interactive_table #>  #> $final_ml_model #> Linear Regression  #>  #> 100 samples #>   3 predictor #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"case-of-categorical-target-variable","dir":"Articles","previous_headings":"Cram ML > Built-in Model","what":"Case of categorical target variable","title":"Cram ML","text":"cram_ml() function can also used classification tasks, whether predicting hard labels class probabilities. controlled via classify argument loss_name. , demonstrate two typical use cases. Also note data inputs needs numeric types, hence Y categorical, contain numeric values representing class observation. need use type factor cram_ml().","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"case-1-predicting-class-labels","dir":"Articles","previous_headings":"Cram ML > Built-in Model > Case of categorical target variable","what":"Case 1: Predicting Class Labels","title":"Cram ML","text":"case, model outputs hard predictions (labels, e.g.¬†0, 1, 2 etc.), metric used classification accuracy‚Äîproportion correctly predicted labels. Use loss_name = \"accuracy\" Set classProbs = FALSE trainControl Set classify = TRUE cram_ml()","code":"set.seed(42)  # Generate binary classification dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rbinom(nrow(X_data), 1, 0.5) data_df <- data.frame(X_data, Y = Y_data)  # Define caret parameters: predict labels (default behavior) caret_params_rf <- list(   method = \"rf\",   trControl = trainControl(method = \"none\") )  # Run CRAM ML with accuracy as loss result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"accuracy\",   caret_params = caret_params_rf,   classify = TRUE )  print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.48750 #> 2 Expected Loss Standard Error  0.43071 #> 3       Expected Loss CI Lower -0.35668 #> 4       Expected Loss CI Upper  1.33168 #>  #> $interactive_table #>  #> $final_ml_model #> Random Forest  #>  #> 100 samples #>   3 predictor #>   2 classes: 'class0', 'class1'  #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"case-2-predicting-class-probabilities","dir":"Articles","previous_headings":"Cram ML > Built-in Model > Case of categorical target variable","what":"Case 2: Predicting Class Probabilities","title":"Cram ML","text":"setup, model outputs class probabilities, loss evaluated using logarithmic loss (logloss)‚Äîstandard metric probabilistic classification. Use loss_name = \"logloss\" Set classProbs = TRUE trainControl Set classify = TRUE cram_ml() Together, arguments allow users apply cram_ml() using wide variety built-machine learning models losses. users need go beyond built-choices, also provide next section friendly workflow specify custom models losses cram_ml().","code":"set.seed(42)  # Generate binary classification dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rbinom(nrow(X_data), 1, 0.5) data_df <- data.frame(X_data, Y = Y_data)  # Define caret parameters for probability output caret_params_rf_probs <- list(   method = \"rf\",   trControl = trainControl(method = \"none\", classProbs = TRUE) )  # Run CRAM ML with logloss as the evaluation loss result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"logloss\",   caret_params = caret_params_rf_probs,   classify = TRUE )  print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.93225 #> 2 Expected Loss Standard Error  0.48118 #> 3       Expected Loss CI Lower -0.01085 #> 4       Expected Loss CI Upper  1.87534 #>  #> $interactive_table #>  #> $final_ml_model #> Random Forest  #>  #> 100 samples #>   3 predictor #>   2 classes: 'class0', 'class1'  #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"custom-model","dir":"Articles","previous_headings":"Cram ML","what":"Custom Model","title":"Cram ML","text":"addition using built-learners via caret, cram_ml() also supports fully custom model workflows. can specify : Model fitting function (custom_fit) Prediction function (custom_predict) Loss function (custom_loss) offers maximum flexibility, allowing CRAM evaluate learning model performance criterion, including regression, classification, even unsupervised losses clustering distance.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"custom_fitdata----","dir":"Articles","previous_headings":"Cram ML > Custom Model","what":"1. custom_fit(data, ...)","title":"Cram ML","text":"function takes data frame returns fitted model. may define additional arguments hyperparameters training settings. data: data frame includes predictors outcome variable Y. Example: basic linear model fit three predictors:","code":"custom_fit <- function(data) {   lm(Y ~ x1 + x2 + x3, data = data) }"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"custom_predictmodel-data","dir":"Articles","previous_headings":"Cram ML > Custom Model","what":"2. custom_predict(model, data)","title":"Cram ML","text":"function generates predictions fitted model new data. returns numeric vector predicted outcomes. model: fitted model returned custom_fit() data: data frame new observations (typically including original predictors) Example: Extract predictors apply standard predict() call:","code":"custom_predict <- function(model, data) {   predictors_only <- data[, setdiff(names(data), \"Y\"), drop = FALSE]   predict(model, newdata = predictors_only) }"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"custom_losspredictions-data","dir":"Articles","previous_headings":"Cram ML > Custom Model","what":"3. custom_loss(predictions, data)","title":"Cram ML","text":"function defines loss metric used evaluate model predictions. return numeric vector individual losses, one per observation. internally aggregated cram_ml() compute overall performance. predictions: numeric vector predicted values model data: data frame containing true outcome values (Y) Example: Define custom loss function using Squared Error (SE)","code":"custom_loss <- function(predictions, data) {   actuals <- data$Y   se_loss <- (predictions - actuals)^2   return(se_loss) }"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"use-cram_ml-with-custom-functions","dir":"Articles","previous_headings":"Cram ML > Custom Model","what":"4. Use cram_ml() with Custom Functions","title":"Cram ML","text":"defined custom training, prediction, loss functions, can pass directly cram_ml() shown , note caret_params loss_name used built-functionalities now NULL:","code":"set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data)  result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   custom_fit = custom_fit,   custom_predict = custom_predict,   custom_loss = custom_loss ) print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.86429 #> 2 Expected Loss Standard Error  0.73665 #> 3       Expected Loss CI Lower -0.57952 #> 4       Expected Loss CI Upper  2.30809 #>  #> $interactive_table #>  #> $final_ml_model #>  #> Call: #> lm(formula = Y ~ x1 + x2 + x3, data = data) #>  #> Coefficients: #> (Intercept)           x1           x2           x3   #>    0.031503     0.057754     0.008829    -0.031611"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"what-is-ml_learning","dir":"Articles","previous_headings":"","what":"üîç What is ml_learning()?","title":"Using ml_learning() for Batch-wise Machine Learning","text":"ml_learning() function implements batch-wise machine learning supervised unsupervised tasks. supports: caret::train() compatible model (regression, classification, clustering, etc.) Custom model training + prediction via custom_fit, custom_predict Custom built-loss functions (e.g., MSE, logloss, accuracy) Optional parallel processing (parallelize_batch = TRUE) ‚Äôs flexible utility powers cram_ml() general-purpose ML workflows, also useful customizing training pipelines.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"when-to-use-ml_learning","dir":"Articles","previous_headings":"","what":"üß† When to use ml_learning()?","title":"Using ml_learning() for Batch-wise Machine Learning","text":"want evaluate model‚Äôs performance multiple growing batches data want track loss evolution time need parallelized learning, e.g., neural nets larger datasets ‚Äôre experimenting custom ML pipelines, loss functions, unsupervised tasks","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"example-linear-regression-with-mse","dir":"Articles","previous_headings":"","what":"üìò Example: Linear Regression with MSE","title":"Using ml_learning() for Batch-wise Machine Learning","text":"","code":"# Simulate regression data set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data) # Define model settings caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\")  # No cross-validation )  # Define batch count nb_batch <- 5 # Run ML learning result_lm <- ml_learning(   data = data_df,   formula = Y ~ .,   batch = nb_batch,   loss_name = \"se\",   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"output-structure","dir":"Articles","previous_headings":"","what":"üì¶ Output Structure","title":"Using ml_learning() for Batch-wise Machine Learning","text":"returned object list :","code":"str(result_lm) #> List of 3 #>  $ final_ml_model:List of 24 #>   ..$ method      : chr \"lm\" #>   ..$ modelInfo   :List of 13 #>   .. ..$ label     : chr \"Linear Regression\" #>   .. ..$ library   : NULL #>   .. ..$ loop      : NULL #>   .. ..$ type      : chr \"Regression\" #>   .. ..$ parameters:'data.frame':    1 obs. of  3 variables: #>   .. .. ..$ parameter: chr \"intercept\" #>   .. .. ..$ class    : chr \"logical\" #>   .. .. ..$ label    : chr \"intercept\" #>   .. ..$ grid      :function (x, y, len = NULL, search = \"grid\")   #>   .. ..$ fit       :function (x, y, wts, param, lev, last, classProbs, ...)   #>   .. ..$ predict   :function (modelFit, newdata, submodels = NULL)   #>   .. ..$ prob      : NULL #>   .. ..$ predictors:function (x, ...)   #>   .. ..$ tags      : chr [1:2] \"Linear Regression\" \"Accepts Case Weights\" #>   .. ..$ varImp    :function (object, ...)   #>   .. ..$ sort      :function (x)   #>   ..$ modelType   : chr \"Regression\" #>   ..$ results     :'data.frame': 0 obs. of  4 variables: #>   .. ..$ RMSE     : num(0)  #>   .. ..$ Rsquared : num(0)  #>   .. ..$ MAE      : num(0)  #>   .. ..$ intercept: logi(0)  #>   ..$ pred        : NULL #>   ..$ bestTune    :'data.frame': 1 obs. of  1 variable: #>   .. ..$ intercept: logi TRUE #>   ..$ call        : language train.formula(form = Y ~ ., data = list(x1 = c(0.276550747291463, 2.28664539270111,  -0.727292059474465, -0.60892| __truncated__ ... #>   ..$ dots        : list() #>   ..$ metric      : chr \"RMSE\" #>   ..$ control     :List of 28 #>   .. ..$ method           : chr \"none\" #>   .. ..$ number           : num 25 #>   .. ..$ repeats          : logi NA #>   .. ..$ search           : chr \"grid\" #>   .. ..$ p                : num 0.75 #>   .. ..$ initialWindow    : NULL #>   .. ..$ horizon          : num 1 #>   .. ..$ fixedWindow      : logi TRUE #>   .. ..$ skip             : num 0 #>   .. ..$ verboseIter      : logi FALSE #>   .. ..$ returnData       : logi TRUE #>   .. ..$ returnResamp     : chr \"final\" #>   .. ..$ savePredictions  : chr \"none\" #>   .. ..$ classProbs       : logi FALSE #>   .. ..$ summaryFunction  :function (data, lev = NULL, model = NULL)   #>   .. ..$ selectionFunction: chr \"best\" #>   .. ..$ preProcOptions   :List of 6 #>   .. .. ..$ thresh   : num 0.95 #>   .. .. ..$ ICAcomp  : num 3 #>   .. .. ..$ k        : num 5 #>   .. .. ..$ freqCut  : num 19 #>   .. .. ..$ uniqueCut: num 10 #>   .. .. ..$ cutoff   : num 0.9 #>   .. ..$ sampling         : NULL #>   .. ..$ index            :List of 1 #>   .. .. ..$ Resample1: int [1:100] 1 2 3 4 5 6 7 8 9 10 ... #>   .. ..$ indexOut         :List of 1 #>   .. .. ..$ Resample1: int(0)  #>   .. ..$ indexFinal       : NULL #>   .. ..$ timingSamps      : num 0 #>   .. ..$ predictionBounds : logi [1:2] FALSE FALSE #>   .. ..$ seeds            : logi NA #>   .. ..$ adaptive         :List of 4 #>   .. .. ..$ min     : num 5 #>   .. .. ..$ alpha   : num 0.05 #>   .. .. ..$ method  : chr \"gls\" #>   .. .. ..$ complete: logi TRUE #>   .. ..$ trim             : logi FALSE #>   .. ..$ allowParallel    : logi TRUE #>   .. ..$ yLimits          : num [1:2] -1.89 2.63 #>   ..$ finalModel  :List of 17 #>   .. ..$ coefficients : Named num [1:4] 0.0315 0.05775 0.00883 -0.03161 #>   .. .. ..- attr(*, \"names\")= chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   .. ..$ residuals    : Named num [1:100] 1.118 -0.859 -0.15 0.132 -0.127 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. ..$ effects      : Named num [1:100] -0.3294 -0.6472 -0.0551 -0.3156 -0.1693 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"(Intercept)\" \"x1\" \"x2\" \"x3\" ... #>   .. ..$ rank         : int 4 #>   .. ..$ fitted.values: Named num [1:100] 0.06922 0.10013 -0.00358 0.02098 0.12662 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. ..$ assign       : int [1:4] 0 1 2 3 #>   .. ..$ qr           :List of 5 #>   .. .. ..$ qr   : num [1:100, 1:4] -10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. .. .. .. ..$ : chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"assign\")= int [1:4] 0 1 2 3 #>   .. .. ..$ qraux: num [1:4] 1.1 1.22 1.09 1.04 #>   .. .. ..$ pivot: int [1:4] 1 2 3 4 #>   .. .. ..$ tol  : num 1e-07 #>   .. .. ..$ rank : int 4 #>   .. .. ..- attr(*, \"class\")= chr \"qr\" #>   .. ..$ df.residual  : int 96 #>   .. ..$ xlevels      : Named list() #>   .. ..$ call         : language lm(formula = .outcome ~ ., data = dat) #>   .. ..$ terms        :Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. .. ..- attr(*, \"response\")= int 1 #>   .. .. .. ..- attr(*, \".Environment\")=<environment: 0x000001967e9e3748>  #>   .. .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. ..$ model        :'data.frame': 100 obs. of  4 variables: #>   .. .. ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   .. .. ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   .. .. ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   .. .. ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>   .. .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. .. .. ..- attr(*, \"response\")= int 1 #>   .. .. .. .. ..- attr(*, \".Environment\")=<environment: 0x000001967e9e3748>  #>   .. .. .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. ..$ xNames       : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. ..$ problemType  : chr \"Regression\" #>   .. ..$ tuneValue    :'data.frame': 1 obs. of  1 variable: #>   .. .. ..$ intercept: logi TRUE #>   .. ..$ obsLevels    : logi NA #>   .. ..$ param        : list() #>   .. ..- attr(*, \"class\")= chr \"lm\" #>   ..$ preProcess  : NULL #>   ..$ trainingData:'data.frame': 100 obs. of  4 variables: #>   .. ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   .. ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   .. ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   .. ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>   ..$ ptype       :'data.frame': 0 obs. of  3 variables: #>   .. ..$ x1: num(0)  #>   .. ..$ x2: num(0)  #>   .. ..$ x3: num(0)  #>   ..$ resample    : NULL #>   ..$ resampledCM : NULL #>   ..$ perfNames   : chr [1:3] \"RMSE\" \"Rsquared\" \"MAE\" #>   ..$ maximize    : logi FALSE #>   ..$ yLimits     : num [1:2] -1.89 2.63 #>   ..$ times       :List of 3 #>   .. ..$ everything: 'proc_time' Named num [1:5] 0.02 0 0 NA NA #>   .. .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   .. ..$ final     : 'proc_time' Named num [1:5] 0 0 0 NA NA #>   .. .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   .. ..$ prediction: logi [1:3] NA NA NA #>   ..$ levels      : logi NA #>   ..$ terms       :Classes 'terms', 'formula'  language Y ~ x1 + x2 + x3 #>   .. .. ..- attr(*, \"variables\")= language list(Y, x1, x2, x3) #>   .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. ..- attr(*, \"response\")= int 1 #>   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>   .. .. ..- attr(*, \"predvars\")= language list(Y, x1, x2, x3) #>   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. ..- attr(*, \"names\")= chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>   ..$ coefnames   : chr [1:3] \"x1\" \"x2\" \"x3\" #>   ..$ xlevels     : Named list() #>   ..- attr(*, \"class\")= chr [1:2] \"train\" \"train.formula\" #>  $ losses        : num [1:100, 1:6] 0 0 0 0 0 0 0 0 0 0 ... #>  $ batch_indices :List of 5 #>   ..$ 1: int [1:20] 56 12 65 34 24 4 9 97 7 1 ... #>   ..$ 2: int [1:20] 98 3 8 64 96 17 63 44 49 28 ... #>   ..$ 3: int [1:20] 62 18 30 21 29 87 99 74 75 90 ... #>   ..$ 4: int [1:20] 69 50 20 52 85 86 19 77 45 36 ... #>   ..$ 5: int [1:20] 42 47 51 40 22 39 83 82 72 68 ..."},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"custom-models-optional","dir":"Articles","previous_headings":"","what":"üîß Custom Models (Optional)","title":"Using ml_learning() for Batch-wise Machine Learning","text":"can plug models: run:","code":"custom_fit <- function(data) {   model <- glm(Y ~ ., data = data)   return(model) }  custom_predict <- function(model, data) {   return(predict(model, newdata = data)) }  custom_loss <- function(preds, data) {   return((data$Y - preds)^2) } ml_learning(data_df, batch = nb_batch, custom_fit = custom_fit,             custom_predict = custom_predict, custom_loss = custom_loss)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"‚ö° Parallelization","title":"Using ml_learning() for Batch-wise Machine Learning","text":"accelerate training, set:","code":"ml_learning(data_df,             formula = Y ~ .,             batch = nb_batch,             caret_params = caret_params_lm,             loss_name = \"mse\",             parallelize_batch = TRUE,             n_cores = 4)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Using ml_learning() for Batch-wise Machine Learning","text":"cram_ml() ‚Äî wrapper full pipeline (estimation + CI) cram_policy() ‚Äî causal policy learning caret::train() ‚Äî underlying training API","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"introduction-what-is-the-cram-method","dir":"Articles","previous_headings":"","what":"Introduction: What is the Cram Method?","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Cram method powerful approach simultaneously learning evaluating decision rules, individualized treatment rules (ITRs), data. Common applications include healthcare (treat), pricing advertising (target much charge), policy (support). Unlike traditional approaches like sample splitting cross-validation, waste part data evaluation , Cram reuses available data efficiently. key distinction cross-validation Cram evaluates final learned model, rather averaging performance across multiple models trained different data splits. Cram: Simultaneously trains model evaluates final learned decision rule using available data improve statistical efficiency precision‚Äîunlike cross-validation sample splitting, reserve part data evaluation . Learns cumulative batches, using new round data refine model check whether ‚Äôs actually improving‚Äîensuring learning translates meaningful gains. Estimates expected outcome across entire population policy learned data sample applied everyone population, just data sample (statistical quantity called ‚Äúpolicy value‚Äù), allows user assess learned policy generalize beyond data sample. Think Cram like cram school: learn bit, test bit, repeat ‚Äî getting better constantly self-evaluating.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"the-cram-workflow","dir":"Articles","previous_headings":"","what":"The Cram Workflow","title":"Using cram_policy() for Policy Learning and Evaluation","text":"core idea Cram method visualized: procedure ensures update backed performance testing, enabling learning evaluation one pass data. Note: schematic represents Cram estimates difference policy value (see definition policy value ) relative baseline policy - example baseline policy healthcare treat nobody (-zeros) randomly treat individuals (assign 1 treatment 0 treatment randomly); policy value difference ‚ÄúDelta‚Äù gives much better (worse) policy learned Cram data relative baseline policy - Cram can also used estimate policy value learned policy directly, without need specify baseline policy (presented part introduction available outputs main functions Cram).","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"the-cram_policy-function","dir":"Articles","previous_headings":"","what":"The cram_policy() Function","title":"Using cram_policy() for Policy Learning and Evaluation","text":"cram_policy() function cramR implements Cram framework binary treatment policy learning.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"key-features-of-cram_policy","dir":"Articles","previous_headings":"The cram_policy() Function","what":"üîë Key Features of cram_policy()","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Model-Agnostic Flexibility: Supports variety learning strategies, including causal_forest, s_learner, m_learner, well fully customizable learners via user-defined fit predict functions. Efficient Design: Built top data.table fast, memory-efficient computation, optional support parallel batch training scale across larger datasets.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"example-running-cram-policy-on-simulated-data","dir":"Articles","previous_headings":"","what":"Example: Running Cram Policy on simulated data","title":"Using cram_policy() for Policy Learning and Evaluation","text":"","code":"library(data.table) # Function to generate sample data with heterogeneous treatment effects: # - Positive effect group # - Neutral effect group # - Adverse effect group generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),                 # Binary variable     discrete = sample(1:5, n, replace = TRUE),  # Discrete variable     continuous = rnorm(n)                       # Continuous variable   )    # Binary treatment assignment (50% treated)   D <- rbinom(n, 1, 0.5)    # Define heterogeneous treatment effects based on X   treatment_effect <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect            -1,            0.1)                                   # Group 2: Neutral effect   )    # Outcome depends on treatment effect + noise   Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +     (1 - D) * rnorm(n)    return(list(X = X, D = D, Y = Y)) }  # Generate a sample dataset set.seed(123) n <- 1000 data <- generate_data(n) X <- data$X D <- data$D Y <- data$Y # Options for batch: # Either an integer specifying the number of batches or a vector/list of batch assignments for all individuals batch <- 20  # Model type for estimating treatment effects # Options for model_type: 'causal_forest', 's_learner', 'm_learner' # Note: you can also set model_type to NULL and specify custom_fit and custom_predict to use your custom model model_type <- \"causal_forest\"    # Options for learner_type: # if model_type == 'causal_forest', choose NULL # if model_type == 's_learner' or 'm_learner', choose between 'ridge', 'fnn' and 'caret' learner_type <- NULL    # Baseline policy to compare against (list of 0/1 for each individual) # Options for baseline_policy: # A list representing the baseline policy assignment for each individual. # If NULL, a default baseline policy of zeros is created. # Examples of baseline policy:  # - All-control baseline: as.list(rep(0, nrow(X))) or NULL # - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE)) baseline_policy <- as.list(rep(0, nrow(X)))    # Whether to parallelize batch processing (i.e. the cram method learns T policies, with T the number of batches. # They are learned in parallel when parallelize_batch is TRUE # vs. learned sequentially using the efficient data.table structure when parallelize_batch is FALSE, recommended for light weight training). # Defaults to FALSE. parallelize_batch <- FALSE      # Model-specific parameters (more details in the article \"Quick Start\") # Examples: NULL defaults to the following: # - causal_forest: list(num.trees = 100) # - ridge: list(alpha = 1) # - caret: list(formula = Y ~ ., caret_params = list(method = \"lm\", trControl = trainControl(method = \"none\"))) # - fnn (Feedforward Neural Network): see below # input_shape <- if (model_type == \"s_learner\") ncol(X) + 1 else ncol(X) # default_model_params <- list( #       input_layer = list(units = 64, activation = 'relu', input_shape = input_shape), #       layers = list( #         list(units = 32, activation = 'relu') #       ), #       output_layer = list(units = 1, activation = 'linear'), #       compile_args = list(optimizer = 'adam', loss = 'mse'), #       fit_params = list(epochs = 5, batch_size = 32, verbose = 0) #     ) model_params <- NULL     # Significance level for confidence intervals (default = 95%) alpha <- 0.05    # Run the Cram policy method result <- cram_policy(   X, D, Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params,   alpha = alpha )  # Display the results print(result) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"interpreting-results","dir":"Articles","previous_headings":"","what":"Interpreting Results","title":"Using cram_policy() for Policy Learning and Evaluation","text":"output cram_policy() includes: raw_results: data frame summarizing key evaluation metrics: Delta Estimate: estimated improvement outcomes using final learned policy compared baseline (e.g., treatment treat-). Delta Standard Error confidence interval bounds: Reflect uncertainty around delta estimate. Policy Value Estimate: estimated average outcome final learned policy applied across population. Policy Value Standard Error confidence interval bounds: Reflect uncertainty policy value estimate. Proportion Treated: fraction population treated learned policy. interactive_table: dynamic, scrollable version raw_results easier exploration filtering. final_policy_model: trained policy model object , fitted according specified model_type, learner_type, user-provided custom_fit custom_predict (details article ‚ÄúQuick Start‚Äù). object can used analysis applying learned policy new data. can inspect apply learned model new data.","code":"result$raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 result$interactive_table class(result$final_policy_model) #> [1] \"causal_forest\" \"grf\" summary(result$final_policy_model) #>                          Length Class  Mode    #> _ci_group_size              1   -none- numeric #> _num_variables              1   -none- numeric #> _num_trees                  1   -none- numeric #> _root_nodes               100   -none- list    #> _child_nodes              100   -none- list    #> _leaf_samples             100   -none- list    #> _split_vars               100   -none- list    #> _split_values             100   -none- list    #> _drawn_samples            100   -none- list    #> _send_missing_left        100   -none- list    #> _pv_values                100   -none- list    #> _pv_num_types               1   -none- numeric #> predictions              1000   -none- numeric #> variance.estimates          0   -none- numeric #> debiased.error           1000   -none- numeric #> excess.error             1000   -none- numeric #> seed                        1   -none- numeric #> num.threads                 1   -none- numeric #> ci.group.size               1   -none- numeric #> X.orig                   3000   -none- numeric #> Y.orig                   1000   -none- numeric #> W.orig                   1000   -none- numeric #> Y.hat                    1000   -none- numeric #> W.hat                    1000   -none- numeric #> clusters                    0   -none- numeric #> equalize.cluster.weights    1   -none- logical #> tunable.params              7   -none- list    #> has.missing.values          1   -none- logical"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"visual-summary-and-notes","dir":"Articles","previous_headings":"","what":"Visual Summary and Notes","title":"Using cram_policy() for Policy Learning and Evaluation","text":"visualization summarizes multiple evaluations across iterations contribute full Cram estimate. Notes: Batching: can pass number (e.g., batch = 5) custom vector control data split. Parallelization: Enable parallelize_batch = TRUE. Custom Learners: Use custom_fit custom_predict plug estimator. (details article ‚ÄúQuick Start‚Äù)","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"why-cram","dir":"Articles","previous_headings":"","what":"Why Cram?","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Compared classic evaluation methods:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Jia, Z., Imai, K., & Li, M. L. (2024). Cram Method Efficient Simultaneous Learning Evaluation. arXiv:2403.07031. K√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners estimating heterogeneous treatment effects using machine learning. Proceedings National Academy Sciences United States America, 116(10), 4156‚Äì4165. https://doi.org/10.1073/pnas.1804597116 Wager, S., & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. Journal American Statistical Association, 113(523), 1228-1242. Athey, S., & Imbens, G. (2016). Recursive partitioning heterogeneous causal effects. Proceedings National Academy Sciences, 113(27), 7353-7360.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"introduction-what-is-the-cram-method","dir":"Articles","previous_headings":"","what":"Introduction: What is the Cram Method?","title":"Introduction & Cram Policy part 1","text":"Cram method powerful approach simultaneously learning evaluating decision rules, individualized treatment rules (ITRs), data. Common applications include healthcare (treat), pricing advertising (target much charge), policy (support). Unlike traditional approaches like sample splitting cross-validation, waste part data evaluation , Cram reuses available data efficiently. key distinction cross-validation Cram evaluates final learned model, rather averaging performance across multiple models trained different data splits. Cram: Simultaneously trains model evaluates final learned decision rule using available data improve statistical efficiency precision‚Äîunlike cross-validation sample splitting, reserve part data evaluation . Learns cumulative batches, using new round data refine model check whether ‚Äôs actually improving‚Äîensuring learning translates meaningful gains. Estimates expected outcome across entire population policy learned data sample applied everyone population, just data sample (statistical quantity called ‚Äúpolicy value‚Äù), allows user assess learned policy generalize beyond data sample. üõ†Ô∏è Think Cram like cram school: learn bit, test bit, repeat ‚Äî getting better constantly self-evaluating.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"the-cram-workflow","dir":"Articles","previous_headings":"","what":"The Cram Workflow","title":"Introduction & Cram Policy part 1","text":"core idea Cram method visualized: procedure ensures update backed performance testing, enabling learning evaluation one pass data. Note: schematic represents Cram estimates difference policy value (see definition policy value ) relative baseline policy - example baseline policy healthcare treat nobody (-zeros) randomly treat individuals (assign 1 treatment 0 treatment randomly); policy value difference ‚ÄúDelta‚Äù gives much better (worse) policy learned Cram data relative baseline policy - Cram can also used estimate policy value learned policy directly, without need specify baseline policy (presented part introduction available outputs main functions Cram; see result table ).","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"the-cram_policy-function","dir":"Articles","previous_headings":"","what":"The cram_policy() Function","title":"Introduction & Cram Policy part 1","text":"cram_policy() function cramR implements Cram framework binary treatment policy learning.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"key-features-of-cram_policy","dir":"Articles","previous_headings":"The cram_policy() Function","what":"üîë Key Features of cram_policy()","title":"Introduction & Cram Policy part 1","text":"Model-Agnostic Flexibility: Supports variety learning strategies, including causal_forest, s_learner, m_learner, well fully customizable learners via user-defined fit predict functions. Efficient Design: Built top data.table fast, memory-efficient computation, optional support parallel batch training scale across larger datasets.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"example-running-cram-policy-on-simulated-data","dir":"Articles","previous_headings":"","what":"Example: Running Cram Policy on simulated data","title":"Introduction & Cram Policy part 1","text":"","code":"library(data.table) # Function to generate sample data with heterogeneous treatment effects: # - Positive effect group # - Neutral effect group # - Adverse effect group generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),                 # Binary variable     discrete = sample(1:5, n, replace = TRUE),  # Discrete variable     continuous = rnorm(n)                       # Continuous variable   )    # Binary treatment assignment (50% treated)   D <- rbinom(n, 1, 0.5)    # Define heterogeneous treatment effects based on X   treatment_effect <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect            -1,            0.1)                                   # Group 2: Neutral effect   )    # Outcome depends on treatment effect + noise   Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +     (1 - D) * rnorm(n)    return(list(X = X, D = D, Y = Y)) }  # Generate a sample dataset set.seed(123) n <- 1000 data <- generate_data(n) X <- data$X D <- data$D Y <- data$Y # Options for batch: # Either an integer specifying the number of batches or a vector/list of batch assignments for all individuals batch <- 20  # Model type for estimating treatment effects # Options for model_type: 'causal_forest', 's_learner', 'm_learner' # Note: you can also set model_type to NULL and specify custom_fit and custom_predict to use your custom model model_type <- \"causal_forest\"    # Options for learner_type: # if model_type == 'causal_forest', choose NULL # if model_type == 's_learner' or 'm_learner', choose between 'ridge', 'fnn' and 'caret' learner_type <- NULL    # Baseline policy to compare against (list of 0/1 for each individual) # Options for baseline_policy: # A list representing the baseline policy assignment for each individual. # If NULL, a default baseline policy of zeros is created. # Examples of baseline policy:  # - All-control baseline: as.list(rep(0, nrow(X))) or NULL # - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE)) baseline_policy <- as.list(rep(0, nrow(X)))    # Whether to parallelize batch processing (i.e. the cram method learns T policies, with T the number of batches. # They are learned in parallel when parallelize_batch is TRUE # vs. learned sequentially using the efficient data.table structure when parallelize_batch is FALSE, recommended for light weight training). # Defaults to FALSE. parallelize_batch <- FALSE      # Model-specific parameters (more details in the article \"Cram Policy part 2\") # Examples: NULL defaults to the following: # - causal_forest: list(num.trees = 100) # - ridge: list(alpha = 1) # - caret: list(formula = Y ~ ., caret_params = list(method = \"lm\", trControl = trainControl(method = \"none\"))) # - fnn (Feedforward Neural Network): see below # input_shape <- if (model_type == \"s_learner\") ncol(X) + 1 else ncol(X) # default_model_params <- list( #       input_layer = list(units = 64, activation = 'relu', input_shape = input_shape), #       layers = list( #         list(units = 32, activation = 'relu') #       ), #       output_layer = list(units = 1, activation = 'linear'), #       compile_args = list(optimizer = 'adam', loss = 'mse'), #       fit_params = list(epochs = 5, batch_size = 32, verbose = 0) #     ) model_params <- NULL     # Significance level for confidence intervals (default = 95%) alpha <- 0.05    # Run the Cram policy method result <- cram_policy(   X, D, Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params,   alpha = alpha )  # Display the results print(result) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"interpreting-results","dir":"Articles","previous_headings":"","what":"Interpreting Results","title":"Introduction & Cram Policy part 1","text":"output cram_policy() includes: raw_results: data frame summarizing key evaluation metrics: Delta Estimate: estimated policy value difference .e.¬†improvement outcomes using final learned policy compared baseline (e.g., treatment treat-). Delta Standard Error confidence interval bounds: Reflect uncertainty around delta estimate. Policy Value Estimate: estimated policy value .e.¬†average outcome final learned policy applied across population. Policy Value Standard Error confidence interval bounds: Reflect uncertainty policy value estimate. Proportion Treated: fraction population treated learned policy. interactive_table: dynamic, scrollable version raw_results easier exploration filtering. final_policy_model: trained policy model object , fitted according specified model_type, learner_type, user-provided custom_fit custom_predict (details article ‚ÄúCram Policy part 2‚Äù). object can used analysis applying learned policy new data. can inspect apply learned model new data.","code":"result$raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 result$interactive_table class(result$final_policy_model) #> [1] \"causal_forest\" \"grf\" summary(result$final_policy_model) #>                          Length Class  Mode    #> _ci_group_size              1   -none- numeric #> _num_variables              1   -none- numeric #> _num_trees                  1   -none- numeric #> _root_nodes               100   -none- list    #> _child_nodes              100   -none- list    #> _leaf_samples             100   -none- list    #> _split_vars               100   -none- list    #> _split_values             100   -none- list    #> _drawn_samples            100   -none- list    #> _send_missing_left        100   -none- list    #> _pv_values                100   -none- list    #> _pv_num_types               1   -none- numeric #> predictions              1000   -none- numeric #> variance.estimates          0   -none- numeric #> debiased.error           1000   -none- numeric #> excess.error             1000   -none- numeric #> seed                        1   -none- numeric #> num.threads                 1   -none- numeric #> ci.group.size               1   -none- numeric #> X.orig                   3000   -none- numeric #> Y.orig                   1000   -none- numeric #> W.orig                   1000   -none- numeric #> Y.hat                    1000   -none- numeric #> W.hat                    1000   -none- numeric #> clusters                    0   -none- numeric #> equalize.cluster.weights    1   -none- logical #> tunable.params              7   -none- list    #> has.missing.values          1   -none- logical"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"visual-summary-and-notes","dir":"Articles","previous_headings":"","what":"Visual Summary and Notes","title":"Introduction & Cram Policy part 1","text":"visualization summarizes multiple evaluations across iterations contribute full Cram estimate. Notes: Batching: can pass number (e.g., batch = 5) custom vector control data split. Parallelization: Enable parallelize_batch = TRUE. Custom Learners: Use custom_fit custom_predict plug estimator. (details article ‚ÄúCram Policy part 2‚Äù)","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"why-cram","dir":"Articles","previous_headings":"","what":"Why Cram?","title":"Introduction & Cram Policy part 1","text":"Compared classic learning evaluation methods, terms data usage learning evaluation:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_1.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Introduction & Cram Policy part 1","text":"Jia, Z., Imai, K., & Li, M. L. (2024). Cram Method Efficient Simultaneous Learning Evaluation, URL: https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf. K√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners estimating heterogeneous treatment effects using machine learning. Proceedings National Academy Sciences United States America, 116(10), 4156‚Äì4165. https://doi.org/10.1073/pnas.1804597116 Wager, S., & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. Journal American Statistical Association, 113(523), 1228-1242. Athey, S., & Imbens, G. (2016). Recursive partitioning heterogeneous causal effects. Proceedings National Academy Sciences, 113(27), 7353-7360.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"cram-policy","dir":"Articles","previous_headings":"","what":"Cram Policy","title":"Cram Policy part 2","text":"article ‚ÄúIntroduction & Cram Policy Part 1‚Äù, introduced Cram method, enables simultaneous learning evaluation binary policy. outlined primary parameters cram_policy() function demonstrated application using example dataset. section, provide detailed discussion configure parameters various use cases, depending notably nature dataset specific policy learning goals. begin simulating dataset consisting covariates X, binary treatment assignment D, continuous outcome Y, use demonstrate cram_policy() function.","code":"library(data.table) # Function to generate sample data with heterogeneous treatment effects: # - Positive effect group # - Neutral effect group # - Adverse effect group generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),                 # Binary variable     discrete = sample(1:5, n, replace = TRUE),  # Discrete variable     continuous = rnorm(n)                       # Continuous variable   )    # Binary treatment assignment (50% treated)   D <- rbinom(n, 1, 0.5)    # Define heterogeneous treatment effects based on X   treatment_effect <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect            -1,            0.1)                                   # Group 2: Neutral effect   )    # Outcome depends on treatment effect + noise   Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +     (1 - D) * rnorm(n)    return(list(X = X, D = D, Y = Y)) }  # Generate a sample dataset set.seed(123) n <- 1000 data <- generate_data(n) X <- data$X D <- data$D Y <- data$Y"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"built-in-model","dir":"Articles","previous_headings":"Cram Policy","what":"Built-in Model","title":"Cram Policy part 2","text":"example, demonstrate use built-modeling options provided cramR package. walk key parameters control cram_policy() behaves, including choice model, learner type, baseline policy, batching strategy. parameters allow flexibility configuring learning process depending use case nature dataset.","code":"# Options for batch: # Either an integer specifying the number of batches or a vector/list of batch assignments for all individuals batch <- 20  # Model type for estimating treatment effects # Options for model_type: 'causal_forest', 's_learner', 'm_learner' # Note: you can also set model_type to NULL and specify custom_fit and custom_predict to use your custom model model_type <- \"causal_forest\"    # Options for learner_type: # if model_type == 'causal_forest', choose NULL # if model_type == 's_learner' or 'm_learner', choose between 'ridge', 'fnn' and 'caret' learner_type <- NULL    # Baseline policy to compare against (list of 0/1 for each individual) # Options for baseline_policy: # A list representing the baseline policy assignment for each individual. # If NULL, a default baseline policy of zeros is created. # Examples of baseline policy:  # - All-control baseline: as.list(rep(0, nrow(X))) or NULL # - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE)) baseline_policy <- as.list(rep(0, nrow(X)))    # Whether to parallelize batch processing (i.e. the cram method learns T policies, with T the number of batches. # They are learned in parallel when parallelize_batch is TRUE # vs. learned sequentially using the efficient data.table structure when parallelize_batch is FALSE, recommended for light weight training). # Defaults to FALSE. parallelize_batch <- FALSE      # Model-specific parameters (more details in the article \"Quick Start\") # Examples: NULL defaults to the following: # - causal_forest: list(num.trees = 100) # - ridge: list(alpha = 1) # - caret: list(formula = Y ~ ., caret_params = list(method = \"lm\", trControl = trainControl(method = \"none\"))) # - fnn (Feedforward Neural Network): see below # input_shape <- if (model_type == \"s_learner\") ncol(X) + 1 else ncol(X) # default_model_params <- list( #       input_layer = list(units = 64, activation = 'relu', input_shape = input_shape), #       layers = list( #         list(units = 32, activation = 'relu') #       ), #       output_layer = list(units = 1, activation = 'linear'), #       compile_args = list(optimizer = 'adam', loss = 'mse'), #       fit_params = list(epochs = 5, batch_size = 32, verbose = 0) #     ) model_params <- NULL"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"how-to-set-model_params","dir":"Articles","previous_headings":"Cram Policy > Built-in Model","what":"How to set model_params?","title":"Cram Policy part 2","text":"model_params argument allows customize hyperparameters model used policy learning. left NULL, cram_policy() fall back sensible defaults depending model learner type. following, give default examples model_params model_type learner_type illustrate can specify model_params. Generally speaking, model_params list containing parameters used underlying model values: model_type = \"causal_forest\": model_params <- NULL, method defaults grf::causal_forest() model_params <- list(num.trees = 100). model_type = \"s_learner\" \"m_learner\", depends learner_type: learner_type = \"ridge\", model_params <- NULL, method defaults glmnet::cv.glmnet() model_params <- list(alpha = 1), corresponding Ridge regression. learner_type = \"fnn\" (Feedforward Neural Network): default Keras model built following architecture: learner_type = \"caret\", defaults linear regression re sampling (see https://topepo.github.io/caret/model-training--tuning.html details caret train parameters): Please note list contain element formula Y refers vector provided input Y, element named caret_params containing parameters choice pass caret::train (see full list parameters : https://topepo.github.io/caret/model-training--tuning.html).","code":"# Determine the input shape based on model_type # For s_learner, the treatment D is added to the covariates to constitute the training data # So the input shape is ncol(X) + 1 input_shape <- if (model_type == \"s_learner\") ncol(X) + 1 else ncol(X)  default_model_params <- list(     input_layer = list(units = 64, activation = 'relu', input_shape = input_shape),      layers = list(       list(units = 32, activation = 'relu')     ),     output_layer = list(units = 1, activation = 'linear'),     compile_args = list(optimizer = 'adam', loss = 'mse'),     fit_params = list(epochs = 5, batch_size = 32, verbose = 0)   ) default_model_params <- list(formula = Y ~ ., caret_params = list(method = \"lm\", trControl = trainControl(method = \"none\")))"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"case-of-categorical-target-y","dir":"Articles","previous_headings":"Cram Policy > Built-in Model","what":"Case of categorical target Y","title":"Cram Policy part 2","text":"using caret, note Y categorical using model_type = \"s_learner\", need choose classification method outputting probabilities .e.¬†using key word classProbs = TRUE trainControl, see following example Random Forest Classifier: Also note data inputs needs numeric types, hence Y categorical, contain numeric values representing class observation. need use type factor cram_policy(). Note M-learner: model_type = \"m_learner\", keep mind Y transformed internally. M-learner requires propensity model transformed outcomes: propensity argument cram_policy, NULL, defaults outcome_transform element model_params accessed follows: NULL, defaults Thus, Y transformed might categorical even though Y categorical .e.¬†might want use regression model. See following reference details M-learner: Jia, Z., Imai, K., & Li, M. L. (2024). Cram Method Efficient Simultaneous Learning Evaluation, URL: https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf. can override defaults passing custom list model_params including parameter name defined underlying package model chose, namely grf::causal_forest(), glmnet::cv.glmnet(), keras caret::train()","code":"model_params <- list(formula = Y ~ ., caret_params = list(method = \"rf\", trControl = trainControl(method = \"none\", classProbs = TRUE))) propensity <- function(X) {rep(0.5, nrow(X))} outcome_transform <- model_params$m_learner_outcome_transform outcome_transform <- function(Y, D, prop_score) {Y * D / prop_score - Y * (1 - D) / (1 - prop_score)}`, where `prop_score` is `propensity(X) # Significance level for confidence intervals (default = 95%) alpha <- 0.05    # Run the CRAM policy method result <- cram_policy(   X, D, Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params,   alpha = alpha )  # Display the results print(result) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"custom-model","dir":"Articles","previous_headings":"Cram Policy","what":"Custom Model","title":"Cram Policy part 2","text":"use model policy learning, trigger set model_type = NULL supply two user-defined functions:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"custom_fitx-y-d----","dir":"Articles","previous_headings":"Cram Policy > Custom Model","what":"1. custom_fit(X, Y, D, ...)","title":"Cram Policy part 2","text":"function takes training data: covariates X, outcomes Y, binary treatment indicators D, returns fitted model object. may also define use additional parameters (e.g., number folds, regularization settings, etc.) within function body. X: matrix data frame features Y: numeric outcome vector D: binary vector indicating treatment assignment (0 1) Example: Custom X-learner Ridge regression 5-fold cross-validation","code":"library(glmnet) #> Loading required package: Matrix #> Loaded glmnet 4.1-8  custom_fit <- function(X, Y, D, n_folds = 5) {   treated_indices <- which(D == 1)   control_indices <- which(D == 0)   X_treated <- X[treated_indices, ]   Y_treated <- Y[treated_indices]   X_control <- X[control_indices, ]   Y_control <- Y[control_indices]   model_treated <- cv.glmnet(as.matrix(X_treated), Y_treated, alpha = 0, nfolds = n_folds)   model_control <- cv.glmnet(as.matrix(X_control), Y_control, alpha = 0, nfolds = n_folds)   tau_control <- Y_treated - predict(model_control, as.matrix(X_treated), s = \"lambda.min\")   tau_treated <- predict(model_treated, as.matrix(X_control), s = \"lambda.min\") - Y_control   X_combined <- rbind(X_treated, X_control)   tau_combined <- c(tau_control, tau_treated)   weights <- c(rep(1, length(tau_control)), rep(1, length(tau_treated)))   final_model <- cv.glmnet(as.matrix(X_combined), tau_combined, alpha = 0, weights = weights, nfolds = n_folds)   return(final_model) }"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"custom_predictmodel-x_new-d_new","dir":"Articles","previous_headings":"Cram Policy > Custom Model","what":"2. custom_predict(model, X_new, D_new)","title":"Cram Policy part 2","text":"function uses fitted model generate binary treatment decision individual X_new. return vector 0s 1s, indicating whether assign treatment (1) (0). may also incorporate custom threshold post-processing logic within function. Example: Apply decision rule ‚Äî treat estimated CATE greater 0","code":"custom_predict <- function(model, X_new, D_new) {   cate <- predict(model, as.matrix(X_new), s = \"lambda.min\")    # Apply decision rule: treat if CATE > 0   as.integer(cate > 0) }"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"use-cram_policy-with-custom_fit-and-custom_predict","dir":"Articles","previous_headings":"Cram Policy > Custom Model","what":"3. Use cram_policy() with custom_fit() and custom_predict()","title":"Cram Policy part 2","text":"custom_fit() custom_predict() defined, can integrate Cram framework passing cram_policy() shown (forget set model_type = NULL):","code":"experiment_results <- cram_policy(   X, D, Y,   batch = 20,   model_type = NULL,   custom_fit = custom_fit,   custom_predict = custom_predict,   alpha = 0.05 ) print(experiment_results) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.22542 #> 2        Delta Standard Error 0.06004 #> 3              Delta CI Lower 0.10774 #> 4              Delta CI Upper 0.34310 #> 5       Policy Value Estimate 0.21085 #> 6 Policy Value Standard Error 0.04280 #> 7       Policy Value CI Lower 0.12696 #> 8       Policy Value CI Upper 0.29475 #> 9          Proportion Treated 0.54500 #>  #> $interactive_table #>  #> $final_policy_model #>  #> Call:  cv.glmnet(x = as.matrix(X_combined), y = tau_combined, weights = weights,      nfolds = n_folds, alpha = 0)  #>  #> Measure: Mean-Squared Error  #>  #>     Lambda Index Measure      SE Nonzero #> min 0.0395   100  0.9219 0.02942       3 #> 1se 0.5868    71  0.9505 0.03367       3"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_part_2.html","id":"references","dir":"Articles","previous_headings":"Cram Policy","what":"References","title":"Cram Policy part 2","text":"Jia, Z., Imai, K., & Li, M. L. (2024). Cram Method Efficient Simultaneous Learning Evaluation, URL: https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf. K√ºnzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners estimating heterogeneous treatment effects using machine learning. Proceedings National Academy Sciences United States America, 116(10), 4156‚Äì4165. https://doi.org/10.1073/pnas.1804597116 Wager, S., & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. Journal American Statistical Association, 113(523), 1228-1242. Athey, S., & Imbens, G. (2016). Recursive partitioning heterogeneous causal effects. Proceedings National Academy Sciences, 113(27), 7353-7360.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"what-is-cram_simulation","dir":"Articles","previous_headings":"","what":"üéØ What is cram_simulation()?","title":"Cram Policy Simulation","text":"cram_simulation() function performs simultaneous policy learning evaluation known data-generating process (DGP). useful : Benchmarking Cram simulated datasets Measuring empirical bias, variance, confidence interval coverage Supporting synthetic (dgp_X) empirical (X) covariates generation","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"inputs-overview","dir":"Articles","previous_headings":"","what":"üì¶ Inputs Overview","title":"Cram Policy Simulation","text":"must supply either: - X: dataset bootstrap (empirical DGP)- dgp_X: function simulates covariates must also define: - dgp_D(X): treatment assignment function - dgp_Y(D, X): outcome generation function","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"example-simulated-data-with-binary-discrete-and-continuous-covariates","dir":"Articles","previous_headings":"","what":"üìò Example: Simulated Data with Binary, Discrete, and Continuous Covariates","title":"Cram Policy Simulation","text":"","code":"set.seed(123)  # dgp_X <- function(n) { #   data.table::data.table( #     binary     = rbinom(n, 1, 0.5), #     discrete   = sample(1:5, n, replace = TRUE), #     continuous = rnorm(n) #   ) # }  n <- 100  X_data <- data.table::data.table(     binary     = rbinom(n, 1, 0.5),     discrete   = sample(1:5, n, replace = TRUE),     continuous = rnorm(n)   )   dgp_D <- function(X) rbinom(nrow(X), 1, 0.5)  dgp_Y <- function(D, X) {   theta <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,  # Group 1: High benefit     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4,  # Group 3: Negative benefit            -1,            0.1)  # Group 2: Neutral effect   )   Y <- D * (theta + rnorm(length(D), mean = 0, sd = 1)) +     (1 - D) * rnorm(length(D))  # Outcome for untreated   return(Y) }  # Parameters nb_simulations <- 100 nb_simulations_truth <- 200 batch <- 5  # Perform CRAM simulation result <- cram_simulation(   X = X_data,   dgp_D = dgp_D,   dgp_Y = dgp_Y,   batch = batch,   nb_simulations = nb_simulations,   nb_simulations_truth = nb_simulations_truth,   sample_size = 500 )  # Access results result$avg_delta_estimate #> NULL result$delta_empirical_bias #> NULL"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"output-summary","dir":"Articles","previous_headings":"","what":"üìä Output Summary","title":"Cram Policy Simulation","text":"Returns list containing: raw_results: summary key averaged metrics interactive_table: interactive HTML widget quick exploration","code":"result$raw_results #>                                  Metric   Value #> 1            Average Proportion Treated 0.52724 #> 2                Average Delta Estimate 0.22597 #> 3          Average Delta Standard Error 0.10424 #> 4                  Delta Empirical Bias 0.01224 #> 5              Delta Empirical Coverage 0.96000 #> 6         Variance Delta Empirical Bias 0.00220 #> 7         Average Policy Value Estimate 0.22580 #> 8   Average Policy Value Standard Error 0.10080 #> 9           Policy Value Empirical Bias 0.01210 #> 10      Policy Value Empirical Coverage 0.92000 #> 11 Variance Policy Value Empirical Bias 0.00071 result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"notes","dir":"Articles","previous_headings":"","what":"üí° Notes","title":"Cram Policy Simulation","text":"Uses batch splitting honest policy learning Variance estimates use influence-function-based asymptotics Simulations grouped sim_id averaged can plug custom_fit custom_predict needed","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Cram Policy Simulation","text":"cram_policy() ‚Äî CRAM real experimental/observational data cram_bandit_sim() ‚Äî contextual bandits cram_ml() ‚Äî general supervised unsupervised ML","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Quick Start with CRAM","text":"Cram package provides unified framework : üß† Cram Policy (cram_policy): Learn evaluate individualized binary treatment rules using Cram. Supports flexible models, including causal forests custom learners. Common examples include whether treat patient, send discount offer, provide financial aid based estimated benefit. üìà Cram ML (cram_ml): Learn evaluate standard machine learning models using Cram. estimates expected loss population level, giving reliable measure well final model likely generalize new data. Supports flexible training via caret custom learners, allows evaluation user-defined loss metrics. Ideal classification, regression, predictive tasks. üé∞ Cram Bandit (cram_bandit): Perform -policy evaluation contextual bandit algorithms using Cram. Supports real data simulation environments built-policies. vignette walks three core modules.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"cram-user-file","dir":"Articles","previous_headings":"","what":"Cram User file","title":"Quick Start with CRAM","text":"reproducible use cases, see example script provided Cram GitHub repository: View user_cram.R GitHub","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"generate-simulated-data","dir":"Articles","previous_headings":"1. cram_policy() ‚Äî Binary Policy Learning & Evaluation","what":"Generate Simulated Data","title":"Quick Start with CRAM","text":"","code":"generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),     discrete = sample(1:5, n, replace = TRUE),     continuous = rnorm(n)   )   D <- rbinom(n, 1, 0.5)   treatment_effect <- ifelse(X$binary == 1 & X$discrete <= 2, 1,                        ifelse(X$binary == 0 & X$discrete >= 4, -1, 0.1))   Y <- D * (treatment_effect + rnorm(n)) + (1 - D) * rnorm(n)   list(X = X, D = D, Y = Y) }  set.seed(123) data <- generate_data(1000) X <- data$X; D <- data$D; Y <- data$Y"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"run-cram_policy-with-causal-forest","dir":"Articles","previous_headings":"1. cram_policy() ‚Äî Binary Policy Learning & Evaluation","what":"Run cram_policy() with causal forest","title":"Quick Start with CRAM","text":"","code":"res <- cram_policy(   X, D, Y,   batch = 20,   model_type = \"causal_forest\",   learner_type = NULL,   baseline_policy = as.list(rep(0, nrow(X))),   alpha = 0.05 ) print(res) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"case-of-categorical-target-y","dir":"Articles","previous_headings":"1. cram_policy() ‚Äî Binary Policy Learning & Evaluation","what":"Case of categorical target Y","title":"Quick Start with CRAM","text":"Use caret choose classification method outputting probabilities .e.¬†using key word classProbs = TRUE trainControl, see following example Random Forest Classifier: Also note data inputs needs numeric types, hence Y categorical, contain numeric values representing class observation. need use type factor cram_policy().","code":"model_params <- list(formula = Y ~ ., caret_params = list(method = \"rf\", trControl = trainControl(method = \"none\", classProbs = TRUE)))"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom-models-with-cram_policy","dir":"Articles","previous_headings":"1. cram_policy() ‚Äî Binary Policy Learning & Evaluation","what":"Custom Models with cram_policy()","title":"Quick Start with CRAM","text":"Set model_params NULL specify custom_fit custom_predict.","code":"custom_fit <- function(X, Y, D, n_folds = 5) {   treated <- which(D == 1); control <- which(D == 0)   m1 <- cv.glmnet(as.matrix(X[treated, ]), Y[treated], alpha = 0, nfolds = n_folds)   m0 <- cv.glmnet(as.matrix(X[control, ]), Y[control], alpha = 0, nfolds = n_folds)   tau1 <- predict(m1, as.matrix(X[control, ]), s = \"lambda.min\") - Y[control]   tau0 <- Y[treated] - predict(m0, as.matrix(X[treated, ]), s = \"lambda.min\")   tau <- c(tau0, tau1); X_all <- rbind(X[treated, ], X[control, ])   final_model <- cv.glmnet(as.matrix(X_all), tau, alpha = 0)   final_model }  custom_predict <- function(model, X, D) {   as.numeric(predict(model, as.matrix(X), s = \"lambda.min\") > 0) }  res <- cram_policy(   X, D, Y,   batch = 20,   model_type = NULL,   custom_fit = custom_fit,   custom_predict = custom_predict ) print(res) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.22542 #> 2        Delta Standard Error 0.06004 #> 3              Delta CI Lower 0.10774 #> 4              Delta CI Upper 0.34310 #> 5       Policy Value Estimate 0.21085 #> 6 Policy Value Standard Error 0.04280 #> 7       Policy Value CI Lower 0.12696 #> 8       Policy Value CI Upper 0.29475 #> 9          Proportion Treated 0.54500 #>  #> $interactive_table #>  #> $final_policy_model #>  #> Call:  cv.glmnet(x = as.matrix(X_all), y = tau, alpha = 0)  #>  #> Measure: Mean-Squared Error  #>  #>     Lambda Index Measure      SE Nonzero #> min 0.0395   100  0.9194 0.02589       3 #> 1se 0.4872    73  0.9423 0.03002       3"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"regression-with-cram_ml","dir":"Articles","previous_headings":"2. cram_ml() ‚Äî ML Learning & Evaluation","what":"Regression with cram_ml()","title":"Quick Start with CRAM","text":"Specify formula caret_paramsconforming popular caret::train() set individual loss loss_name.","code":"set.seed(42) data_df <- data.frame(   x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100), Y = rnorm(100) )  caret_params <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  res <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"se\",   caret_params = caret_params ) print(res) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.86429 #> 2 Expected Loss Standard Error  0.73665 #> 3       Expected Loss CI Lower -0.57952 #> 4       Expected Loss CI Upper  2.30809 #>  #> $interactive_table #>  #> $final_ml_model #> Linear Regression  #>  #> 100 samples #>   3 predictor #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"classification-with-cram_ml","dir":"Articles","previous_headings":"2. cram_ml() ‚Äî ML Learning & Evaluation","what":"Classification with cram_ml()","title":"Quick Start with CRAM","text":"data inputs needs numeric types, hence Y categorical, contain numeric values representing class observation. need use type factor cram_ml().","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"case-1-predicting-class-labels","dir":"Articles","previous_headings":"2. cram_ml() ‚Äî ML Learning & Evaluation > Classification with cram_ml()","what":"Case 1: Predicting Class labels","title":"Quick Start with CRAM","text":"case, model outputs hard predictions (labels, e.g.¬†0, 1, 2 etc.), metric used classification accuracy‚Äîproportion correctly predicted labels. Use loss_name = \"accuracy\" Set classProbs = FALSE trainControl Set classify = TRUE cram_ml()","code":"set.seed(42)  # Generate binary classification dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rbinom(nrow(X_data), 1, 0.5) data_df <- data.frame(X_data, Y = Y_data)  # Define caret parameters: predict labels (default behavior) caret_params_rf <- list(   method = \"rf\",   trControl = trainControl(method = \"none\") )  # Run CRAM ML with accuracy as loss result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"accuracy\",   caret_params = caret_params_rf,   classify = TRUE )  print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.48750 #> 2 Expected Loss Standard Error  0.43071 #> 3       Expected Loss CI Lower -0.35668 #> 4       Expected Loss CI Upper  1.33168 #>  #> $interactive_table #>  #> $final_ml_model #> Random Forest  #>  #> 100 samples #>   3 predictor #>   2 classes: 'class0', 'class1'  #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"case-2-predicting-class-probabilities","dir":"Articles","previous_headings":"2. cram_ml() ‚Äî ML Learning & Evaluation > Classification with cram_ml()","what":"Case 2: Predicting Class Probabilities","title":"Quick Start with CRAM","text":"setup, model outputs class probabilities, loss evaluated using logarithmic loss (logloss)‚Äîstandard metric probabilistic classification. Use loss_name = \"logloss\" Set classProbs = TRUE trainControl Set classify = TRUE cram_ml() addition using built-learners via caret, cram_ml() also supports fully custom model workflows. can specify : Model fitting function (custom_fit) Prediction function (custom_predict) Loss function (custom_loss) See vignette ‚ÄúCram ML‚Äù details.","code":"set.seed(42)  # Generate binary classification dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rbinom(nrow(X_data), 1, 0.5) data_df <- data.frame(X_data, Y = Y_data)  # Define caret parameters for probability output caret_params_rf_probs <- list(   method = \"rf\",   trControl = trainControl(method = \"none\", classProbs = TRUE) )  # Run CRAM ML with logloss as the evaluation loss result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"logloss\",   caret_params = caret_params_rf_probs,   classify = TRUE )  print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.93225 #> 2 Expected Loss Standard Error  0.48118 #> 3       Expected Loss CI Lower -0.01085 #> 4       Expected Loss CI Upper  1.87534 #>  #> $interactive_table #>  #> $final_ml_model #> Random Forest  #>  #> 100 samples #>   3 predictor #>   2 classes: 'class0', 'class1'  #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"cram_bandit-contextual-bandits-for-on-policy-statistical-evaluation","dir":"Articles","previous_headings":"","what":"3. cram_bandit() ‚Äî Contextual Bandits for On-policy Statistical Evaluation","title":"Quick Start with CRAM","text":"Specify: pi: array shape (T √ó B, T, K) (T √ó B, T), : TT number learning steps (policy updates) BB batch size KK number arms T√óBT \\times B total number contexts natural 3D version, pi[j, t, ] gives probability policy œÄÃÇt\\hat{\\pi}_t assigns arm context XjX_j. 2D version, keep probabilities assigned chosen arm AjA_j context XjX_j historical data - probabilities arms aa context XjX_j. arm: vector length T√óBT \\times B indicating arm selected context. reward: vector observed rewards length T√óBT \\times B. batch: (optional) Integer batch size BB. Default 1. alpha: Significance level confidence intervals.","code":"set.seed(42) T <- 100; K <- 4 pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K)) for (t in 1:T) for (j in 1:T) pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ]) arm <- sample(1:K, T, replace = TRUE) reward <- rnorm(T, 1, 0.5)  res <- cram_bandit(pi, arm, reward, batch=1, alpha=0.05) print(res) #> $raw_results #>                        Metric   Value #> 1       Policy Value Estimate 0.67621 #> 2 Policy Value Standard Error 0.04394 #> 3       Policy Value CI Lower 0.59008 #> 4       Policy Value CI Upper 0.76234 #>  #> $interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Quick Start with CRAM","text":"cram_policy(): Learn evaluate binary policy. cram_ml(): Learn evaluate ML models. cram_bandit(): Cramming contextual bandits -policy statistical evaluation.","code":""},{"path":"https://yanisvdc.github.io/cramR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yanis Vandecasteele. Maintainer, author. Michael Lingzhi Li. Contributor. Kosuke Imai. Contributor. Zeyang Jia. Contributor. Longlin Wang. Contributor.","code":""},{"path":"https://yanisvdc.github.io/cramR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Vandecasteele Y (2025). cramR: Cram Method Efficient Simultaneous Learning Evaluation. R package version 0.1.0, https://yanisvdc.github.io/cramR , https://github.com/yanisvdc/cramR.","code":"@Manual{,   title = {cramR: The Cram Method for Efficient Simultaneous Learning and Evaluation},   author = {Yanis Vandecasteele},   year = {2025},   note = {R package version 0.1.0, https://yanisvdc.github.io/cramR },   url = {https://github.com/yanisvdc/cramR}, }"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"overview","dir":"","previous_headings":"üìö What is Cram?","what":"Overview","title":"CRAM","text":"üéØ Train & Evaluate ML Models üîÅ Cram vs.¬†Sample-Splitting & Cross-Validation üß† Typical use case: Policy Learning üåç Real-World Applications","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-train--evaluate-ml-models","dir":"","previous_headings":"üìö What is Cram?","what":"üéØ Train & Evaluate ML models","title":"CRAM","text":"Cram method efficient approach simultaneous learning evaluation using generic machine learning (ML) algorithm. single pass batched data, proposed method repeatedly trains ML algorithm tests empirical performance.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-cram-vs-sample-splitting--cross-validation","dir":"","previous_headings":"üìö What is Cram?","what":"üîÅ Cram vs.¬†Sample-Splitting & Cross-Validation","title":"CRAM","text":"utilizes entire sample learning evaluation, cramming significantly data-efficient sample-splitting, reserves portion data purely evaluation. Also, key distinction cross-validation Cram evaluates final learned model directly, rather using proxy average performance multiple fold-specific models trained different data subsets‚Äîresulting sharper inference better alignment real-world deployment.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-typical-use-case-policy-learning","dir":"","previous_headings":"üìö What is Cram?","what":"üß† Typical use case: Policy Learning","title":"CRAM","text":"Cram method naturally applies policy learning setting, popular subfield ML focused learning decision rule (also called treatment rule policy) assigns treatments actions individuals based features, goal maximizing expected outcome (e.g., health, profit, welfare). Cramming allows users learn individualized decision rule estimate average outcome result learned decision rule deployed entire population beyond data sample.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-real-world-applications","dir":"","previous_headings":"üìö What is Cram?","what":"üåç Real-World Applications","title":"CRAM","text":"particularly relevant high-stakes applications decisions must personalized statistically reliable. Common examples include: Healthcare: determining receive treatment based individual characteristics Advertising pricing: setting optimal prices maximize revenue Policy interventions: deciding individuals regions receive targeted support improve outcomes","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-key-features","dir":"","previous_headings":"","what":"üéØ Key Features","title":"CRAM","text":"üß† Cram Policy (cram_policy): Learn evaluate individualized binary treatment rules using Cram. Supports flexible models, including causal forests custom learners. Common examples include whether treat patient, send discount offer, provide financial aid based estimated benefit. üìà Cram ML (cram_ml): Learn evaluate standard machine learning models using Cram. estimates expected loss population level, giving reliable measure well final model likely generalize new data. Supports flexible training via caret custom learners, allows evaluation user-defined loss metrics. Ideal classification, regression, predictive tasks. üé∞ Cram Bandit (cram_bandit): Perform -policy evaluation contextual bandit algorithms using Cram. Supports real data simulation environments built-policies. users ML background, may informative compare supervised learning introduce contextual bandit setting. supervised learning, data point comes known label. contrast, contextual bandit setting involves context (feature vector), choice among multiple actions, reward observed chosen action. Thus, label (reward) first unknown revealed action chosen - note labels (rewards) associated non-chosen actions remain unknown (partial feedback), makes learning evaluation challenging. Contextual bandits appear applications online system selects actions based context maximize outcomes‚Äîlike showing ads recommendations observing user clicks purchases. Contextual bandit algorithms aim learn policy chooses best action context maximize expected reward, engagement (clicks) conversion. Cram Bandit estimates well final learned policy perform deployed entire population, based data collected policy.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-documentation","dir":"","previous_headings":"","what":"üìö Documentation","title":"CRAM","text":"Introduction & Cram Policy Function Reference ‚Äôs New","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_Ô∏è-installation","dir":"","previous_headings":"","what":"üõ†Ô∏è Installation","title":"CRAM","text":"install development version Cram GitHub:","code":"# Install devtools if needed install.packages(\"devtools\")  # Install cramR from GitHub devtools::install_github(\"yanisvdc/cramR\")"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-citation","dir":"","previous_headings":"üìÑ Citation & ü§ù Contributing","what":"üìö Citation","title":"CRAM","text":"use Cram research, please cite following papers: can also cite R package:","code":"@techreport{jia2024cram,   title        = {The Cram Method for Efficient Simultaneous Learning and Evaluation},   author       = {Jia, Zeyang and Imai, Kosuke and Li, Michael Lingzhi},   institution  = {Harvard Business School},   type         = {Working Paper},   year         = {2024},   url          = {https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf},   note         = {Accessed April 2025} } @misc{jia2025crammingcontextualbanditsonpolicy,       title={Cramming Contextual Bandits for On-policy Statistical Evaluation},        author={Zeyang Jia and Kosuke Imai and Michael Lingzhi Li},       year={2025},       eprint={2403.07031},       archivePrefix={arXiv},       primaryClass={cs.LG},       url={https://arxiv.org/abs/2403.07031},  } @Manual{,   title  = {cramR: The Cram Method for Efficient Simultaneous Learning and Evaluation},   author = {Yanis Vandecasteele and Michael Lingzhi Li and Kosuke Imai and Zeyang Jia and Longlin Wang},   year   = {2025},   note   = {R package version 0.1.0},   url    = {https://github.com/yanisvdc/cramR} }"},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-how-to-contribute","dir":"","previous_headings":"üìÑ Citation & ü§ù Contributing","what":"ü§ù How to Contribute","title":"CRAM","text":"welcome contributions! contribute:","code":"# 1. Fork the repository.  # 2. Create a new branch git checkout -b feature/your-feature  # 3. Commit your changes git commit -am 'Add some feature'  # 4. Push to the branch git push origin feature/your-feature  # 5. Create a pull request.  # 6. Open an Issue or PR at: # https://github.com/yanisvdc/cramR/issues"},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-contact","dir":"","previous_headings":"","what":"üìß Contact","title":"CRAM","text":"questions issues, please open issue.","code":""},{"path":"https://yanisvdc.github.io/cramR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Yanis Vandecasteele Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":null,"dir":"Reference","previous_headings":"","what":"Averaged CRAM with Permutations ‚Äî averaged_cram","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"function implements Averaged CRAM randomly permuting batches (except last batch kept ) averaging performance results.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"","code":"averaged_cram(   X,   D,   Y,   batch,   model_type,   learner_type = NULL,   alpha = 0.05,   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   num_permutations = 10 )"},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"X matrix data frame covariates. D binary vector treatment indicators (0 1). Y vector outcomes. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". alpha Significance level confidence intervals. Default 0.05 (95% confidence). baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. num_permutations Number random permutations batches.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"list averaged performance variance estimates.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"","code":"X <- matrix(rnorm(1000), nrow = 100, ncol = 10) D <- sample(0:1, 100, replace = TRUE) Y <- rnorm(100) avg_cram_results <- averaged_cram(X, D, Y,                                   batch = 20,                                   model_type = \"m_learner\",                                   learner_type = \"ridge\",                                   num_permutations = 3) #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"Performs Cram method -policy Statistical Evaluation Contextual Bandit","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"","code":"cram_bandit(pi, arm, reward, batch = 1, alpha = 0.05)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"pi row j, column t, depth , gives pi_t(Xj, ) arm arms selected time step reward rewards time step batch Batch size alpha Confidence level intervals (default = 0.05).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"list containing: raw_results: Data frame performance metrics interactive_table: DT::datatable interactive view final_ml_model: Trained model object","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":null,"dir":"Reference","previous_headings":"","what":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"function implements armed bandit policy evaluation formula estimating \\(\\Delta(\\pi_T; \\pi_0)\\) given user-provided formula.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"","code":"cram_bandit_est(pi, reward, arm, batch = 1)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"pi 3-d array, row j, column t, depth , gives pi_t(Xj, ) reward vector rewards arm vector arms chosen batch vector integer. vector, gives batch assignment context. integer, interpreted batch size contexts assigned batch order dataset.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"estimated policy value difference \\(\\Delta(\\pi_T; \\pi_0)\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":null,"dir":"Reference","previous_headings":"","what":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"function performs simulation cram bandit policy learning evaluation.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"","code":"cram_bandit_sim(   horizon,   simulations,   bandit,   policy,   alpha = 0.05,   do_parallel = FALSE,   seed = 42 )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"horizon number timesteps simulations number simulations bandit bandit, generating contextual features observed rewards policy policy, choosing arm timestep alpha Significance level confidence intervals calculating empirical coverage. Default 0.05 (95% confidence). do_parallel Whether parallelize simulations. Default FALSE. seed Seed simulation","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"**list** containing: raw_results data frame summarizing key metrics:   Average Prediction Error Policy Value Estimate,   Average Prediction Error Variance Estimate,   Empirical Coverage alpha level Confidence Interval. interactive_table interactive table summarizing key metrics detailed exploration.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Bandit Variance ‚Äî cram_bandit_var","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"function implements crammed variance estimator bandit policy","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"","code":"cram_bandit_var(pi, reward, arm, batch = 1)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"pi 3-d array, row j, column t, depth , gives pi_t(Xj, ) reward vector rewards arm vector arms chosen batch Batch size","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"crammed variance estimate bandit.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"function returns cram estimator policy value difference (delta).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"","code":"cram_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"estimated policy value difference \\(\\(\\hat{\\Delta}\\)(\\(\\hat{\\pi}\\)_T; \\(\\pi\\)_0)\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"function computes Cram expected loss estimator \\(\\hat{R}_{\\mathrm{cram}}\\) based given loss matrix batch indices.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"","code":"cram_expected_loss(loss, batch_indices)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"loss matrix loss values \\(N\\) rows (data points) \\(K+1\\) columns (batches). assume first column loss matrix contains zeros. following nb_batch columns contain losses trained model individual. batch_indices list element vector indices corresponding batch. example: split(1:N, rep(1:nb_batch, = N / nb_batch)).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"estimated Cram expected loss \\(\\hat{R}_{\\mathrm{cram}}\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"","code":"# Example usage set.seed(123) N <- 100  # Number of data points K <- 10   # Number of batches  # Generate a loss matrix with K+1 columns, first column as zeros loss <- matrix(rnorm(N * (K+1)), nrow = N, ncol = K+1) loss[, 1] <- 0  # Ensure first column is zero  # Create batch indices dynamically batch_indices <- split(1:N, rep(1:K, length.out = N))  # Compute Cram Expected Loss cram_expected_loss(loss, batch_indices) #> [1] 0.2474586"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Learning ‚Äî cram_learning","title":"CRAM Learning ‚Äî cram_learning","text":"function performs learning part cram method.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Learning ‚Äî cram_learning","text":"","code":"cram_learning(   X,   D,   Y,   batch,   model_type = \"causal_forest\",   learner_type = \"ridge\",   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   n_cores = detectCores() - 1,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Learning ‚Äî cram_learning","text":"X matrix data frame covariates sample. D vector binary treatment indicators (1 treated, 0 untreated). Y vector outcome values sample. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". baseline_policy list providing baseline policy (binary 0 1) sample. NULL, baseline policy defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. n_cores Number cores use parallelization parallelize_batch set TRUE. Defaults detectCores() - 1. propensity propensity score","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Learning ‚Äî cram_learning","text":"list containing: final_policy_model final fitted policy model, depending model_type learner_type. policies matrix learned policies, column represents batch's learned policy first column baseline policy. batch_indices indices batch, either generated (batch integer) provided user.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Learning ‚Äî cram_learning","text":"","code":"# Example usage X_data <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D_data <- sample(c(0, 1), 100, replace = TRUE) Y_data <- rnorm(100) nb_batch <- 5  # Perform CRAM learning result <- cram_learning(X = X_data, D = D_data, Y = Y_data, batch = nb_batch)  # Access the learned policies and final model policies_matrix <- result$policies final_model <- result$final_policy_model batch_indices <- result$batch_indices"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"Performs CRAM method (Causal Regularization via Approximate Models) simultaneous machine learning evaluation experimental observational studies unknown data generating processes.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"","code":"cram_ml(   data,   batch,   formula = NULL,   caret_params = NULL,   parallelize_batch = FALSE,   loss_name = NULL,   custom_fit = NULL,   custom_predict = NULL,   custom_loss = NULL,   alpha = 0.05,   classify = FALSE )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"data matrix data frame covariates. supervised learning, must include target variable specified `formula`. batch Integer specifying number batches vector pre-defined batch assignments. formula Optional formula supervised learning (e.g., y ~ .). Use NULL unsupervised methods like clustering. caret_params List parameters `caret::train()` containing: method: Model type (e.g., \"rf\", \"glm\", \"xgbTree\" supervised learning) Additional method-specific parameters parallelize_batch Logical indicating whether parallelize batch processing (default = FALSE). loss_name Name loss metric (supported: \"se\", \"logloss\", \"accuracy\"). custom_fit Optional custom model training function. custom_predict Optional custom prediction function. custom_loss Optional custom loss function. alpha Confidence level intervals (default = 0.05). classify Indicate classification problem. Defaults FALSE.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"list containing: raw_results: Data frame performance metrics interactive_table: DT::datatable interactive view final_ml_model: Trained model object","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"CRAM method implements novel approach simultaneous model training evaluation unknown data distributions. Key features: Automated batch-wise model training Cross-validation compatible Supports supervised unsupervised learning Provides confidence intervals loss estimates","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"","code":"# Load necessary libraries library(caret) #> Loading required package: ggplot2 #> Loading required package: lattice  # Set seed for reproducibility set.seed(42)  # Generate example dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100)  # Continuous target variable for regression data_df <- data.frame(X_data, Y = Y_data)  # Ensure target variable is included  # Define caret parameters for simple linear regression (no cross-validation) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  nb_batch <- 5  # Run ML learning function result <- cram_ml(   data = data_df,   formula = Y ~ .,  # Linear regression model   batch = nb_batch,   loss_name = 'se',   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Policy ‚Äî cram_policy","title":"CRAM Policy ‚Äî cram_policy","text":"function performs cram method (simultaneous policy learning evaluation) binary policies data including covariates (X), binary treatment indicator (D) outcomes (Y).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Policy ‚Äî cram_policy","text":"","code":"cram_policy(   X,   D,   Y,   batch,   model_type = \"causal_forest\",   learner_type = \"ridge\",   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   alpha = 0.05,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Policy ‚Äî cram_policy","text":"X matrix data frame covariates sample. D vector binary treatment indicators (1 treated, 0 non-treated). Y vector outcome values sample. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression, \"fnn\" Feedforward Neural Network \"caret\" Caret. Default \"ridge\". baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. alpha Significance level confidence intervals. Default 0.05 (95% confidence). propensity propensity score function binary treatment indicator (D) (probability unit receive treatment). Defaults 0.5 (random assignment).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Policy ‚Äî cram_policy","text":"list containing: raw_results: data frame summarizing key metrics truncated decimals: Delta Estimate: estimated treatment effect (delta). Delta Standard Error: standard error delta estimate. Delta CI Lower: lower bound confidence interval delta. Delta CI Upper: upper bound confidence interval delta. Policy Value Estimate: estimated policy value. Policy Value Standard Error: standard error policy value estimate. Policy Value CI Lower: lower bound confidence interval policy value. Policy Value CI Upper: upper bound confidence interval policy value. Proportion Treated: proportion individuals treated final policy. interactive_table: interactive table summarizing key metrics detailed exploration. final_policy_model: final fitted policy model based model_type learner_type custom_fit.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Policy ‚Äî cram_policy","text":"","code":"# Example data X_data <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D_data <- D_data <- as.integer(sample(c(0, 1), 100, replace = TRUE)) Y_data <- rnorm(100) nb_batch <- 5  # Perform CRAM policy result <- cram_policy(X = X_data,                           D = D_data,                           Y = Y_data,                           batch = nb_batch)  # Access results result$raw_results #>                        Metric    Value #> 1              Delta Estimate  0.36852 #> 2        Delta Standard Error  0.26573 #> 3              Delta CI Lower -0.15230 #> 4              Delta CI Upper  0.88935 #> 5       Policy Value Estimate  0.53258 #> 6 Policy Value Standard Error  0.27544 #> 7       Policy Value CI Lower -0.00727 #> 8       Policy Value CI Upper  1.07243 #> 9          Proportion Treated  1.00000 result$interactive_table  {\"x\":{\"filter\":\"none\",\"vertical\":false,\"caption\":\"<caption>CRAM Experiment Results<\\/caption>\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],[\"Delta Estimate\",\"Delta Standard Error\",\"Delta CI Lower\",\"Delta CI Upper\",\"Policy Value Estimate\",\"Policy Value Standard Error\",\"Policy Value CI Lower\",\"Policy Value CI Upper\",\"Proportion Treated\"],[0.36852,0.26573,-0.1523,0.88935,0.5325800000000001,0.27544,-0.00727,1.07243,1]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Metric<\\/th>\\n      <th>Value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Metric\",\"targets\":1},{\"name\":\"Value\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}result$final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 100  #> Variable importance:  #>     1     2     3     4     5  #> 0.089 0.187 0.142 0.124 0.160"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"function returns cram estimator policy value (psi).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"","code":"cram_policy_value_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity Propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"estimated policy value.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Simulation ‚Äî cram_simulation","title":"CRAM Simulation ‚Äî cram_simulation","text":"function performs cram method (simultaneous learning evaluation) simulation data, data generation process (DGP) known. data generation process X can given directly function induced provided dataset via row-wise bootstrapping. Results averaged across Monte Carlo replicates given DGP.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Simulation ‚Äî cram_simulation","text":"","code":"cram_simulation(   X = NULL,   dgp_X = NULL,   dgp_D,   dgp_Y,   batch,   nb_simulations,   nb_simulations_truth = NULL,   sample_size,   model_type = \"causal_forest\",   learner_type = \"ridge\",   alpha = 0.05,   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Simulation ‚Äî cram_simulation","text":"X Optional. matrix data frame covariates sample inducing empirically DGP covariates. dgp_X Optional. function generate covariate data simulations. dgp_D vectorized function generate binary treatment assignments sample. dgp_Y vectorized function generate outcome variable sample given treatment covariates. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. nb_simulations number simulations (Monte Carlo replicates) run. nb_simulations_truth Optional. number additional simmulations (Monte Carlo replicates) beyond nb_simulations use calculating true policy value difference (delta) true policy value (psi) sample_size number samples simulation. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". alpha Significance level confidence intervals. Default 0.05 (95% confidence). baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. propensity propensity score model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Simulation ‚Äî cram_simulation","text":"list containing: avg_proportion_treated average proportion treated individuals across simulations. avg_delta_estimate average delta estimate across simulations. avg_delta_standard_error average standard error delta estimates. delta_empirical_bias empirical bias delta estimates. delta_empirical_coverage empirical coverage delta confidence intervals. avg_policy_value_estimate average policy value estimate across simulations. avg_policy_value_standard_error average standard error policy value estimates. policy_value_empirical_bias empirical bias policy value estimates. policy_value_empirical_coverage empirical coverage policy value confidence intervals.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Simulation ‚Äî cram_simulation","text":"","code":"# Define data generation process (DGP) functions X_data <- data.table::data.table(   binary = rbinom(100, 1, 0.5),                 # Binary variable (0 or 1)   discrete = sample(1:5, 100, replace = TRUE),  # Discrete variable (1 to 5)   continuous = rnorm(100)                       # Continuous variable ) dgp_D <- function(X) rbinom(nrow(X), 1, 0.5) dgp_Y <- function(D, X) { theta <- ifelse( X[, binary] == 1 & X[, discrete] <= 2,  # Group 1: High benefit 1, ifelse(X[, binary] == 0 & X[, discrete] >= 4,  # Group 3: High adverse effect -1, 0.1)  # Group 2: Neutral effect ) Y <- D * (theta + rnorm(length(D), mean = 0, sd = 1)) +   (1 - D) * rnorm(length(D))  # Outcome for untreated  return(Y) }  # Parameters: nb_simulations <- 10 nb_simulations_truth <- 2 batch <- 5  # Perform CRAM simulation result <- cram_simulation(X = X_data, dgp_D = dgp_D, dgp_Y = dgp_Y,                           batch = batch, nb_simulations = nb_simulations,                           nb_simulations_truth = nb_simulations_truth,                           sample_size=50)  # Access results result$avg_delta_estimate #> NULL result$delta_empirical_bias #> NULL"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"function estimates asymptotic variance cram estimator policy value difference (delta).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"","code":"cram_variance_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"estimated variance \\(\\hat{v}^2_T\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"function estimates asymptotic variance cram estimator policy value (psi).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"","code":"cram_variance_estimator_policy_value(   X,   Y,   D,   pi,   batch_indices,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity Propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"estimated variance \\(\\hat{w}^2_T\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"function computes variance estimator \\(\\hat{\\sigma}^2_2\\) based given loss matrix batch indices.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"","code":"cram_var_expected_loss(loss, batch_indices)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"loss matrix loss values \\(N\\) rows (data points) \\(K+1\\) columns (batches). assume first column loss matrix contains zeros. following nb_batch columns contain losses trained model individual. batch_indices list element vector indices corresponding batch. example: split(1:N, rep(1:nb_batch, = N / nb_batch)).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"estimated Cram expected loss \\(\\hat{R}_{\\mathrm{cram}}\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"","code":"# Example usage set.seed(123) N <- 100  # Number of data points K <- 10   # Number of batches  # Generate a loss matrix with K+1 columns, first column as zeros loss <- matrix(rnorm(N * (K+1)), nrow = N, ncol = K+1) loss[, 1] <- 0  # Ensure first column is zero  # Create batch indices dynamically batch_indices <- split(1:N, rep(1:K, length.out = N))  # Compute Cram Expected Loss Variance cram_var_expected_loss(loss, batch_indices) #> [1] 1.995626"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Model ‚Äî fit_model","title":"Fit Model ‚Äî fit_model","text":"function trains given unfitted model provided data parameters, according model type learner type.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Model ‚Äî fit_model","text":"","code":"fit_model(model, X, Y, D, model_type, learner_type, model_params, propensity)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Model ‚Äî fit_model","text":"model unfitted model object, returned `set_model`. X matrix data frame covariates samples. Y vector outcome values. D vector binary treatment indicators (1 treated, 0 untreated). model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. propensity propensity score","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Model ‚Äî fit_model","text":"fitted model object.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Model ‚Äî fit_model","text":"","code":"# Example usage for Ridge Regression S-learner set.seed(123) X <- matrix(rnorm(1000), nrow = 100, ncol = 10) D <- sample(0:1, 100, replace = TRUE) Y <- rnorm(100) # Set up the model model <- set_model(\"s_learner\", \"ridge\") # Define model parameters model_params <- list(alpha = 0) # Fit the model fitted_model <- fit_model(                         model, X, Y, D = D,                         model_type = \"s_learner\",                         learner_type = \"ridge\",                         model_params = model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Model ML ‚Äî fit_model_ml","title":"Fit Model ML ‚Äî fit_model_ml","text":"function trains given unfitted model provided data parameters, according model type learner type.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Model ML ‚Äî fit_model_ml","text":"","code":"fit_model_ml(data, formula, caret_params, classify)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Model ML ‚Äî fit_model_ml","text":"data dataset formula formula caret_params parameters caret model classify Indicate classification problem. Defaults FALSE","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Model ML ‚Äî fit_model_ml","text":"fitted model object.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Mock Dataset ‚Äî generate_data","title":"Generate Mock Dataset ‚Äî generate_data","text":"function generates simulated dataset covariates, treatment assignments, outcomes testing experimentation. dataset includes heterogeneous treatment effects across groups, mimicking realistic causal inference scenarios.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Mock Dataset ‚Äî generate_data","text":"","code":"generate_data(n)"},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Mock Dataset ‚Äî generate_data","text":"n Integer. number observations generate.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Mock Dataset ‚Äî generate_data","text":"list containing: X data.table three variables: D Binary treatment assignment (0 1). Y Numeric outcome based treatment effects covariates.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Mock Dataset ‚Äî generate_data","text":"","code":"# Generate a dataset with 1000 observations data <- generate_data(1000) str(data) #> List of 3 #>  $ X:Classes 'data.table' and 'data.frame':\t1000 obs. of  3 variables: #>   ..$ binary    : int [1:1000] 1 0 0 0 1 1 0 0 1 1 ... #>   ..$ discrete  : int [1:1000] 3 4 4 4 1 4 1 1 3 2 ... #>   ..$ continuous: num [1:1000] 2.047 -0.485 0.363 1.911 -0.275 ... #>   ..- attr(*, \".internal.selfref\")=<externalptr>  #>  $ D: int [1:1000] 0 0 1 1 1 0 1 0 0 0 ... #>  $ Y: num [1:1000] -0.218 -0.2 -2.267 0.635 1.702 ... head(data$X) #>    binary discrete continuous #>     <int>    <int>      <num> #> 1:      1        3  2.0470005 #> 2:      0        4 -0.4845959 #> 3:      0        4  0.3625913 #> 4:      0        4  1.9112226 #> 5:      1        1 -0.2745191 #> 6:      1        4 -0.6519016 head(data$D) #> [1] 0 0 1 1 1 0 head(data$Y) #> [1] -0.2178099 -0.1997781 -2.2670366  0.6351589  1.7018075 -0.1513984"},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized ML Learning ‚Äî ml_learning","title":"Generalized ML Learning ‚Äî ml_learning","text":"function performs batch-wise learning **supervised** **unsupervised** machine learning models.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized ML Learning ‚Äî ml_learning","text":"","code":"ml_learning(   data,   formula = NULL,   batch,   parallelize_batch = FALSE,   loss_name = NULL,   caret_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   custom_loss = NULL,   n_cores = detectCores() - 1,   classify = FALSE )"},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized ML Learning ‚Äî ml_learning","text":"data matrix data frame features. supervised model used, also include target variable. formula Optional formula specifying relationship target predictors **supervised learning** (use `NULL` unsupervised learning). batch Either integer specifying number batches (randomly sampled) vector length equal sample size indicating batch assignment observation. parallelize_batch Logical. Whether parallelize batch processing. Defaults `FALSE`. - `TRUE`, batch models trained parallel. - `FALSE`, training performed sequentially using `data.table` efficiency. loss_name name loss function used (e.g., `\"se\"`, `\"logloss\"`). caret_params **list** parameters pass `caret::train()` function. - Required: `method` (e.g., `\"glm\"`, `\"rf\"`). custom_fit **custom function** training user-defined models. Defaults `NULL`. custom_predict **custom function** making predictions user-defined models. Defaults `NULL`. custom_loss Optional **custom function** computing loss trained model data. return **vector** containing per-instance losses. n_cores Number CPU cores use parallel processing (`parallelize_batch = TRUE`). Defaults `detectCores() - 1`. classify Indicate classification problem. Defaults FALSE","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized ML Learning ‚Äî ml_learning","text":"**list** containing: final_ml_model final trained ML model. losses matrix losses column represents batch's trained model. first column contains zeros (baseline model). batch_indices indices observations batch.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generalized ML Learning ‚Äî ml_learning","text":"","code":"# Load necessary libraries library(caret)  # Set seed for reproducibility set.seed(42)  # Generate example dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100)  # Continuous target variable for regression data_df <- data.frame(X_data, Y = Y_data)  # Ensure target variable is included  # Define caret parameters for simple linear regression (no cross-validation) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  # Define the batch count (not used in this simple example) nb_batch <- 5  # Run ML learning function result_lm <- ml_learning(   data = data_df,   formula = Y ~ .,  # Linear regression model   batch = nb_batch,   loss_name = 'se',   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with the Specified Model ‚Äî model_predict","title":"Predict with the Specified Model ‚Äî model_predict","text":"function performs inference using trained model, providing flexibility different types models Causal Forest, Ridge Regression, Feedforward Neural Networks (FNNs).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with the Specified Model ‚Äî model_predict","text":"","code":"model_predict(model, X, D, model_type, learner_type, model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with the Specified Model ‚Äî model_predict","text":"model trained model object returned `fit_model` function. X matrix data frame covariates predictions required. D vector binary treatment indicators (1 treated, 0 untreated). Optional, depending model type. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with the Specified Model ‚Äî model_predict","text":"vector predictions CATE estimates, depending model_type learner_type.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict with the Specified Model ‚Äî model_predict","text":"","code":"# Load required library library(grf)  # Example: Predicting with a Causal Forest model set.seed(123) X <- matrix(rnorm(1000), nrow = 100, ncol = 10)  # Covariates Y <- rnorm(100)                                 # Outcomes D <- sample(0:1, 100, replace = TRUE)           # Treatment indicators cf_model <- causal_forest(X, Y, D)             # Train Causal Forest new_X <- matrix(rnorm(100), nrow = 10, ncol = 10) # New data for predictions predictions <- model_predict(model = cf_model, X = new_X, model_type = \"causal_forest\")"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with the Specified Model ‚Äî model_predict_ml","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"function performs inference using trained model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"","code":"model_predict_ml(   model,   data,   formula,   caret_params,   cram_policy_handle = FALSE )"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"model trained model object returned `fit_model_ml` function. data dataset formula formula caret_params parameters caret model cram_policy_handle Internal use. Post-process predictions differently cram policy use. Defaults FALSE.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"vector predictions CATE estimates, depending model_type learner_type.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Model ‚Äî set_model","title":"Set Model ‚Äî set_model","text":"function maps model type learner type corresponding model function.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Model ‚Äî set_model","text":"","code":"set_model(model_type, learner_type, model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Model ‚Äî set_model","text":"model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. FNNs, following elements defined model params list: input_layer list defining input layer. Must include: units Number units input layer. activation Activation function input layer. input_shape Input shape layer.  layers list lists, sublist specifies hidden layer : units Number units layer. activation Activation function layer.  output_layer list defining output layer. Must include: units Number units output layer. activation Activation function output layer (e.g., \"linear\" \"sigmoid\").  compile_args list arguments compiling model. Must include: optimizer Optimizer training (e.g., \"adam\" \"sgd\"). loss Loss function (e.g., \"mse\" \"binary_crossentropy\"). metrics Optional list metrics evaluation (e.g., c(\"accuracy\")).  learners (e.g., \"ridge\" \"causal_forest\"), model_params can include relevant hyperparameters.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Model ‚Äî set_model","text":"instantiated model object corresponding model function.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Model ‚Äî set_model","text":"","code":"# Example: Causal Forest with default parameters set_model(\"causal_forest\", NULL, model_params = list(num.trees = 100)) #> function (X, Y, W, Y.hat = NULL, W.hat = NULL, num.trees = 2000,  #>     sample.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,  #>     sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(X)) +  #>         20), ncol(X)), min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,  #>     honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,  #>     stabilize.splits = TRUE, ci.group.size = 2, tune.parameters = \"none\",  #>     tune.num.trees = 200, tune.num.reps = 50, tune.num.draws = 1000,  #>     compute.oob.predictions = TRUE, num.threads = NULL, seed = runif(1,  #>         0, .Machine$integer.max))  #> { #>     has.missing.values <- validate_X(X, allow.na = TRUE) #>     validate_sample_weights(sample.weights, X) #>     Y <- validate_observations(Y, X) #>     W <- validate_observations(W, X) #>     clusters <- validate_clusters(clusters, X) #>     samples.per.cluster <- validate_equalize_cluster_weights(equalize.cluster.weights,  #>         clusters, sample.weights) #>     num.threads <- validate_num_threads(num.threads) #>     all.tunable.params <- c(\"sample.fraction\", \"mtry\", \"min.node.size\",  #>         \"honesty.fraction\", \"honesty.prune.leaves\", \"alpha\",  #>         \"imbalance.penalty\") #>     default.parameters <- list(sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(X)) +  #>         20), ncol(X)), min.node.size = 5, honesty.fraction = 0.5,  #>         honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0) #>     args.orthog <- list(X = X, num.trees = max(50, num.trees/4),  #>         sample.weights = sample.weights, clusters = clusters,  #>         equalize.cluster.weights = equalize.cluster.weights,  #>         sample.fraction = sample.fraction, mtry = mtry, min.node.size = 5,  #>         honesty = TRUE, honesty.fraction = 0.5, honesty.prune.leaves = honesty.prune.leaves,  #>         alpha = alpha, imbalance.penalty = imbalance.penalty,  #>         ci.group.size = 1, tune.parameters = tune.parameters,  #>         num.threads = num.threads, seed = seed) #>     if (is.null(Y.hat)) { #>         forest.Y <- do.call(regression_forest, c(Y = list(Y),  #>             args.orthog)) #>         Y.hat <- predict(forest.Y)$predictions #>     } #>     else if (length(Y.hat) == 1) { #>         Y.hat <- rep(Y.hat, nrow(X)) #>     } #>     else if (length(Y.hat) != nrow(X)) { #>         stop(\"Y.hat has incorrect length.\") #>     } #>     if (is.null(W.hat)) { #>         forest.W <- do.call(regression_forest, c(Y = list(W),  #>             args.orthog)) #>         W.hat <- predict(forest.W)$predictions #>     } #>     else if (length(W.hat) == 1) { #>         W.hat <- rep(W.hat, nrow(X)) #>     } #>     else if (length(W.hat) != nrow(X)) { #>         stop(\"W.hat has incorrect length.\") #>     } #>     Y.centered <- Y - Y.hat #>     W.centered <- W - W.hat #>     data <- create_train_matrices(X, outcome = Y.centered, treatment = W.centered,  #>         sample.weights = sample.weights) #>     args <- list(num.trees = num.trees, clusters = clusters,  #>         samples.per.cluster = samples.per.cluster, sample.fraction = sample.fraction,  #>         mtry = mtry, min.node.size = min.node.size, honesty = honesty,  #>         honesty.fraction = honesty.fraction, honesty.prune.leaves = honesty.prune.leaves,  #>         alpha = alpha, imbalance.penalty = imbalance.penalty,  #>         stabilize.splits = stabilize.splits, ci.group.size = ci.group.size,  #>         compute.oob.predictions = compute.oob.predictions, num.threads = num.threads,  #>         seed = seed, reduced.form.weight = 0, legacy.seed = get_legacy_seed()) #>     tuning.output <- NULL #>     if (!identical(tune.parameters, \"none\")) { #>         if (identical(tune.parameters, \"all\")) { #>             tune.parameters <- all.tunable.params #>         } #>         else { #>             tune.parameters <- unique(match.arg(tune.parameters,  #>                 all.tunable.params, several.ok = TRUE)) #>         } #>         if (!honesty) { #>             tune.parameters <- tune.parameters[!grepl(\"honesty\",  #>                 tune.parameters)] #>         } #>         tune.parameters.defaults <- default.parameters[tune.parameters] #>         tuning.output <- tune_forest(data = data, nrow.X = nrow(X),  #>             ncol.X = ncol(X), args = args, tune.parameters = tune.parameters,  #>             tune.parameters.defaults = tune.parameters.defaults,  #>             tune.num.trees = tune.num.trees, tune.num.reps = tune.num.reps,  #>             tune.num.draws = tune.num.draws, train = causal_train) #>         args <- utils::modifyList(args, as.list(tuning.output[[\"params\"]])) #>     } #>     forest <- do.call.rcpp(causal_train, c(data, args)) #>     class(forest) <- c(\"causal_forest\", \"grf\") #>     forest[[\"seed\"]] <- seed #>     forest[[\"num.threads\"]] <- num.threads #>     forest[[\"ci.group.size\"]] <- ci.group.size #>     forest[[\"X.orig\"]] <- X #>     forest[[\"Y.orig\"]] <- Y #>     forest[[\"W.orig\"]] <- W #>     forest[[\"Y.hat\"]] <- Y.hat #>     forest[[\"W.hat\"]] <- W.hat #>     forest[[\"clusters\"]] <- clusters #>     forest[[\"equalize.cluster.weights\"]] <- equalize.cluster.weights #>     forest[[\"sample.weights\"]] <- sample.weights #>     forest[[\"tunable.params\"]] <- args[all.tunable.params] #>     forest[[\"tuning.output\"]] <- tuning.output #>     forest[[\"has.missing.values\"]] <- has.missing.values #>     forest #> } #> <bytecode: 0x000002595af94430> #> <environment: namespace:grf>"},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"function validates provided baseline policy sets default baseline policy zeros individuals.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"","code":"test_baseline_policy(baseline_policy, n)"},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"baseline_policy list representing baseline policy individual. NULL, default baseline policy zeros created. n integer specifying number individuals population.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"validated default baseline policy list numeric values.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"","code":"# Example: Default baseline policy baseline_policy <- test_baseline_policy(NULL, n = 10)  # Example: Valid baseline policy valid_policy <- as.list(rep(1, 10)) baseline_policy <- test_baseline_policy(valid_policy, n = 10)  # Example: Invalid baseline policy if (FALSE) { # \\dontrun{ invalid_policy <- c(1, 0, 1, 0) baseline_policy <- test_baseline_policy(invalid_policy, n = 10) } # }"},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate or Generate Batch Assignments ‚Äî test_batch","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"function validates provided batch assignment generates random batch assignments individuals.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"","code":"test_batch(batch, n)"},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"batch Either integer specifying number batches vector/list batch assignments individuals. n integer specifying number individuals population.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"list containing: batches list element contains indices individuals assigned specific batch. nb_batch total number batches.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"","code":"# Example: Generate random batch assignments result <- test_batch(3, n = 9) print(result) #> $batches #> $batches$`1` #> [1] 3 1 5 #>  #> $batches$`2` #> [1] 8 6 7 #>  #> $batches$`3` #> [1] 2 9 4 #>  #>  #> $nb_batch #> [1] 3 #>   # Example: Validate a batch assignment vector batch_vector <- c(1, 1, 2, 2, 3, 3, 1, 2, 3) result <- test_batch(batch_vector, n = 9) print(result) #> $batches #> $batches$`1` #> [1] 1 2 7 #>  #> $batches$`2` #> [1] 3 4 8 #>  #> $batches$`3` #> [1] 5 6 9 #>  #>  #> $nb_batch #> [1] 3 #>   # Example: Invalid batch assignment if (FALSE) { # \\dontrun{ invalid_batch <- c(1, 1, 2) result <- test_batch(invalid_batch, n = 9) } # }"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate User-Provided Parameters for a Model ‚Äî validate_params","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"function validates user-provided parameters formal arguments specified model function. ensures user-specified parameters recognized model raises error invalid parameters.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"","code":"validate_params(model_function, model_type, learner_type, user_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"model_function model function parameters validated (e.g., grf::causal_forest). model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". user_params named list parameters provided user.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"named list validated parameters safe pass model function.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"","code":"# Example with causal_forest from grf library(grf) set.seed(123) my_X <- matrix(rnorm(1000), nrow = 100, ncol = 10)  # Covariates my_Y <- rnorm(100)                                  # Outcome variable my_W <- sample(0:1, 100, replace = TRUE)            # Binary treatment indicator # Define user parameters user_params <- list(num.trees = 100)  # Validate parameters valid_params <- validate_params(grf::causal_forest, \"causal_forest\", NULL, user_params)  # Use the validated parameters to call the model # X, Y, W must still be passed explicitly cf_model <- do.call(grf::causal_forest, c(list(X = my_X, Y = my_Y, W = my_W), valid_params))"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"function validates user-provided parameters Feedforward Neural Network (FNN) model. ensures correct structure input_layer, layers, output_layer, compile_args fit_params.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"","code":"validate_params_fnn(model_type, learner_type, model_params, X)"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params named list parameters provided user configuring FNN model. X matrix data frame covariates parameters validated.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"named list validated parameters merged defaults missing values.","code":""},{"path":"https://yanisvdc.github.io/cramR/news/index.html","id":"cramr-010","dir":"Changelog","previous_headings":"","what":"cramR 0.1.0","title":"cramR 0.1.0","text":"Initial CRAN submission.","code":""}]

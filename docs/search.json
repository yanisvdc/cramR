[{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"what-is-cram_bandit","dir":"Articles","previous_headings":"","what":"What is cram_bandit()?","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"cram_bandit() function performs -policy statistical evaluation contextual bandit policies using CRAM method. Given: - pi: 3D array entry pi[j, t, ] probability choosing arm time t context j - arm: vector actions chosen timestep - reward: observed rewards - alpha: Confidence level inference returns: - point estimate policy value - Standard error confidence interval - Results tabular interactive format","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"example-simulated-contextual-bandit-evaluation","dir":"Articles","previous_headings":"","what":"Example: Simulated Contextual Bandit Evaluation","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"","code":"# Set random seed for reproducibility set.seed(42)  # Define parameters T <- 100  # Number of timesteps K <- 4    # Number of arms  # Simulate a 3D array `pi` of shape (T, T, K) # - First dimension: Individuals (context Xj) # - Second dimension: Time steps (pi_t) # - Third dimension: Arms (depth) pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K))  # Normalize probabilities so that each row sums to 1 across arms for (t in 1:T) {   for (j in 1:T) {     pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])     } }  # Simulate arm selections (randomly choosing an arm) arm <- sample(1:K, T, replace = TRUE)  # Simulate rewards (assume normally distributed rewards) reward <- rnorm(T, mean = 1, sd = 0.5)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"run-the-cram-bandit-method","dir":"Articles","previous_headings":"Example: Simulated Contextual Bandit Evaluation","what":"Run the CRAM Bandit method","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"","code":"result <- cram_bandit(pi, arm, reward)"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"summary-table","dir":"Articles","previous_headings":"Results","what":"Summary Table","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"","code":"result$raw_results #>                        Metric   Value #> 1       Policy Value Estimate 0.67621 #> 2 Policy Value Standard Error 0.04394 #> 3       Policy Value CI Lower 0.59008 #> 4       Policy Value CI Upper 0.76234"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"interactive-table","dir":"Articles","previous_headings":"Results","what":"Interactive Table","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"","code":"result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"array pi shape (T √ó T √ó K), slice along third dimension representing probability distribution arms given time. method assumes policy used collect data one evaluated (-policy). Confidence intervals based asymptotic theory using influence functions.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_bandit() for On-Policy Evaluation in Contextual Bandits","text":"method builds : -policy evaluation contextual bandits Influence-function-based variance estimation See also: DT::datatable() interactive result viewing","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"what-is-cram_bandit_sim","dir":"Articles","previous_headings":"","what":"üéØ What is cram_bandit_sim()?","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"function cram_bandit_sim() evaluates contextual bandit algorithms using CRAM method. built top contextual package, several extensions batch-aware evaluation high-precision probability tracking.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"how-it-works","dir":"Articles","previous_headings":"","what":"üß© How it works","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"CRAM contextual bandits relies accurately estimating action probabilities œÄt(Xt,)\\pi_t(X_t, a_t). essential computing unbiased -policy estimators: VÃÇœÄ=1T‚àët=1Trt‚ãÖùüô=œÄt(Xt)œÄt(Xt,) \\hat{V}_\\pi = \\frac{1}{T} \\sum_{t=1}^T \\frac{r_t \\cdot \\mathbb{1}_{a_t = \\pi_t(X_t)}}{\\pi_t(X_t, a_t)} , : Use custom subclasses contextual::Policy contextual::Bandit Store update model parameters (, b, etc.) Reconstruct probability selected arm chosen","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"custom-bandit-contextuallinearbandit","dir":"Articles","previous_headings":"","what":"üß† Custom Bandit: ContextualLinearBandit","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"extend standard linear bandit explicitly expose underlying beta matrix used reward generation. allows us compute true expected rewards compare estimated values.","code":"bandit <- ContextualLinearBandit$new(k = 3, d = 5)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"batch-aware-policies","dir":"Articles","previous_headings":"","what":"üèóÔ∏è Batch-Aware Policies","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"allow CRAM‚Äôs learning-evaluation cycles, implement batch versions common policies: batch policy updates internal parameters every batch_size steps, enabling stable, evaluable policies.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"how-probabilities-are-computed","dir":"Articles","previous_headings":"","what":"üî¢ How Probabilities Are Computed","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"use custom logic reconstruct selection probabilities œÄt(Xt,)\\pi_t(X_t, a_t) algorithm internal model state:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"epsilon-greedy","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ Epsilon-Greedy","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"exploration probability Œµ, probability selecting greedy arm : P(|Xt)=(1‚àíœµ)‚ãÖ1#greedy arms+œµK P(a_t | X_t) = \\left(1 - \\epsilon\\right) \\cdot \\frac{1}{\\#\\text{greedy arms}} + \\frac{\\epsilon}{K} detect arms greedy computing Œ∏k=Ak‚àí1bk\\theta_k = A_k^{-1} b_k evaluating expected rewards.","code":"get_proba_c_eps_greedy(eps, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"linucb-disjoint","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ LinUCB Disjoint","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"LinUCB selects arms adding exploration bonus based uncertainty: P(|Xt)=(1‚àíœµ)‚ãÖ1#best arms+œµK P(a_t | X_t) = \\left(1 - \\epsilon \\right) \\cdot \\frac{1}{\\#\\text{best arms}} + \\frac{\\epsilon}{K} accounts confidence intervals using Œºk+Œ±‚ãÖœÉk\\mu_k + \\alpha \\cdot \\sigma_k. CRAM tracks precisely using:","code":"get_proba_ucb_disjoint(alpha, eps, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"thompson-sampling","dir":"Articles","previous_headings":"üî¢ How Probabilities Are Computed","what":"‚úÖ Thompson Sampling","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"subtle: integrate probability chosen arm outperforms others given posterior uncertainty: P(|Xt)=‚Ñô[Œ∏atTXt>Œ∏kTXt‚àÄk‚â†] P(a_t | X_t) = \\mathbb{P}\\left[\\theta_{a_t}^T X_t > \\theta_k^T X_t \\quad \\forall k \\neq a_t\\right] perform numerical integration multivariate Gaussians using: computationally expensive precise.","code":"get_proba_thompson(sigma, A_list, b_list, contexts, chosen_arms)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"estimand-calculation","dir":"Articles","previous_headings":"","what":"üß™ Estimand Calculation","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"policies reconstructed, compute true expected value using independent contexts known reward function: essential evaluate bias coverage CRAM estimators.","code":"compute_estimand(data_group, list_betas, policy, policy_name, batch_size, bandit)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"optimizations","dir":"Articles","previous_headings":"","what":"üî• Optimizations","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"Probabilities vectorized across timesteps batches use Sherman-Morrison updates fast matrix inverses function extract_2d_from_3d() performs efficient 3D slicing isolate selected arm probabilities Confidence intervals calculated using asymptotic variance estimates","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"example","dir":"Articles","previous_headings":"","what":"üì¶ Example","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"","code":"# Set up bandit and policy bandit <- ContextualLinearBandit$new(k = 3, d = 5) policy <- BatchLinUCBDisjointPolicyEpsilon$new(alpha = 1.0, epsilon = 0.1, batch_size = 10)  # Run CRAM Bandit simulation results <- cram_bandit_sim(   horizon = 100,   simulations = 5,   bandit = bandit,   policy = policy,   alpha = 0.05 )  head(results)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_helpers.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Using cram_bandit_sim() with Contextual Bandit Policies","text":"contextual: Bandit simulation framework cram_bandit() ‚Äî compute value/CI one policy trajectory compute_probas() ‚Äî probability matrix reconstruction engine","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"what-is-cram_bandit_sim","dir":"Articles","previous_headings":"","what":"What is cram_bandit_sim()?","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"cram_bandit_sim() function runs -policy simulation contextual bandit algorithms using CRAM method. evaluates statistical properties policy value estimates : Prediction error Variance estimation error Empirical coverage confidence intervals useful benchmarking bandit policies controlled, simulated environments.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"requirements","dir":"Articles","previous_headings":"","what":"Requirements","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"need provide: bandit: contextual bandit environment object (e.g.¬†ContextualLinearBandit) policy: policy object (e.g.¬†BatchContextualLinTSPolicy) horizon: number time steps simulations: number repeated simulations Optional: alpha, seed, do_parallel","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"example-cram-simulation-with-lints-policy","dir":"Articles","previous_headings":"","what":"Example: CRAM Simulation with LinTS Policy","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"","code":"# Setup library(contextual)  # Define a bandit bandit <- ContextualLinearBandit$new(k = 3, d = 5, sigma = 0.1)  # Define a policy policy <- BatchContextualLinTSPolicy$new(v = 0.2, batch_size = 5)  # Run simulation result <- cram_bandit_sim(   horizon = 100,   simulations = 10,   bandit = bandit,   policy = policy,   alpha = 0.05,   do_parallel = FALSE,   seed = 123 )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"what-does-it-return","dir":"Articles","previous_headings":"","what":"What Does It Return?","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"output data.table one row per simulation, includes: estimate: estimated policy value variance_est: estimated variance estimand: true policy value (computed held-context data) prediction_error: estimate - estimand est_rel_error: relative error estimate variance_prediction_error: relative error variance ci_lower, ci_upper: bounds confidence interval std_error: standard error Plus summary metrics like average prediction error empirical coverage","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"example-output-preview","dir":"Articles","previous_headings":"","what":"Example Output Preview","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"Expected columns: sim, estimate, variance_est, estimand, prediction_error, est_rel_error, variance_prediction_error, std_error, ci_lower, ci_upper","code":"head(result)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"list_betas updated internally track true parameters per simulation first simulation discarded design (due writing issues contextual) Approximately 20% simulations excluded final error summaries (robustness outliers)","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"recommended-use-cases","dir":"Articles","previous_headings":"","what":"Recommended Use Cases","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"Validate bandit policies repeated experiments Compare bias variance different policy types Analyze empirical coverage confidence intervals Stress-test policies different batch sizes, sigma levels, dimensions","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_bandit_simulation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_bandit_sim() for On-Policy Simulation and Evaluation","text":"simulation builds : Contextual bandits (contextual package) -policy CRAM estimation Influence-function-based CI construction See also: cram_policy() -policy CRAM cram_bandit() single-run evaluation BatchContextualLinTSPolicy, LinUCBDisjointPolicyEpsilon, etc.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"what-is-cram_learning","dir":"Articles","previous_headings":"","what":"üîç What is cram_learning()?","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"cram_learning() function runs core learning routine CRAM framework. learns batch-wise policies using cumulative data splits supports: Different model types (causal_forest, s_learner, m_learner) Different learners (ridge, fnn) Flexible batching (sequential parallel) Support custom models (custom_fit, custom_predict) function typically called internally wrappers like cram_policy(), cram_simulation(), cram_ml() ‚Äî can also used standalone advanced use.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"when-to-use-it","dir":"Articles","previous_headings":"","what":"üß† When to use it?","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Use cram_learning() directly want: - control model training batch handling - debug visualize policy learning phase - inject custom models outside built-ones","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"example-running-cram_learning-with-causal-forest","dir":"Articles","previous_headings":"","what":"üìò Example: Running cram_learning() with Causal Forest","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"","code":"# Simulated data X <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D <- sample(c(0, 1), 100, replace = TRUE) Y <- rnorm(100) # Parameters batch <- 20 model_type <- 'causal_forest'      # causal_forest, s_learner, or m_learner learner_type <- NULL               # NULL for causal_forest baseline_policy <- as.list(rep(0, nrow(X)))  # or random: as.list(sample(c(0, 1), nrow(X), TRUE)) parallelize_batch <- FALSE         # Set to TRUE for parallelized learning model_params <- NULL               # e.g., list(num.trees = 100) for causal_forest # Run cram_learning learning_result <- cram_learning(   X = X,   D = D,   Y = Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"output","dir":"Articles","previous_headings":"","what":"üì¶ Output","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Returns list :","code":"str(learning_result) #> List of 3 #>  $ final_policy_model:List of 28 #>   ..$ _ci_group_size          : num 2 #>   ..$ _num_variables          : num 5 #>   ..$ _num_trees              : num 100 #>   ..$ _root_nodes             :List of 100 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. .. [list output truncated] #>   ..$ _child_nodes            :List of 100 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num 0 #>   .. .. ..$ : num 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. ..$ :List of 2 #>   .. .. ..$ : num [1:3] 1 0 0 #>   .. .. ..$ : num [1:3] 2 0 0 #>   .. .. [list output truncated] #>   ..$ _leaf_samples           :List of 100 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 91 33 98 72 13 81 29 9 36 75 ... #>   .. .. ..$ : num [1:13] 66 25 67 1 43 92 38 8 70 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 7 91 72 43 64 16 38 56 42 #>   .. .. ..$ : num [1:16] 36 68 54 67 31 90 57 25 27 71 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 3 17 11 56 63 29 43 38 77 55 ... #>   .. .. ..$ : num [1:9] 8 92 13 94 93 47 83 4 5 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 43 2 76 39 24 86 56 8 83 1 ... #>   .. .. ..$ : num [1:8] 35 94 0 22 21 85 3 52 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 65 16 46 72 54 42 80 94 88 28 ... #>   .. .. ..$ : num [1:13] 45 85 67 38 3 74 95 73 4 30 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 76 88 74 15 29 30 #>   .. .. ..$ : num [1:19] 21 26 41 85 27 77 17 0 87 73 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 41 75 62 80 40 82 #>   .. .. ..$ : num [1:19] 46 23 53 4 38 63 89 95 39 47 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:20] 53 89 24 7 66 54 92 10 40 15 ... #>   .. .. ..$ : num [1:5] 68 80 41 83 51 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 41 45 53 64 16 82 72 32 96 17 ... #>   .. .. ..$ : num [1:12] 8 83 66 50 31 77 94 76 62 1 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 64 66 94 57 14 39 84 23 78 67 ... #>   .. .. ..$ : num [1:13] 72 59 49 10 17 69 31 9 83 96 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 24 95 43 82 20 72 53 80 48 92 ... #>   .. .. ..$ : num [1:12] 90 3 52 4 16 31 34 12 11 97 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 52 82 35 33 97 84 12 23 #>   .. .. ..$ : num [1:17] 96 17 31 40 90 0 8 18 61 89 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 73 38 58 49 9 88 63 25 30 6 ... #>   .. .. ..$ : num [1:11] 12 3 68 94 98 85 57 44 21 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 6 74 39 15 44 25 66 32 38 24 ... #>   .. .. ..$ : num [1:8] 98 36 55 51 69 68 85 42 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 30 43 74 24 99 58 31 52 36 73 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 32 91 62 42 77 76 40 67 94 64 ... #>   .. .. ..$ : num [1:9] 79 53 87 7 46 13 99 56 74 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 76 88 15 57 44 54 74 64 70 46 #>   .. .. ..$ : num [1:15] 96 83 26 72 73 47 87 45 69 1 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 71 27 13 64 72 79 47 69 42 44 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 33 80 69 40 27 78 22 13 94 36 ... #>   .. .. ..$ : num [1:11] 6 4 32 14 58 48 52 19 49 92 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 45 83 92 78 86 27 64 5 22 77 ... #>   .. .. ..$ : num [1:11] 21 62 48 38 74 37 63 60 4 80 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 59 45 67 29 6 69 57 55 94 91 ... #>   .. .. ..$ : num [1:12] 13 30 3 89 1 84 38 33 43 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 33 67 74 57 35 84 79 3 94 70 ... #>   .. .. ..$ : num [1:14] 91 53 32 60 26 54 5 61 83 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 19 15 12 4 6 97 55 48 20 #>   .. .. ..$ : num [1:16] 49 96 92 34 62 8 32 60 26 69 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 62 96 52 15 53 27 77 98 59 13 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 15 73 76 36 39 94 16 63 93 66 ... #>   .. .. ..$ : num [1:13] 69 65 75 38 28 13 83 21 47 45 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 76 11 71 97 81 37 82 28 16 83 ... #>   .. .. ..$ : num [1:9] 89 10 44 38 63 47 34 52 66 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 20 26 56 69 32 60 76 75 13 #>   .. .. ..$ : num [1:16] 45 38 68 25 19 2 67 96 95 47 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:15] 9 0 61 81 49 86 67 23 5 88 ... #>   .. .. ..$ : num [1:10] 74 87 38 30 3 19 60 13 58 63 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:4] 34 81 44 57 #>   .. .. ..$ : num [1:21] 40 23 46 95 48 22 16 1 25 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:6] 55 98 60 91 81 57 #>   .. .. ..$ : num [1:19] 64 39 25 77 22 11 74 37 10 38 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 39 44 63 11 70 79 50 8 92 58 ... #>   .. .. ..$ : num [1:9] 36 51 0 3 5 26 16 41 57 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 8 92 80 28 32 76 70 50 73 30 ... #>   .. .. ..$ : num [1:14] 51 10 78 23 68 16 64 44 31 66 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 31 72 0 88 41 83 34 94 39 24 #>   .. .. ..$ : num [1:15] 37 7 12 56 53 69 89 71 99 28 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 71 69 76 24 11 36 50 43 98 94 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 36 81 14 39 91 88 86 29 #>   .. .. ..$ : num [1:17] 7 78 84 85 54 75 40 99 97 28 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 32 81 20 36 51 77 14 39 22 41 ... #>   .. .. ..$ : num [1:9] 99 79 52 28 65 12 10 48 90 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 93 74 70 71 20 85 72 91 82 14 ... #>   .. .. ..$ : num [1:9] 38 65 81 90 61 9 49 8 86 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 66 91 43 42 20 85 90 35 70 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 95 92 44 70 14 82 91 81 42 2 ... #>   .. .. ..$ : num [1:12] 25 35 74 52 43 84 21 20 71 89 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:18] 22 95 61 92 18 35 24 67 69 42 ... #>   .. .. ..$ : num [1:7] 84 56 68 16 63 99 48 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 7 41 13 22 72 82 60 35 55 64 ... #>   .. .. ..$ : num [1:9] 48 95 67 43 61 1 38 63 21 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 41 82 88 26 28 55 60 14 17 7 ... #>   .. .. ..$ : num [1:14] 35 13 51 77 22 36 93 71 57 92 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 55 78 26 86 11 88 93 5 #>   .. .. ..$ : num [1:17] 4 32 63 87 49 97 70 85 18 14 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:21] 60 73 12 44 13 35 88 55 40 53 ... #>   .. .. ..$ : num [1:4] 26 0 81 28 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 2 85 48 93 23 6 19 88 74 91 ... #>   .. .. ..$ : num [1:12] 25 26 0 61 41 96 21 45 62 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 78 93 58 57 48 14 91 16 6 15 ... #>   .. .. ..$ : num [1:12] 43 96 41 0 73 77 75 65 72 99 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 59 5 21 6 49 8 15 54 88 0 ... #>   .. .. ..$ : num [1:9] 50 68 25 1 89 79 11 52 48 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 70 4 59 93 54 23 11 95 79 48 ... #>   .. .. ..$ : num [1:8] 68 90 0 13 96 21 56 45 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 8 11 87 94 0 57 68 47 40 78 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 4 95 78 70 23 82 57 16 67 15 ... #>   .. .. ..$ : num [1:14] 34 37 77 45 62 0 11 27 47 43 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 40 80 15 6 65 46 69 72 76 78 ... #>   .. .. ..$ : num [1:8] 53 13 43 4 37 48 89 75 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 17 76 83 69 55 34 22 82 62 95 ... #>   .. .. ..$ : num [1:12] 25 89 28 13 11 10 48 56 84 37 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 48 4 52 23 39 97 36 84 76 54 #>   .. .. ..$ : num [1:15] 90 71 32 62 86 85 27 89 81 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 73 47 25 32 92 83 15 19 56 37 ... #>   .. .. ..$ : num [1:11] 61 22 7 85 98 23 27 42 52 78 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 91 24 31 57 56 82 36 20 42 27 ... #>   .. .. ..$ : num [1:11] 87 53 51 84 61 95 99 10 77 11 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 95 84 48 3 66 12 79 20 #>   .. .. ..$ : num [1:17] 90 75 56 71 73 80 10 43 41 8 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 58 32 64 20 6 95 48 85 23 40 ... #>   .. .. ..$ : num [1:11] 37 26 81 71 96 49 0 73 1 87 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 95 85 14 91 16 15 76 36 57 #>   .. .. ..$ : num [1:16] 71 87 89 49 81 26 99 25 96 83 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:21] 15 1 2 67 88 13 97 53 71 3 ... #>   .. .. ..$ : num [1:4] 38 10 8 65 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 81 37 1 22 68 12 44 63 89 14 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 45 76 54 72 32 90 42 49 12 63 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:19] 23 64 36 12 6 32 58 19 34 54 ... #>   .. .. ..$ : num [1:6] 18 75 22 98 86 90 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 2 76 4 15 66 38 42 17 8 75 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 9 72 80 20 93 71 76 #>   .. .. ..$ : num [1:18] 54 11 3 66 69 84 26 44 0 96 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 86 17 55 93 60 26 29 76 #>   .. .. ..$ : num [1:17] 53 48 50 52 14 38 18 21 64 5 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 46 64 29 58 52 33 93 66 3 6 ... #>   .. .. ..$ : num [1:14] 99 37 27 25 86 49 90 0 71 60 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 40 14 31 77 88 45 59 39 92 49 ... #>   .. .. ..$ : num [1:14] 84 30 75 50 4 52 37 56 80 19 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 72 64 6 19 14 42 17 53 3 60 ... #>   .. .. ..$ : num [1:11] 54 50 11 84 13 79 77 92 1 67 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:16] 58 6 32 36 19 42 66 74 76 5 ... #>   .. .. ..$ : num [1:9] 45 69 37 83 28 22 11 99 77 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 40 17 30 25 65 88 78 43 28 19 ... #>   .. .. ..$ : num [1:14] 1 67 95 11 13 81 58 36 37 79 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 83 5 78 84 61 42 35 28 44 7 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 43 96 10 49 66 44 19 82 89 40 ... #>   .. .. ..$ : num [1:11] 0 5 69 52 21 51 98 55 27 42 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 12 75 79 69 24 55 29 41 #>   .. .. ..$ : num [1:17] 90 44 52 65 39 49 0 37 2 70 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 34 12 88 71 54 32 70 42 29 79 ... #>   .. .. ..$ : num [1:8] 68 0 90 87 83 69 65 80 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 91 20 29 14 76 64 34 44 16 39 ... #>   .. .. ..$ : num [1:14] 45 89 13 99 72 75 8 47 83 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 20 30 45 60 26 17 55 34 16 64 ... #>   .. .. ..$ : num [1:13] 69 89 36 57 76 27 85 0 98 24 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:13] 9 61 88 5 93 45 42 81 39 73 ... #>   .. .. ..$ : num [1:12] 98 48 65 12 55 53 68 28 13 80 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 26 30 81 61 79 90 68 55 83 64 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 11 94 62 43 39 56 85 28 53 64 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 62 87 85 30 32 53 96 88 70 33 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 48 95 49 81 19 88 53 32 87 65 ... #>   .. .. ..$ : num [1:8] 75 3 69 11 42 22 12 31 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:10] 48 14 19 83 81 71 63 49 89 53 #>   .. .. ..$ : num [1:15] 74 87 77 42 96 75 3 26 11 12 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 24 15 44 16 6 42 2 19 58 5 ... #>   .. .. ..$ : num [1:14] 10 87 90 81 83 45 62 47 17 43 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:12] 42 3 19 66 6 67 24 5 54 14 ... #>   .. .. ..$ : num [1:13] 26 83 69 21 96 99 89 87 65 72 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:8] 26 56 78 21 45 86 17 2 #>   .. .. ..$ : num [1:17] 76 36 37 41 57 47 50 0 90 23 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 90 27 15 66 31 8 50 80 5 83 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 35 33 14 92 39 50 70 58 74 15 ... #>   .. .. ..$ : num [1:14] 49 98 51 18 53 10 86 38 8 77 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:17] 40 1 54 74 4 60 91 33 94 15 ... #>   .. .. ..$ : num [1:8] 98 0 51 18 86 77 13 81 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:19] 57 92 42 95 88 24 58 36 40 14 ... #>   .. .. ..$ : num [1:6] 49 56 81 83 31 28 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 91 56 1 36 35 70 19 74 75 34 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:11] 53 93 82 49 15 30 65 63 72 95 ... #>   .. .. ..$ : num [1:14] 85 3 41 94 8 46 99 24 43 98 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:9] 27 47 0 91 82 18 8 64 9 #>   .. .. ..$ : num [1:16] 3 53 65 85 19 37 38 15 46 33 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 13 45 79 72 48 34 39 95 71 47 ... #>   .. .. ..$ : num [1:11] 31 46 26 64 66 27 59 61 51 97 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:20] 51 57 55 27 71 39 29 69 45 92 ... #>   .. .. ..$ : num [1:5] 66 1 48 4 50 #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:25] 50 71 6 13 44 35 97 9 42 79 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 64 12 58 24 97 42 63 35 76 33 ... #>   .. .. ..$ : num [1:11] 26 38 60 28 51 13 8 27 9 18 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:14] 20 94 35 57 30 79 46 78 70 48 ... #>   .. .. ..$ : num [1:11] 1 18 49 26 81 47 8 41 28 51 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 41 15 78 8 49 94 2 #>   .. .. ..$ : num [1:18] 60 52 25 3 84 99 48 23 43 65 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:15] 82 57 19 91 40 72 64 5 3 58 ... #>   .. .. ..$ : num [1:10] 56 83 87 99 75 96 65 69 77 38 #>   .. .. [list output truncated] #>   ..$ _split_vars             :List of 100 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 2 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 4 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 3 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. ..$ : num [1:3] 1 0 0 #>   .. ..$ : num [1:3] 0 0 0 #>   .. .. [list output truncated] #>   ..$ _split_values           :List of 100 #>   .. ..$ : num [1:3] -0.118 -1 -1 #>   .. ..$ : num [1:3] -0.479 -1 -1 #>   .. ..$ : num [1:3] 0.0645 -1 -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] 0.0609 -1 -1 #>   .. ..$ : num [1:3] -0.247 -1 -1 #>   .. ..$ : num [1:3] -0.752 -1 -1 #>   .. ..$ : num [1:3] 0.556 -1 -1 #>   .. ..$ : num [1:3] -0.368 -1 -1 #>   .. ..$ : num [1:3] 0.112 -1 -1 #>   .. ..$ : num [1:3] 0.072 -1 -1 #>   .. ..$ : num [1:3] -0.156 -1 -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] 0.559 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.0192 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.0709 -1 -1 #>   .. ..$ : num [1:3] 0.108 -1 -1 #>   .. ..$ : num [1:3] 0.518 -1 -1 #>   .. ..$ : num [1:3] -0.244 -1 -1 #>   .. ..$ : num [1:3] -0.00557 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.363 -1 -1 #>   .. ..$ : num [1:3] 0.0655 -1 -1 #>   .. ..$ : num [1:3] 0.00789 -1 -1 #>   .. ..$ : num [1:3] 0.449 -1 -1 #>   .. ..$ : num [1:3] -0.474 -1 -1 #>   .. ..$ : num [1:3] -0.308 -1 -1 #>   .. ..$ : num [1:3] 0.393 -1 -1 #>   .. ..$ : num [1:3] 0.0208 -1 -1 #>   .. ..$ : num [1:3] -0.103 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.356 -1 -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.363 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.133 -1 -1 #>   .. ..$ : num [1:3] 0.259 -1 -1 #>   .. ..$ : num [1:3] 0.112 -1 -1 #>   .. ..$ : num [1:3] -0.368 -1 -1 #>   .. ..$ : num [1:3] -0.0103 -1 -1 #>   .. ..$ : num [1:3] 0.524 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.427 -1 -1 #>   .. ..$ : num [1:3] 0.434 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.604 -1 -1 #>   .. ..$ : num [1:3] 0.429 -1 -1 #>   .. ..$ : num [1:3] -0.016 -1 -1 #>   .. ..$ : num [1:3] 0.199 -1 -1 #>   .. ..$ : num [1:3] -0.0779 -1 -1 #>   .. ..$ : num [1:3] -0.0526 -1 -1 #>   .. ..$ : num [1:3] 0.118 -1 -1 #>   .. ..$ : num [1:3] 0.0465 -1 -1 #>   .. ..$ : num [1:3] 0.468 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.344 -1 -1 #>   .. ..$ : num [1:3] -0.341 -1 -1 #>   .. ..$ : num [1:3] -0.00557 -1 -1 #>   .. ..$ : num [1:3] 0.192 -1 -1 #>   .. ..$ : num [1:3] -0.264 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] -0.262 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.408 -1 -1 #>   .. ..$ : num [1:3] -0.341 -1 -1 #>   .. ..$ : num [1:3] 0.424 -1 -1 #>   .. ..$ : num [1:3] 0.255 -1 -1 #>   .. ..$ : num [1:3] -0.323 -1 -1 #>   .. ..$ : num [1:3] 0.122 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.337 -1 -1 #>   .. ..$ : num [1:3] -0.105 -1 -1 #>   .. ..$ : num [1:3] 0.0465 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] -0.489 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.172 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num [1:3] 0.244 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] -0.105 -1 -1 #>   .. ..$ : num [1:3] -0.163 -1 -1 #>   .. ..$ : num [1:3] 0.344 -1 -1 #>   .. ..$ : num [1:3] 0.376 -1 -1 #>   .. ..$ : num -1 #>   .. ..$ : num [1:3] 0.07 -1 -1 #>   .. ..$ : num [1:3] 0.176 -1 -1 #>   .. ..$ : num [1:3] 0.0673 -1 -1 #>   .. ..$ : num [1:3] 0.424 -1 -1 #>   .. .. [list output truncated] #>   ..$ _drawn_samples          :List of 100 #>   .. ..$ : num [1:50] 38 35 31 12 81 33 43 6 9 72 ... #>   .. ..$ : num [1:50] 98 44 11 56 16 43 33 38 81 93 ... #>   .. ..$ : num [1:50] 40 29 92 94 63 13 37 76 93 47 ... #>   .. ..$ : num [1:50] 17 1 47 21 52 56 8 51 18 24 ... #>   .. ..$ : num [1:50] 21 52 56 67 55 4 60 44 66 6 ... #>   .. ..$ : num [1:50] 77 79 80 47 21 6 85 87 22 88 ... #>   .. ..$ : num [1:50] 92 96 20 84 45 54 79 15 89 42 ... #>   .. ..$ : num [1:50] 98 42 39 45 40 3 21 4 13 82 ... #>   .. ..$ : num [1:50] 29 96 59 94 61 56 75 17 38 41 ... #>   .. ..$ : num [1:50] 26 49 50 33 71 75 48 76 67 66 ... #>   .. ..$ : num [1:50] 52 0 18 42 3 23 21 75 96 43 ... #>   .. ..$ : num [1:50] 80 23 0 18 61 98 33 35 24 11 ... #>   .. ..$ : num [1:50] 21 63 24 38 70 67 61 74 75 58 ... #>   .. ..$ : num [1:50] 9 85 39 74 88 24 99 51 38 58 ... #>   .. ..$ : num [1:50] 64 12 94 87 59 1 68 0 52 74 ... #>   .. ..$ : num [1:50] 56 1 43 24 94 69 31 96 0 32 ... #>   .. ..$ : num [1:50] 57 27 56 82 41 77 45 21 91 72 ... #>   .. ..$ : num [1:50] 73 78 45 8 33 82 88 13 65 66 ... #>   .. ..$ : num [1:50] 14 66 45 7 80 64 40 21 6 8 ... #>   .. ..$ : num [1:50] 22 52 49 77 35 38 83 92 27 66 ... #>   .. ..$ : num [1:50] 53 87 45 1 21 32 30 56 50 16 ... #>   .. ..$ : num [1:50] 37 74 53 88 58 35 9 55 56 43 ... #>   .. ..$ : num [1:50] 45 61 13 11 80 95 36 4 19 56 ... #>   .. ..$ : num [1:50] 27 49 5 20 62 97 51 45 34 80 ... #>   .. ..$ : num [1:50] 66 13 61 57 45 41 15 51 83 73 ... #>   .. ..$ : num [1:50] 36 39 28 34 8 51 78 45 81 13 ... #>   .. ..$ : num [1:50] 38 5 86 6 13 47 95 25 88 59 ... #>   .. ..$ : num [1:50] 58 60 29 23 75 9 54 63 3 11 ... #>   .. ..$ : num [1:50] 95 57 52 19 1 55 87 37 34 80 ... #>   .. ..$ : num [1:50] 77 10 51 4 84 16 40 39 86 19 ... #>   .. ..$ : num [1:50] 23 36 11 54 58 70 39 3 49 41 ... #>   .. ..$ : num [1:50] 68 55 39 59 78 23 80 65 31 11 ... #>   .. ..$ : num [1:50] 43 15 85 12 59 11 52 81 55 0 ... #>   .. ..$ : num [1:50] 99 34 56 94 43 7 2 37 6 70 ... #>   .. ..$ : num [1:50] 48 32 54 14 96 31 52 51 20 39 ... #>   .. ..$ : num [1:50] 32 86 2 81 12 64 48 90 93 29 ... #>   .. ..$ : num [1:50] 48 90 17 38 70 14 91 71 8 58 ... #>   .. ..$ : num [1:50] 32 86 91 11 65 13 40 20 49 21 ... #>   .. ..$ : num [1:50] 52 54 95 29 21 61 80 66 42 67 ... #>   .. ..$ : num [1:50] 91 25 89 9 61 99 54 14 80 42 ... #>   .. ..$ : num [1:50] 13 93 52 41 19 89 15 83 17 61 ... #>   .. ..$ : num [1:50] 36 61 15 1 64 95 7 9 46 16 ... #>   .. ..$ : num [1:50] 18 65 23 53 28 40 99 87 68 44 ... #>   .. ..$ : num [1:50] 97 81 32 86 56 35 44 60 5 26 ... #>   .. ..$ : num [1:50] 59 89 61 47 45 14 51 2 3 70 ... #>   .. ..$ : num [1:50] 91 73 89 68 2 70 79 48 76 26 ... #>   .. ..$ : num [1:50] 88 10 23 8 21 77 6 70 13 2 ... #>   .. ..$ : num [1:50] 12 11 77 63 95 1 4 13 62 93 ... #>   .. ..$ : num [1:50] 87 11 68 69 90 24 3 93 8 78 ... #>   .. ..$ : num [1:50] 69 16 77 62 93 47 40 4 87 24 ... #>   .. ..$ : num [1:50] 4 13 43 7 15 65 18 22 62 99 ... #>   .. ..$ : num [1:50] 62 29 97 90 50 17 92 76 45 47 ... #>   .. ..$ : num [1:50] 62 66 35 78 94 4 18 96 27 90 ... #>   .. ..$ : num [1:50] 35 73 79 3 92 76 89 11 66 29 ... #>   .. ..$ : num [1:50] 80 81 36 21 48 8 51 54 16 75 ... #>   .. ..$ : num [1:50] 99 77 73 3 40 64 95 9 0 74 ... #>   .. ..$ : num [1:50] 6 75 41 20 72 17 95 65 85 73 ... #>   .. ..$ : num [1:50] 95 6 16 81 29 48 69 87 30 75 ... #>   .. ..$ : num [1:50] 71 8 90 15 93 95 2 34 76 24 ... #>   .. ..$ : num [1:50] 31 81 90 68 95 70 29 65 2 77 ... #>   .. ..$ : num [1:50] 65 15 93 54 53 61 77 25 6 36 ... #>   .. ..$ : num [1:50] 41 74 60 65 35 46 77 73 25 36 ... #>   .. ..$ : num [1:50] 38 20 19 0 14 2 59 75 26 70 ... #>   .. ..$ : num [1:50] 66 23 57 2 72 17 8 50 21 77 ... #>   .. ..$ : num [1:50] 53 26 17 86 48 73 52 93 18 89 ... #>   .. ..$ : num [1:50] 37 96 53 91 11 87 46 0 90 33 ... #>   .. ..$ : num [1:50] 17 9 75 39 56 52 99 59 71 66 ... #>   .. ..$ : num [1:50] 52 79 75 12 88 17 50 66 59 45 ... #>   .. ..$ : num [1:50] 37 30 44 42 45 1 76 87 77 78 ... #>   .. ..$ : num [1:50] 11 30 63 58 24 37 69 22 19 52 ... #>   .. ..$ : num [1:50] 46 72 40 51 27 83 21 56 17 19 ... #>   .. ..$ : num [1:50] 46 40 43 5 7 37 49 12 27 81 ... #>   .. ..$ : num [1:50] 9 83 35 78 75 24 31 33 79 71 ... #>   .. ..$ : num [1:50] 3 44 32 39 42 69 29 55 78 57 ... #>   .. ..$ : num [1:50] 4 18 64 51 56 98 20 75 31 47 ... #>   .. ..$ : num [1:50] 55 99 64 29 58 14 72 45 89 27 ... #>   .. ..$ : num [1:50] 54 25 26 76 52 36 65 89 47 46 ... #>   .. ..$ : num [1:50] 5 91 76 93 74 61 36 71 54 13 ... #>   .. ..$ : num [1:50] 17 95 15 66 38 12 44 99 9 85 ... #>   .. ..$ : num [1:50] 95 79 90 12 28 69 56 54 93 3 ... #>   .. ..$ : num [1:50] 14 48 87 31 75 54 28 15 33 7 ... #>   .. ..$ : num [1:50] 89 15 88 44 48 96 77 7 28 9 ... #>   .. ..$ : num [1:50] 52 16 22 47 20 3 92 98 57 28 ... #>   .. ..$ : num [1:50] 0 54 44 89 74 10 39 65 24 84 ... #>   .. ..$ : num [1:50] 21 49 42 31 60 54 95 29 46 30 ... #>   .. ..$ : num [1:50] 49 5 56 37 21 54 42 26 14 76 ... #>   .. ..$ : num [1:50] 8 51 61 39 32 15 98 40 97 49 ... #>   .. ..$ : num [1:50] 18 21 14 13 59 33 77 49 79 98 ... #>   .. ..$ : num [1:50] 99 87 59 96 77 49 74 31 66 70 ... #>   .. ..$ : num [1:50] 5 57 6 59 24 56 92 15 19 9 ... #>   .. ..$ : num [1:50] 21 82 40 18 9 98 46 63 0 83 ... #>   .. ..$ : num [1:50] 3 93 15 7 94 88 9 22 19 49 ... #>   .. ..$ : num [1:50] 92 50 79 69 46 68 58 39 59 32 ... #>   .. ..$ : num [1:50] 43 90 47 20 34 32 11 26 9 71 ... #>   .. ..$ : num [1:50] 12 50 35 34 93 53 67 8 95 33 ... #>   .. ..$ : num [1:50] 36 99 96 28 8 67 62 18 16 27 ... #>   .. ..$ : num [1:50] 20 47 63 41 17 48 57 34 18 2 ... #>   .. ..$ : num [1:50] 34 15 49 65 38 22 82 51 78 99 ... #>   .. ..$ : num [1:50] 39 86 75 87 62 23 13 96 8 30 ... #>   .. .. [list output truncated] #>   ..$ _send_missing_left      :List of 100 #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. ..$ : logi [1:3] TRUE TRUE TRUE #>   .. .. [list output truncated] #>   ..$ _pv_values              :List of 100 #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.1434 -0.2106 -0.2106 -0.0856 0.2587 ... #>   .. .. ..$ : num [1:7] 0.1616 0.0373 0.0373 0.2264 0.2705 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.6 -0.175 -0.175 0.226 0.298 ... #>   .. .. ..$ : num [1:7] 0.0253 -0.1244 -0.1244 0.0712 0.2635 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.4359 -0.0281 -0.0281 -0.0255 0.2743 ... #>   .. .. ..$ : num [1:7] -0.4072 0.0124 0.0124 -0.0373 0.2478 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.093 0.086 0.086 0.0512 0.2665 ... #>   .. .. ..$ : num [1:7] -0.131 0.369 0.369 -0.359 0.287 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.283 0.154 0.154 0.187 0.281 ... #>   .. .. ..$ : num [1:7] 0.1205 0.0123 0.0123 -0.1004 0.2563 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.137 0.137 0.137 -0.234 0.208 ... #>   .. .. ..$ : num [1:7] 0.0188 0.0851 0.0851 0.0074 0.2602 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5369 -0.146 -0.146 -0.0117 0.2712 ... #>   .. .. ..$ : num [1:7] -0.157 -0.147 -0.147 0.112 0.25 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2483 -0.1215 -0.1215 -0.0178 0.2653 ... #>   .. .. ..$ : num [1:7] -0.163 0.132 0.132 0.044 0.281 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0735 -0.0312 -0.0312 -0.1274 0.2736 ... #>   .. .. ..$ : num [1:7] -0.0507 0.0802 0.0802 0.0785 0.2524 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.1 0.143 0.143 -0.13 0.261 ... #>   .. .. ..$ : num [1:7] -0.227 -0.0169 -0.0169 0.1358 0.2472 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.00532 -0.18847 -0.18847 -0.01481 0.23083 ... #>   .. .. ..$ : num [1:7] -0.4153 -0.2061 -0.2061 -0.0864 0.2705 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.5007 -0.1714 -0.1714 0.0625 0.259 ... #>   .. .. ..$ : num [1:7] -0.088 -0.1313 -0.1313 -0.0434 0.248 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0566 0.155 0.155 0.0704 0.236 ... #>   .. .. ..$ : num [1:7] -0.3628 0.0456 0.0456 -0.0648 0.3231 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0635 0.132 0.132 0.0597 0.2513 ... #>   .. .. ..$ : num [1:7] 0.312 0.164 0.164 0.274 0.297 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.0987 -0.052 -0.052 0.1134 0.261 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5017 0.0587 0.0587 0.0437 0.2645 ... #>   .. .. ..$ : num [1:7] 0.27 0.141 0.141 0.174 0.226 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3428 0.0475 0.0475 -0.2269 0.2681 ... #>   .. .. ..$ : num [1:7] -0.386624 -0.038608 -0.038608 0.000421 0.271994 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.33512 -0.07353 -0.07353 -0.00189 0.26369 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.362 -0.213 -0.213 -0.192 0.262 ... #>   .. .. ..$ : num [1:7] 0.0422 0.2217 0.2217 -0.0312 0.2391 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.256 0.0651 0.0651 0.0559 0.2701 ... #>   .. .. ..$ : num [1:7] 0.1232 0.0491 0.0491 -0.0965 0.2532 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0122 0.0532 0.0532 -0.1579 0.2809 ... #>   .. .. ..$ : num [1:7] -0.071 -0.257 -0.257 0.148 0.28 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.285 0.013 0.013 -0.36 0.261 ... #>   .. .. ..$ : num [1:7] 0.18211 0.00624 0.00624 -0.09823 0.25469 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1411 -0.0856 -0.0856 0.2372 0.2556 ... #>   .. .. ..$ : num [1:7] -0.11206 0.13994 0.13994 0.00914 0.2775 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.18888 0.13044 0.13044 0.00613 0.24482 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.07789 -0.12131 -0.12131 -0.00407 0.27233 ... #>   .. .. ..$ : num [1:7] -0.5053 -0.0324 -0.0324 0.0621 0.2785 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0164 -0.0126 -0.0126 -0.0951 0.241 ... #>   .. .. ..$ : num [1:7] -0.3475 -0.0799 -0.0799 0.1586 0.2471 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.00292 -0.1469 -0.1469 0.1695 0.23999 ... #>   .. .. ..$ : num [1:7] 0.069782 -0.000126 -0.000126 0.043476 0.247289 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.13051 0.08542 0.08542 -0.00868 0.24723 ... #>   .. .. ..$ : num [1:7] 0.1217 -0.2042 -0.2042 0.0107 0.2377 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2682 -0.0303 -0.0303 -0.0124 0.2998 ... #>   .. .. ..$ : num [1:7] -0.0847 -0.1186 -0.1186 0.0755 0.2418 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.35 0.177 0.177 0.135 0.287 ... #>   .. .. ..$ : num [1:7] -0.1774 -0.0477 -0.0477 -0.0665 0.2469 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.2086 -0.0352 -0.0352 -0.099 0.2604 ... #>   .. .. ..$ : num [1:7] 0.1494 -0.1744 -0.1744 -0.0491 0.2932 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0937 0.1195 0.1195 -0.0953 0.2453 ... #>   .. .. ..$ : num [1:7] -0.2555 -0.0744 -0.0744 0.0236 0.2843 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1891 0.0861 0.0861 -0.1655 0.2727 ... #>   .. .. ..$ : num [1:7] -0.1163 -0.0871 -0.0871 0.2171 0.264 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.297 0.1708 0.1708 -0.0269 0.2354 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.319 -0.0463 -0.0463 -0.2839 0.2803 ... #>   .. .. ..$ : num [1:7] -0.3788 -0.0457 -0.0457 -0.0509 0.2643 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0811 0.0789 0.0789 0.0211 0.2738 ... #>   .. .. ..$ : num [1:7] -0.9136 -0.0681 -0.0681 0.2144 0.2568 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.07215 -0.00358 -0.00358 -0.06436 0.27236 ... #>   .. .. ..$ : num [1:7] 0.38 0.173 0.173 0.323 0.275 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.1496 -0.0498 -0.0498 0.1231 0.2756 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3115 0.0369 0.0369 0.0424 0.2886 ... #>   .. .. ..$ : num [1:7] -0.6055 0.0834 0.0834 -0.1047 0.2191 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.320212 0.040369 0.040369 0.000657 0.264658 ... #>   .. .. ..$ : num [1:7] -0.0359 -0.0429 -0.0429 0.1945 0.2513 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2933 -0.0579 -0.0579 -0.0142 0.2544 ... #>   .. .. ..$ : num [1:7] -0.0794 -0.1119 -0.1119 0.0164 0.2599 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2977 -0.0183 -0.0183 -0.1142 0.26 ... #>   .. .. ..$ : num [1:7] -0.11953 -0.09832 -0.09832 0.00601 0.2564 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.181 0.124 0.124 -0.016 0.256 ... #>   .. .. ..$ : num [1:7] -0.1017 0.2074 0.2074 0.0553 0.2534 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.2453 -0.0477 -0.0477 0.088 0.2628 ... #>   .. .. ..$ : num [1:7] -0.1571 0.2505 0.2505 -0.0195 0.2327 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.162 0.0194 0.0194 -0.2 0.2557 ... #>   .. .. ..$ : num [1:7] -0.127 0.0355 0.0355 0.1246 0.2692 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0557 -0.0681 -0.0681 -0.2782 0.2812 ... #>   .. .. ..$ : num [1:7] -0.579 0.105 0.105 0.082 0.253 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0236 0.0534 0.0534 -0.0865 0.2659 ... #>   .. .. ..$ : num [1:7] -0.541 0.167 0.167 -0.184 0.232 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.05813 -0.00118 -0.00118 -0.31238 0.25002 ... #>   .. .. ..$ : num [1:7] -0.259 0.1575 0.1575 0.0419 0.2883 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0723 0.1622 0.1622 -0.1906 0.2577 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.147 -0.147 -0.147 -0.206 0.244 ... #>   .. .. ..$ : num [1:7] -0.0584 0.0075 0.0075 -0.0885 0.2528 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.21198 0.00653 0.00653 0.04426 0.24045 ... #>   .. .. ..$ : num [1:7] -0.2251 -0.2601 -0.2601 0.0559 0.2331 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.684014 0.000903 0.000903 -0.135834 0.261857 ... #>   .. .. ..$ : num [1:7] -0.3293 -0.0824 -0.0824 0.1466 0.2537 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.047 -0.155 -0.155 -0.198 0.27 ... #>   .. .. ..$ : num [1:7] -0.00826 0.24196 0.24196 0.10557 0.26012 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0023 0.0543 0.0543 -0.0516 0.2451 ... #>   .. .. ..$ : num [1:7] -0.0877 0.0492 0.0492 0.0299 0.2692 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.4313 -0.1403 -0.1403 -0.0738 0.2713 ... #>   .. .. ..$ : num [1:7] -0.3991 -0.0413 -0.0413 0.1585 0.2259 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2092 -0.1641 -0.1641 -0.0101 0.2645 ... #>   .. .. ..$ : num [1:7] -0.101 -0.07 -0.07 0.142 0.274 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2424 -0.0774 -0.0774 -0.247 0.2512 ... #>   .. .. ..$ : num [1:7] 0.1411 0.2438 0.2438 0.0474 0.2794 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.061 0.0438 0.0438 -0.123 0.2773 ... #>   .. .. ..$ : num [1:7] 0.2601 -0.0357 -0.0357 0.1061 0.2293 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0174 -0.2038 -0.2038 -0.1081 0.2913 ... #>   .. .. ..$ : num [1:7] -0.3657 -0.0115 -0.0115 0.6287 0.3105 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.151 -0.0247 -0.0247 0.0997 0.2755 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.039 -0.0311 -0.0311 0.2737 0.2709 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.3079 -0.2607 -0.2607 0.0915 0.2807 ... #>   .. .. ..$ : num [1:7] -0.281 0.193 0.193 0.103 0.273 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0407 0.0955 0.0955 -0.0265 0.2506 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0891 -0.24 -0.24 0.1725 0.2584 ... #>   .. .. ..$ : num [1:7] -0.0293 0.0214 0.0214 -0.205 0.2618 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5816 -0.1006 -0.1006 0.0784 0.2591 ... #>   .. .. ..$ : num [1:7] 0.00782 0.22312 0.22312 -0.10291 0.26679 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5206 -0.2007 -0.2007 -0.0741 0.2757 ... #>   .. .. ..$ : num [1:7] 0.1234 -0.0596 -0.0596 0.1997 0.2517 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2418 -0.0606 -0.0606 -0.1878 0.268 ... #>   .. .. ..$ : num [1:7] -0.1779 -0.0247 -0.0247 -0.2202 0.2433 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.481 -0.1452 -0.1452 0.0851 0.2667 ... #>   .. .. ..$ : num [1:7] -0.35 0.11 0.11 -0.162 0.241 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.295 0.118 0.118 0.157 0.25 ... #>   .. .. ..$ : num [1:7] -0.2211 -0.045 -0.045 -0.0855 0.2421 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1829 -0.0979 -0.0979 -0.1224 0.2157 ... #>   .. .. ..$ : num [1:7] 0.0791 -0.1033 -0.1033 -0.0849 0.255 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.19424 0.13646 0.13646 0.00721 0.24229 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0913 -0.1505 -0.1505 0.2721 0.2664 ... #>   .. .. ..$ : num [1:7] -0.12944 0.17027 0.17027 0.00202 0.2774 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.14 0.00348 0.00348 0.24093 0.29438 ... #>   .. .. ..$ : num [1:7] 0.2361 0.0165 0.0165 -0.1712 0.2849 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.153 -0.186 -0.186 -0.198 0.291 ... #>   .. .. ..$ : num [1:7] 0.0948 0.1405 0.1405 -0.0393 0.2441 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.428 -0.348 -0.348 -0.248 0.287 ... #>   .. .. ..$ : num [1:7] -0.627 -0.221 -0.221 0.131 0.254 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.276 -0.3847 -0.3847 0.0224 0.2445 ... #>   .. .. ..$ : num [1:7] 0.0934 0.1072 0.1072 -0.0625 0.2662 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.2431 -0.058 -0.058 -0.0527 0.2623 ... #>   .. .. ..$ : num [1:7] -0.0646 0.0923 0.0923 0.1512 0.2782 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.1905 0.0987 0.0987 0.0433 0.2455 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.4114 0.0431 0.0431 -0.1235 0.2758 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.171 0.0472 0.0472 0.0164 0.2587 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.237 0.143 0.143 -0.018 0.257 ... #>   .. .. ..$ : num [1:7] 0.174 -0.14 -0.14 0.249 0.294 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0508 0.0221 0.0221 0.1389 0.2508 ... #>   .. .. ..$ : num [1:7] 0.0127 0.0227 0.0227 0.0598 0.2685 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0945 0.029 0.029 -0.0364 0.2586 ... #>   .. .. ..$ : num [1:7] -0.179 0.0673 0.0673 0.2349 0.231 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.5913 -0.0178 -0.0178 -0.0182 0.2644 ... #>   .. .. ..$ : num [1:7] -0.36618 0.132671 0.132671 -0.000512 0.255963 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.177 0.17 0.17 -0.133 0.265 ... #>   .. .. ..$ : num [1:7] -0.0474 -0.104 -0.104 0.013 0.2617 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.0844 -0.1482 -0.1482 -0.1062 0.2587 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.17712 -0.00182 -0.00182 -0.36857 0.24232 ... #>   .. .. ..$ : num [1:7] -0.2335 0.0897 0.0897 0.1864 0.2765 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0761 0.0472 0.0472 -0.2607 0.252 ... #>   .. .. ..$ : num [1:7] -0.3197 0.1377 0.1377 0.0382 0.2496 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0514 0.0258 0.0258 -0.1524 0.254 ... #>   .. .. ..$ : num [1:7] 0.442 0.358 0.358 0.154 0.263 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] -0.098 -0.1401 -0.1401 -0.0548 0.2536 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0509 -0.0355 -0.0355 0.173 0.2508 ... #>   .. .. ..$ : num [1:7] 0.00194 0.13453 0.13453 0.02499 0.27838 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.0342 -0.0847 -0.0847 -0.2556 0.2816 ... #>   .. .. ..$ : num [1:7] 0.28422 0.00692 0.00692 -0.05958 0.25885 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.138 -0.2345 -0.2345 0.0462 0.2486 ... #>   .. .. ..$ : num [1:7] 0.1353 -0.2393 -0.2393 -0.0199 0.2392 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.0258 -0.1609 -0.1609 -0.026 0.2696 ... #>   .. .. ..$ : num [1:7] -0.0309 0.265 0.265 -0.1258 0.2168 ... #>   .. ..$ :List of 1 #>   .. .. ..$ : num [1:7] 0.02149 0.00541 0.00541 0.03591 0.26366 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.112 -0.184 -0.184 0.157 0.283 ... #>   .. .. ..$ : num [1:7] -0.2495 -0.0508 -0.0508 0.1312 0.2648 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.02079 -0.00497 -0.00497 -0.1303 0.24454 ... #>   .. .. ..$ : num [1:7] -0.227 0.162 0.162 0.196 0.281 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] -0.1716 0.367 0.367 0.0641 0.2971 ... #>   .. .. ..$ : num [1:7] -0.1606 -0.0407 -0.0407 0.1448 0.2398 ... #>   .. ..$ :List of 3 #>   .. .. ..$ : num(0)  #>   .. .. ..$ : num [1:7] 0.612 -0.237 -0.237 -0.199 0.295 ... #>   .. .. ..$ : num [1:7] 0.0457 0.1203 0.1203 0.205 0.2378 ... #>   .. .. [list output truncated] #>   ..$ _pv_num_types           : num 7 #>   ..$ predictions             : num [1:100, 1] 0.1017 0.0629 -0.0916 -0.1134 0.0676 ... #>   ..$ variance.estimates      : num[0 , 0 ]  #>   ..$ debiased.error          : num [1:100, 1] 0.0521 0.0654 0.6136 3.7177 1.6982 ... #>   ..$ excess.error            : num [1:100, 1] 0.00211 0.00225 0.00213 0.00569 0.00172 ... #>   ..$ seed                    : num 9.8e+08 #>   ..$ num.threads             : num 0 #>   ..$ ci.group.size           : num 2 #>   ..$ X.orig                  : num [1:100, 1:5] 0.9354 0.2134 -0.0381 -0.9358 -0.1557 ... #>   ..$ Y.orig                  : num [1:100] -0.527 0.234 -0.771 1.807 -1.437 ... #>   ..$ W.orig                  : num [1:100] 1 1 1 0 1 0 1 0 0 1 ... #>   ..$ Y.hat                   : num [1:100] -0.3849 -0.0265 -0.0267 -0.1986 -0.2333 ... #>   ..$ W.hat                   : num [1:100] 0.509 0.435 0.478 0.639 0.621 ... #>   ..$ clusters                : num(0)  #>   ..$ equalize.cluster.weights: logi FALSE #>   ..$ tunable.params          :List of 7 #>   .. ..$ sample.fraction     : num 0.5 #>   .. ..$ mtry                : num 5 #>   .. ..$ min.node.size       : num 5 #>   .. ..$ honesty.fraction    : num 0.5 #>   .. ..$ honesty.prune.leaves: logi TRUE #>   .. ..$ alpha               : num 0.05 #>   .. ..$ imbalance.penalty   : num 0 #>   ..$ has.missing.values      : logi FALSE #>   ..- attr(*, \"class\")= chr [1:2] \"causal_forest\" \"grf\" #>  $ policies          : num [1:100, 1:21] 0 0 0 0 0 0 0 0 0 0 ... #>  $ batch_indices     :List of 20 #>   ..$ 1 : int [1:5] 31 60 94 27 88 #>   ..$ 2 : int [1:5] 4 71 80 83 12 #>   ..$ 3 : int [1:5] 15 22 62 45 18 #>   ..$ 4 : int [1:5] 57 64 25 2 43 #>   ..$ 5 : int [1:5] 97 13 69 17 87 #>   ..$ 6 : int [1:5] 99 40 92 75 14 #>   ..$ 7 : int [1:5] 44 58 47 90 35 #>   ..$ 8 : int [1:5] 30 7 91 95 78 #>   ..$ 9 : int [1:5] 41 93 54 86 29 #>   ..$ 10: int [1:5] 6 36 61 100 67 #>   ..$ 11: int [1:5] 70 63 50 74 52 #>   ..$ 12: int [1:5] 9 73 66 39 33 #>   ..$ 13: int [1:5] 32 77 19 10 3 #>   ..$ 14: int [1:5] 48 85 42 46 96 #>   ..$ 15: int [1:5] 38 68 72 79 16 #>   ..$ 16: int [1:5] 56 11 81 28 59 #>   ..$ 17: int [1:5] 51 34 37 82 20 #>   ..$ 18: int [1:5] 55 5 84 65 89 #>   ..$ 19: int [1:5] 98 53 49 1 8 #>   ..$ 20: int [1:5] 26 24 23 21 76"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"advanced-configuration","dir":"Articles","previous_headings":"","what":"üõ†Ô∏è Advanced Configuration","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"can plug custom training/prediction functions: can also provide advanced model_params like: neural networks:","code":"custom_fit <- function(X, Y, D) { ... } custom_predict <- function(model, X, D) { ... } list(num.trees = 200)  # for grf::causal_forest fnn_params <- list(   input_layer = list(units = 64, activation = 'relu', input_shape = c(ncol(X))),   layers = list(list(units = 32, activation = 'relu')),   output_layer = list(units = 1, activation = 'linear'),   compile_args = list(optimizer = 'adam', loss = 'mse'),   fit_params = list(epochs = 5, batch_size = 32, verbose = 0) )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_learning.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"‚ö° Parallelization","title":"Using cram_learning() for Sequential or Parallel Policy Training","text":"Set parallelize_batch = TRUE enable foreach-based parallel training (e.g.¬†across 4 cores):","code":"learning_result <- cram_learning(   X, D, Y, batch,   model_type = \"s_learner\",   learner_type = \"ridge\",   parallelize_batch = TRUE,   n_cores = 4 )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"what-is-cram_ml","dir":"Articles","previous_headings":"","what":"What is cram_ml()?","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"cram_ml() function provides flexible efficient way evaluate machine learning models using CRAM framework. allows : Reuse full dataset train evaluate final model Specify loss function (e.g., \"mse\", \"accuracy\", etc.) Use either caret-compatible learners fully custom model training prediction functions makes CRAM ML suitable regression, classification, supervised learning setup want reliable evaluation model built available data.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"example-linear-regression-with-mean-squared-error-mse","dir":"Articles","previous_headings":"","what":"Example: Linear Regression with Mean Squared Error (MSE)","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"","code":"# Simulate dataset set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data) # Define caret parameters for linear regression (no cross-validation) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  nb_batch <- 5"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"run-the-cram-ml-method","dir":"Articles","previous_headings":"Example: Linear Regression with Mean Squared Error (MSE)","what":"Run the CRAM ML method","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"","code":"result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = nb_batch,   loss_name = \"se\",   caret_params = caret_params_lm )"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"summary-table","dir":"Articles","previous_headings":"Results","what":"Summary Table","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"","code":"result$raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.86429 #> 2 Expected Loss Standard Error  0.73665 #> 3       Expected Loss CI Lower -0.57952 #> 4       Expected Loss CI Upper  2.30809"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"interactive-table","dir":"Articles","previous_headings":"Results","what":"Interactive Table","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"","code":"result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"access-the-final-model","dir":"Articles","previous_headings":"","what":"Access the Final Model","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"final model trained batches returned caret::train().","code":"str(result$final_ml_model) #> List of 24 #>  $ method      : chr \"lm\" #>  $ modelInfo   :List of 13 #>   ..$ label     : chr \"Linear Regression\" #>   ..$ library   : NULL #>   ..$ loop      : NULL #>   ..$ type      : chr \"Regression\" #>   ..$ parameters:'data.frame':   1 obs. of  3 variables: #>   .. ..$ parameter: chr \"intercept\" #>   .. ..$ class    : chr \"logical\" #>   .. ..$ label    : chr \"intercept\" #>   ..$ grid      :function (x, y, len = NULL, search = \"grid\")   #>   ..$ fit       :function (x, y, wts, param, lev, last, classProbs, ...)   #>   ..$ predict   :function (modelFit, newdata, submodels = NULL)   #>   ..$ prob      : NULL #>   ..$ predictors:function (x, ...)   #>   ..$ tags      : chr [1:2] \"Linear Regression\" \"Accepts Case Weights\" #>   ..$ varImp    :function (object, ...)   #>   ..$ sort      :function (x)   #>  $ modelType   : chr \"Regression\" #>  $ results     :'data.frame':    0 obs. of  4 variables: #>   ..$ RMSE     : num(0)  #>   ..$ Rsquared : num(0)  #>   ..$ MAE      : num(0)  #>   ..$ intercept: logi(0)  #>  $ pred        : NULL #>  $ bestTune    :'data.frame':    1 obs. of  1 variable: #>   ..$ intercept: logi TRUE #>  $ call        : language train.formula(form = Y ~ ., data = list(x1 = c(0.276550747291463, 2.28664539270111,  -0.727292059474465, -0.60892| __truncated__ ... #>  $ dots        : list() #>  $ metric      : chr \"RMSE\" #>  $ control     :List of 28 #>   ..$ method           : chr \"none\" #>   ..$ number           : num 25 #>   ..$ repeats          : logi NA #>   ..$ search           : chr \"grid\" #>   ..$ p                : num 0.75 #>   ..$ initialWindow    : NULL #>   ..$ horizon          : num 1 #>   ..$ fixedWindow      : logi TRUE #>   ..$ skip             : num 0 #>   ..$ verboseIter      : logi FALSE #>   ..$ returnData       : logi TRUE #>   ..$ returnResamp     : chr \"final\" #>   ..$ savePredictions  : chr \"none\" #>   ..$ classProbs       : logi FALSE #>   ..$ summaryFunction  :function (data, lev = NULL, model = NULL)   #>   ..$ selectionFunction: chr \"best\" #>   ..$ preProcOptions   :List of 6 #>   .. ..$ thresh   : num 0.95 #>   .. ..$ ICAcomp  : num 3 #>   .. ..$ k        : num 5 #>   .. ..$ freqCut  : num 19 #>   .. ..$ uniqueCut: num 10 #>   .. ..$ cutoff   : num 0.9 #>   ..$ sampling         : NULL #>   ..$ index            :List of 1 #>   .. ..$ Resample1: int [1:100] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ indexOut         :List of 1 #>   .. ..$ Resample1: int(0)  #>   ..$ indexFinal       : NULL #>   ..$ timingSamps      : num 0 #>   ..$ predictionBounds : logi [1:2] FALSE FALSE #>   ..$ seeds            : logi NA #>   ..$ adaptive         :List of 4 #>   .. ..$ min     : num 5 #>   .. ..$ alpha   : num 0.05 #>   .. ..$ method  : chr \"gls\" #>   .. ..$ complete: logi TRUE #>   ..$ trim             : logi FALSE #>   ..$ allowParallel    : logi TRUE #>   ..$ yLimits          : num [1:2] -1.89 2.63 #>  $ finalModel  :List of 17 #>   ..$ coefficients : Named num [1:4] 0.0315 0.05775 0.00883 -0.03161 #>   .. ..- attr(*, \"names\")= chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   ..$ residuals    : Named num [1:100] 1.118 -0.859 -0.15 0.132 -0.127 ... #>   .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   ..$ effects      : Named num [1:100] -0.3294 -0.6472 -0.0551 -0.3156 -0.1693 ... #>   .. ..- attr(*, \"names\")= chr [1:100] \"(Intercept)\" \"x1\" \"x2\" \"x3\" ... #>   ..$ rank         : int 4 #>   ..$ fitted.values: Named num [1:100] 0.06922 0.10013 -0.00358 0.02098 0.12662 ... #>   .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   ..$ assign       : int [1:4] 0 1 2 3 #>   ..$ qr           :List of 5 #>   .. ..$ qr   : num [1:100, 1:4] -10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. ..$ : chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. .. .. ..$ : chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"assign\")= int [1:4] 0 1 2 3 #>   .. ..$ qraux: num [1:4] 1.1 1.22 1.09 1.04 #>   .. ..$ pivot: int [1:4] 1 2 3 4 #>   .. ..$ tol  : num 1e-07 #>   .. ..$ rank : int 4 #>   .. ..- attr(*, \"class\")= chr \"qr\" #>   ..$ df.residual  : int 96 #>   ..$ xlevels      : Named list() #>   ..$ call         : language lm(formula = .outcome ~ ., data = dat) #>   ..$ terms        :Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. ..- attr(*, \"response\")= int 1 #>   .. .. ..- attr(*, \".Environment\")=<environment: 0x000001eb98eb98f0>  #>   .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   ..$ model        :'data.frame':    100 obs. of  4 variables: #>   .. ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   .. ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   .. ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   .. ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>   .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. .. ..- attr(*, \"response\")= int 1 #>   .. .. .. ..- attr(*, \".Environment\")=<environment: 0x000001eb98eb98f0>  #>   .. .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   ..$ xNames       : chr [1:3] \"x1\" \"x2\" \"x3\" #>   ..$ problemType  : chr \"Regression\" #>   ..$ tuneValue    :'data.frame':    1 obs. of  1 variable: #>   .. ..$ intercept: logi TRUE #>   ..$ obsLevels    : logi NA #>   ..$ param        : list() #>   ..- attr(*, \"class\")= chr \"lm\" #>  $ preProcess  : NULL #>  $ trainingData:'data.frame':    100 obs. of  4 variables: #>   ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>  $ ptype       :'data.frame':    0 obs. of  3 variables: #>   ..$ x1: num(0)  #>   ..$ x2: num(0)  #>   ..$ x3: num(0)  #>  $ resample    : NULL #>  $ resampledCM : NULL #>  $ perfNames   : chr [1:3] \"RMSE\" \"Rsquared\" \"MAE\" #>  $ maximize    : logi FALSE #>  $ yLimits     : num [1:2] -1.89 2.63 #>  $ times       :List of 3 #>   ..$ everything: 'proc_time' Named num [1:5] 0.14 0 0.14 NA NA #>   .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   ..$ final     : 'proc_time' Named num [1:5] 0 0 0 NA NA #>   .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   ..$ prediction: logi [1:3] NA NA NA #>  $ levels      : logi NA #>  $ terms       :Classes 'terms', 'formula'  language Y ~ x1 + x2 + x3 #>   .. ..- attr(*, \"variables\")= language list(Y, x1, x2, x3) #>   .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. ..$ : chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>   .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. ..- attr(*, \"intercept\")= int 1 #>   .. ..- attr(*, \"response\")= int 1 #>   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>   .. ..- attr(*, \"predvars\")= language list(Y, x1, x2, x3) #>   .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. ..- attr(*, \"names\")= chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>  $ coefnames   : chr [1:3] \"x1\" \"x2\" \"x3\" #>  $ xlevels     : Named list() #>  - attr(*, \"class\")= chr [1:2] \"train\" \"train.formula\""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"customization","dir":"Articles","previous_headings":"","what":"Customization","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"can customize CRAM ML pipeline : custom_fit: training function custom_predict: prediction function custom_loss: loss function loss_name: built-options like \"se\", \"logloss\", \"accuracy\", \"euclidean_distance\"","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"integer (random batching), vector predefined batch indices Supports supervised unsupervised workflows Confidence intervals computed using asymptotic theory influence functions","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_ml() for Simultaneous Machine Learning and Evaluation","text":"function builds : Batch-wise learning evaluation Cross-validation logic extended inference Influence-function-based variance estimation See also: caret::train() stats::kmeans() DT::datatable()","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"what-is-ml_learning","dir":"Articles","previous_headings":"","what":"üîç What is ml_learning()?","title":"Using ml_learning() for Batch-wise Machine Learning","text":"ml_learning() function implements batch-wise machine learning supervised unsupervised tasks. supports: caret::train() compatible model (regression, classification, clustering, etc.) Custom model training + prediction via custom_fit, custom_predict Custom built-loss functions (e.g., MSE, logloss, accuracy) Optional parallel processing (parallelize_batch = TRUE) ‚Äôs flexible utility powers cram_ml() general-purpose ML workflows, also useful customizing training pipelines.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"when-to-use-ml_learning","dir":"Articles","previous_headings":"","what":"üß† When to use ml_learning()?","title":"Using ml_learning() for Batch-wise Machine Learning","text":"want evaluate model‚Äôs performance multiple growing batches data want track loss evolution time need parallelized learning, e.g., neural nets larger datasets ‚Äôre experimenting custom ML pipelines, loss functions, unsupervised tasks","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"example-linear-regression-with-mse","dir":"Articles","previous_headings":"","what":"üìò Example: Linear Regression with MSE","title":"Using ml_learning() for Batch-wise Machine Learning","text":"","code":"# Simulate regression data set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data) # Define model settings caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\")  # No cross-validation )  # Define batch count nb_batch <- 5 # Run ML learning result_lm <- ml_learning(   data = data_df,   formula = Y ~ .,   batch = nb_batch,   loss_name = \"se\",   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"output-structure","dir":"Articles","previous_headings":"","what":"üì¶ Output Structure","title":"Using ml_learning() for Batch-wise Machine Learning","text":"returned object list :","code":"str(result_lm) #> List of 3 #>  $ final_ml_model:List of 24 #>   ..$ method      : chr \"lm\" #>   ..$ modelInfo   :List of 13 #>   .. ..$ label     : chr \"Linear Regression\" #>   .. ..$ library   : NULL #>   .. ..$ loop      : NULL #>   .. ..$ type      : chr \"Regression\" #>   .. ..$ parameters:'data.frame':    1 obs. of  3 variables: #>   .. .. ..$ parameter: chr \"intercept\" #>   .. .. ..$ class    : chr \"logical\" #>   .. .. ..$ label    : chr \"intercept\" #>   .. ..$ grid      :function (x, y, len = NULL, search = \"grid\")   #>   .. ..$ fit       :function (x, y, wts, param, lev, last, classProbs, ...)   #>   .. ..$ predict   :function (modelFit, newdata, submodels = NULL)   #>   .. ..$ prob      : NULL #>   .. ..$ predictors:function (x, ...)   #>   .. ..$ tags      : chr [1:2] \"Linear Regression\" \"Accepts Case Weights\" #>   .. ..$ varImp    :function (object, ...)   #>   .. ..$ sort      :function (x)   #>   ..$ modelType   : chr \"Regression\" #>   ..$ results     :'data.frame': 0 obs. of  4 variables: #>   .. ..$ RMSE     : num(0)  #>   .. ..$ Rsquared : num(0)  #>   .. ..$ MAE      : num(0)  #>   .. ..$ intercept: logi(0)  #>   ..$ pred        : NULL #>   ..$ bestTune    :'data.frame': 1 obs. of  1 variable: #>   .. ..$ intercept: logi TRUE #>   ..$ call        : language train.formula(form = Y ~ ., data = list(x1 = c(0.276550747291463, 2.28664539270111,  -0.727292059474465, -0.60892| __truncated__ ... #>   ..$ dots        : list() #>   ..$ metric      : chr \"RMSE\" #>   ..$ control     :List of 28 #>   .. ..$ method           : chr \"none\" #>   .. ..$ number           : num 25 #>   .. ..$ repeats          : logi NA #>   .. ..$ search           : chr \"grid\" #>   .. ..$ p                : num 0.75 #>   .. ..$ initialWindow    : NULL #>   .. ..$ horizon          : num 1 #>   .. ..$ fixedWindow      : logi TRUE #>   .. ..$ skip             : num 0 #>   .. ..$ verboseIter      : logi FALSE #>   .. ..$ returnData       : logi TRUE #>   .. ..$ returnResamp     : chr \"final\" #>   .. ..$ savePredictions  : chr \"none\" #>   .. ..$ classProbs       : logi FALSE #>   .. ..$ summaryFunction  :function (data, lev = NULL, model = NULL)   #>   .. ..$ selectionFunction: chr \"best\" #>   .. ..$ preProcOptions   :List of 6 #>   .. .. ..$ thresh   : num 0.95 #>   .. .. ..$ ICAcomp  : num 3 #>   .. .. ..$ k        : num 5 #>   .. .. ..$ freqCut  : num 19 #>   .. .. ..$ uniqueCut: num 10 #>   .. .. ..$ cutoff   : num 0.9 #>   .. ..$ sampling         : NULL #>   .. ..$ index            :List of 1 #>   .. .. ..$ Resample1: int [1:100] 1 2 3 4 5 6 7 8 9 10 ... #>   .. ..$ indexOut         :List of 1 #>   .. .. ..$ Resample1: int(0)  #>   .. ..$ indexFinal       : NULL #>   .. ..$ timingSamps      : num 0 #>   .. ..$ predictionBounds : logi [1:2] FALSE FALSE #>   .. ..$ seeds            : logi NA #>   .. ..$ adaptive         :List of 4 #>   .. .. ..$ min     : num 5 #>   .. .. ..$ alpha   : num 0.05 #>   .. .. ..$ method  : chr \"gls\" #>   .. .. ..$ complete: logi TRUE #>   .. ..$ trim             : logi FALSE #>   .. ..$ allowParallel    : logi TRUE #>   .. ..$ yLimits          : num [1:2] -1.89 2.63 #>   ..$ finalModel  :List of 17 #>   .. ..$ coefficients : Named num [1:4] 0.0315 0.05775 0.00883 -0.03161 #>   .. .. ..- attr(*, \"names\")= chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   .. ..$ residuals    : Named num [1:100] 1.118 -0.859 -0.15 0.132 -0.127 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. ..$ effects      : Named num [1:100] -0.3294 -0.6472 -0.0551 -0.3156 -0.1693 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"(Intercept)\" \"x1\" \"x2\" \"x3\" ... #>   .. ..$ rank         : int 4 #>   .. ..$ fitted.values: Named num [1:100] 0.06922 0.10013 -0.00358 0.02098 0.12662 ... #>   .. .. ..- attr(*, \"names\")= chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. ..$ assign       : int [1:4] 0 1 2 3 #>   .. ..$ qr           :List of 5 #>   .. .. ..$ qr   : num [1:100, 1:4] -10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:100] \"X56\" \"X12\" \"X65\" \"X34\" ... #>   .. .. .. .. ..$ : chr [1:4] \"(Intercept)\" \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"assign\")= int [1:4] 0 1 2 3 #>   .. .. ..$ qraux: num [1:4] 1.1 1.22 1.09 1.04 #>   .. .. ..$ pivot: int [1:4] 1 2 3 4 #>   .. .. ..$ tol  : num 1e-07 #>   .. .. ..$ rank : int 4 #>   .. .. ..- attr(*, \"class\")= chr \"qr\" #>   .. ..$ df.residual  : int 96 #>   .. ..$ xlevels      : Named list() #>   .. ..$ call         : language lm(formula = .outcome ~ ., data = dat) #>   .. ..$ terms        :Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. .. ..- attr(*, \"response\")= int 1 #>   .. .. .. ..- attr(*, \".Environment\")=<environment: 0x000001fc02e4b2d8>  #>   .. .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. ..$ model        :'data.frame': 100 obs. of  4 variables: #>   .. .. ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   .. .. ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   .. .. ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   .. .. ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>   .. .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language .outcome ~ x1 + x2 + x3 #>   .. .. .. .. ..- attr(*, \"variables\")= language list(.outcome, x1, x2, x3) #>   .. .. .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. .. .. ..$ : chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. .. .. ..- attr(*, \"response\")= int 1 #>   .. .. .. .. ..- attr(*, \".Environment\")=<environment: 0x000001fc02e4b2d8>  #>   .. .. .. .. ..- attr(*, \"predvars\")= language list(.outcome, x1, x2, x3) #>   .. .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. .. .. ..- attr(*, \"names\")= chr [1:4] \".outcome\" \"x1\" \"x2\" \"x3\" #>   .. ..$ xNames       : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. ..$ problemType  : chr \"Regression\" #>   .. ..$ tuneValue    :'data.frame': 1 obs. of  1 variable: #>   .. .. ..$ intercept: logi TRUE #>   .. ..$ obsLevels    : logi NA #>   .. ..$ param        : list() #>   .. ..- attr(*, \"class\")= chr \"lm\" #>   ..$ preProcess  : NULL #>   ..$ trainingData:'data.frame': 100 obs. of  4 variables: #>   .. ..$ .outcome: num [1:100] 1.187534 -0.758921 -0.153358 0.152764 -0.000241 ... #>   .. ..$ x1      : num [1:100] 0.277 2.287 -0.727 -0.609 1.215 ... #>   .. ..$ x2      : num [1:100] -1.238 0.108 0.59 1.123 -0.997 ... #>   .. ..$ x3      : num [1:100] -1.034 2.037 -0.054 -0.466 -1.068 ... #>   ..$ ptype       :'data.frame': 0 obs. of  3 variables: #>   .. ..$ x1: num(0)  #>   .. ..$ x2: num(0)  #>   .. ..$ x3: num(0)  #>   ..$ resample    : NULL #>   ..$ resampledCM : NULL #>   ..$ perfNames   : chr [1:3] \"RMSE\" \"Rsquared\" \"MAE\" #>   ..$ maximize    : logi FALSE #>   ..$ yLimits     : num [1:2] -1.89 2.63 #>   ..$ times       :List of 3 #>   .. ..$ everything: 'proc_time' Named num [1:5] 0.13 0 0.12 NA NA #>   .. .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   .. ..$ final     : 'proc_time' Named num [1:5] 0 0 0 NA NA #>   .. .. ..- attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ... #>   .. ..$ prediction: logi [1:3] NA NA NA #>   ..$ levels      : logi NA #>   ..$ terms       :Classes 'terms', 'formula'  language Y ~ x1 + x2 + x3 #>   .. .. ..- attr(*, \"variables\")= language list(Y, x1, x2, x3) #>   .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>   .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"x1\" \"x2\" \"x3\" #>   .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1 #>   .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. ..- attr(*, \"response\")= int 1 #>   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>   .. .. ..- attr(*, \"predvars\")= language list(Y, x1, x2, x3) #>   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"numeric\" #>   .. .. .. ..- attr(*, \"names\")= chr [1:4] \"Y\" \"x1\" \"x2\" \"x3\" #>   ..$ coefnames   : chr [1:3] \"x1\" \"x2\" \"x3\" #>   ..$ xlevels     : Named list() #>   ..- attr(*, \"class\")= chr [1:2] \"train\" \"train.formula\" #>  $ losses        : num [1:100, 1:6] 0 0 0 0 0 0 0 0 0 0 ... #>  $ batch_indices :List of 5 #>   ..$ 1: int [1:20] 56 12 65 34 24 4 9 97 7 1 ... #>   ..$ 2: int [1:20] 98 3 8 64 96 17 63 44 49 28 ... #>   ..$ 3: int [1:20] 62 18 30 21 29 87 99 74 75 90 ... #>   ..$ 4: int [1:20] 69 50 20 52 85 86 19 77 45 36 ... #>   ..$ 5: int [1:20] 42 47 51 40 22 39 83 82 72 68 ..."},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"custom-models-optional","dir":"Articles","previous_headings":"","what":"üîß Custom Models (Optional)","title":"Using ml_learning() for Batch-wise Machine Learning","text":"can plug models: run:","code":"custom_fit <- function(data) {   model <- glm(Y ~ ., data = data)   return(model) }  custom_predict <- function(model, data) {   return(predict(model, newdata = data)) }  custom_loss <- function(preds, data) {   return((data$Y - preds)^2) } ml_learning(data_df, batch = nb_batch, custom_fit = custom_fit,             custom_predict = custom_predict, custom_loss = custom_loss)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"‚ö° Parallelization","title":"Using ml_learning() for Batch-wise Machine Learning","text":"accelerate training, set:","code":"ml_learning(data_df,             formula = Y ~ .,             batch = nb_batch,             caret_params = caret_params_lm,             loss_name = \"mse\",             parallelize_batch = TRUE,             n_cores = 4)"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_ml_learning.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Using ml_learning() for Batch-wise Machine Learning","text":"cram_ml() ‚Äî wrapper full pipeline (estimation + CI) cram_policy() ‚Äî causal policy learning caret::train() ‚Äî underlying training API","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"introduction-what-is-the-cram-method","dir":"Articles","previous_headings":"","what":"Introduction: What is the CRAM Method?","title":"Using cram_policy() for Policy Learning and Evaluation","text":"CRAM method powerful approach simultaneously learning evaluating decision rules, individualized treatment rules (ITRs), data. Unlike traditional approaches like sample splitting cross-validation, waste part data evaluation , CRAM reuses available data efficiently. key distinction cross-validation CRAM evaluates final learned model, rather averaging performance across multiple models trained different data splits. CRAM: Sequentially trains evaluates models batches data Tracks model evolution minimal assumptions Estimates treatment effects policy values using entire dataset, increasing efficiency statistical power Think CRAM like cram school: learn bit, test bit, repeat ‚Äî getting better constantly self-evaluating.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"the-cram-workflow","dir":"Articles","previous_headings":"","what":"The CRAM Workflow","title":"Using cram_policy() for Policy Learning and Evaluation","text":"core idea CRAM method visualized: procedure ensures update backed performance testing, enabling learning evaluation one pass data.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"cram-estimation-in-practice","dir":"Articles","previous_headings":"","what":"CRAM Estimation in Practice","title":"Using cram_policy() for Policy Learning and Evaluation","text":"schematic represents CRAM averages many mini train-test steps, ultimately estimating improvement final learned rule baseline. Mathematically, estimated policy value difference : ŒîÃÇ(œÄT;œÄ0)=‚àët=1T‚àí1ŒîÃÇ(œÄt,œÄt‚àí1) \\hat{\\Delta}(\\pi_T; \\pi_0) = \\sum_{t=1}^{T-1} \\hat{\\Delta}(\\pi_t, \\pi_{t-1}) : œÄt\\pi_t policy learned batch t Evaluation done using unseen remaining data","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"the-cram_policy-function","dir":"Articles","previous_headings":"","what":"The cram_policy() Function","title":"Using cram_policy() for Policy Learning and Evaluation","text":"cram_policy() function cramR implements CRAM framework binary treatment policy learning.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"key-features-of-cram_policy","dir":"Articles","previous_headings":"The cram_policy() Function","what":"üîë Key Features of cram_policy()","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Model-Agnostic Flexibility: Supports variety learning strategies, including causal_forest, s_learner, m_learner, well fully customizable learners via user-defined fit predict functions. Efficient Design: Built top data.table fast, memory-efficient computation, optional support parallel batch training scale across larger datasets.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"example-running-cram-policy-on-simulated-data","dir":"Articles","previous_headings":"","what":"Example: Running CRAM Policy on simulated data","title":"Using cram_policy() for Policy Learning and Evaluation","text":"","code":"library(data.table) # Function to generate sample data with heterogeneous treatment effects: # - Positive effect group # - Neutral effect group # - Adverse effect group generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),                 # Binary variable     discrete = sample(1:5, n, replace = TRUE),  # Discrete variable     continuous = rnorm(n)                       # Continuous variable   )    # Binary treatment assignment (50% treated)   D <- rbinom(n, 1, 0.5)    # Define heterogeneous treatment effects based on X   treatment_effect <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect            -1,            0.1)                                   # Group 2: Neutral effect   )    # Outcome depends on treatment effect + noise   Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +     (1 - D) * rnorm(n)    return(list(X = X, D = D, Y = Y)) }  # Generate a sample dataset set.seed(123) n <- 1000 data <- generate_data(n) X <- data$X D <- data$D Y <- data$Y # Number of batches to split the data into batch <- 20    # Model type for estimating treatment effects # Options: \"causal_forest\", \"s_learner\", \"m_learner\" model_type <- \"causal_forest\"    # Learner type used inside s/m-learners # NULL is required for causal_forest; use \"ridge\" or \"fnn\" for s/m learners learner_type <- NULL    # Baseline policy to compare against (list of 0/1 for each individual) # Common options: # - All-control baseline: as.list(rep(0, nrow(X))) # - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE)) baseline_policy <- as.list(rep(0, nrow(X)))    # Whether to parallelize across batches (TRUE for faster but memory-heavy runs) parallelize_batch <- FALSE    # Model-specific parameters # Examples: # - causal_forest: list(num.trees = 100) # - ridge: list(alpha = 1) # - fnn (Feedforward Neural Network): see below model_params <- NULL    # Advanced example for fnn (not used here): # default_model_params <- list( #   input_layer = list(units = 64, activation = 'relu', input_shape = input_shape), #   layers = list(list(units = 32, activation = 'relu')), #   output_layer = list(units = 1, activation = 'linear'), #   compile_args = list(optimizer = 'adam', loss = 'mse'), #   fit_params = list(epochs = 5, batch_size = 32, verbose = 0) # )  # Significance level for confidence intervals (default = 95%) alpha <- 0.05    # Run the CRAM policy method result <- cram_policy(   X, D, Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params,   alpha = alpha )  # Display the results print(result) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"interpreting-results","dir":"Articles","previous_headings":"","what":"Interpreting Results","title":"Using cram_policy() for Policy Learning and Evaluation","text":"output cram_policy() includes: raw_results: data frame summarizing key metrics: Delta Estimate, Delta Standard Error, corresponding confidence interval bounds Policy Value Estimate, Policy Value Standard Error, confidence interval bounds Proportion Treated final policy interactive_table: interactive table view results exploration final_policy_model: final fitted policy model based specified model_type, learner_type, custom_fit","code":"result$raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"accessing-the-final-learned-policy","dir":"Articles","previous_headings":"","what":"Accessing the Final Learned Policy","title":"Using cram_policy() for Policy Learning and Evaluation","text":"can inspect apply learned model new data.","code":"class(result$final_policy_model) #> [1] \"causal_forest\" \"grf\" summary(result$final_policy_model) #>                          Length Class  Mode    #> _ci_group_size              1   -none- numeric #> _num_variables              1   -none- numeric #> _num_trees                  1   -none- numeric #> _root_nodes               100   -none- list    #> _child_nodes              100   -none- list    #> _leaf_samples             100   -none- list    #> _split_vars               100   -none- list    #> _split_values             100   -none- list    #> _drawn_samples            100   -none- list    #> _send_missing_left        100   -none- list    #> _pv_values                100   -none- list    #> _pv_num_types               1   -none- numeric #> predictions              1000   -none- numeric #> variance.estimates          0   -none- numeric #> debiased.error           1000   -none- numeric #> excess.error             1000   -none- numeric #> seed                        1   -none- numeric #> num.threads                 1   -none- numeric #> ci.group.size               1   -none- numeric #> X.orig                   3000   -none- numeric #> Y.orig                   1000   -none- numeric #> W.orig                   1000   -none- numeric #> Y.hat                    1000   -none- numeric #> W.hat                    1000   -none- numeric #> clusters                    0   -none- numeric #> equalize.cluster.weights    1   -none- logical #> tunable.params              7   -none- list    #> has.missing.values          1   -none- logical"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"visual-summary","dir":"Articles","previous_headings":"","what":"Visual Summary","title":"Using cram_policy() for Policy Learning and Evaluation","text":"visualization summarizes multiple evaluations across iterations contribute full CRAM estimate.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"notes-and-tips","dir":"Articles","previous_headings":"","what":"Notes and Tips","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Batching: can pass number (e.g., batch = 5) custom vector control data split. Parallelization: Enable parallelize_batch = TRUE. Custom Learners: Use custom_fit custom_predict plug estimator.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"why-cram","dir":"Articles","previous_headings":"","what":"Why CRAM?","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Compared classic evaluation methods: practice, CRAM: Reduces variance policy evaluation Produces tighter confidence intervals Accommodates online learners large-scale models","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Using cram_policy() for Policy Learning and Evaluation","text":"Jia, Z., Imai, K., & Li, M. L. (2024). CRAM Method Efficient Simultaneous Learning Evaluation. arXiv:2403.07031. Wager & Athey (2018). Causal Forests. Athey & Imbens (2016). S/M-learner framework.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"what-is-cram_simulation","dir":"Articles","previous_headings":"","what":"üéØ What is cram_simulation()?","title":"Using cram_simulation() for CRAM with Known DGP","text":"cram_simulation() function performs simultaneous policy learning evaluation known data-generating process (DGP). useful : Benchmarking CRAM simulated datasets Measuring empirical bias, variance, confidence interval coverage Supporting synthetic (dgp_X) empirical (X) covariate generation","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"inputs-overview","dir":"Articles","previous_headings":"","what":"üì¶ Inputs Overview","title":"Using cram_simulation() for CRAM with Known DGP","text":"must supply either: - X: dataset bootstrap (empirical DGP)- dgp_X: function simulates covariates must also define: - dgp_D(X): treatment assignment function - dgp_Y(D, X): outcome generation function","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"example-simulated-data-with-binary-discrete-and-continuous-covariates","dir":"Articles","previous_headings":"","what":"üìò Example: Simulated Data with Binary, Discrete, and Continuous Covariates","title":"Using cram_simulation() for CRAM with Known DGP","text":"","code":"# Define data generation process (DGP) functions X_data <- data.table::data.table(   binary = rbinom(100, 1, 0.5),                 # Binary variable (0 or 1)   discrete = sample(1:5, 100, replace = TRUE),  # Discrete variable (1 to 5)   continuous = rnorm(100)                       # Continuous variable )  dgp_D <- function(X) rbinom(nrow(X), 1, 0.5)  dgp_Y <- function(D, X) {   theta <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,  # Group 1: High benefit     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4,  # Group 3: High adverse effect            -1,            0.1)  # Group 2: Neutral effect   )   Y <- D * (theta + rnorm(length(D), mean = 0, sd = 1)) +     (1 - D) * rnorm(length(D))  # Outcome for untreated   return(Y) }  # Parameters nb_simulations <- 10 nb_simulations_truth <- 2 batch <- 5  # Perform CRAM simulation result <- cram_simulation(   X = X_data,   dgp_D = dgp_D,   dgp_Y = dgp_Y,   batch = batch,   nb_simulations = nb_simulations,   nb_simulations_truth = nb_simulations_truth,   sample_size = 50 )  # Access results result$avg_delta_estimate #> NULL result$delta_empirical_bias #> NULL"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"output-summary","dir":"Articles","previous_headings":"","what":"üìä Output Summary","title":"Using cram_simulation() for CRAM with Known DGP","text":"Returns list containing: raw_results: summary key averaged metrics interactive_table: interactive HTML widget quick exploration","code":"result$raw_results #>                                  Metric    Value #> 1            Average Proportion Treated  0.60000 #> 2                Average Delta Estimate -0.00705 #> 3          Average Delta Standard Error  0.23362 #> 4                  Delta Empirical Bias  0.12369 #> 5              Delta Empirical Coverage  1.00000 #> 6         Variance Delta Empirical Bias  0.42625 #> 7         Average Policy Value Estimate -0.00444 #> 8   Average Policy Value Standard Error  0.24662 #> 9           Policy Value Empirical Bias  0.06418 #> 10      Policy Value Empirical Coverage  0.90000 #> 11 Variance Policy Value Empirical Bias  0.33513 result$interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"notes","dir":"Articles","previous_headings":"","what":"üí° Notes","title":"Using cram_simulation() for CRAM with Known DGP","text":"Uses batch splitting honest policy learning Variance estimates use influence-function-based asymptotics Simulations grouped sim_id averaged can plug custom_fit custom_predict needed","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/cram_policy_simulation.html","id":"see-also","dir":"Articles","previous_headings":"","what":"üìö See Also","title":"Using cram_simulation() for CRAM with Known DGP","text":"cram_policy() ‚Äî CRAM real experimental/observational data cram_bandit_sim() ‚Äî contextual bandits cram_ml() ‚Äî general supervised unsupervised ML","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Quick Start with CRAM","text":"CRAM package provides unified framework : üß† Cram Policy (cram_policy): Learn evaluate individualized binary treatment rules using CRAM. Offers flexible model choices, including causal forests custom learners. üìà Cram ML (cram_ml): Learn evaluate ML models using CRAM. Supports flexible model training (via caret user-defined functions) custom loss functions. üé∞ Cram Bandit (cram_bandit): Learn perform -policy evaluation contextual bandit algorithms using CRAM. Supports real data simulation environments built-policies. vignette walks three core modules.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"cram-policy","dir":"Articles","previous_headings":"","what":"1. CRAM Policy","title":"Quick Start with CRAM","text":"begin simulating dataset consisting covariates X, binary treatment assignment D, continuous outcome Y, use demonstrate cram_policy() function.","code":"library(data.table) # Function to generate sample data with heterogeneous treatment effects: # - Positive effect group # - Neutral effect group # - Adverse effect group generate_data <- function(n) {   X <- data.table(     binary = rbinom(n, 1, 0.5),                 # Binary variable     discrete = sample(1:5, n, replace = TRUE),  # Discrete variable     continuous = rnorm(n)                       # Continuous variable   )    # Binary treatment assignment (50% treated)   D <- rbinom(n, 1, 0.5)    # Define heterogeneous treatment effects based on X   treatment_effect <- ifelse(     X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect     1,     ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect            -1,            0.1)                                   # Group 2: Neutral effect   )    # Outcome depends on treatment effect + noise   Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +     (1 - D) * rnorm(n)    return(list(X = X, D = D, Y = Y)) }  # Generate a sample dataset set.seed(123) n <- 1000 data <- generate_data(n) X <- data$X D <- data$D Y <- data$Y"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"built-in-model","dir":"Articles","previous_headings":"1. CRAM Policy","what":"Built-in Model","title":"Quick Start with CRAM","text":"example, demonstrate use built-modeling options provided cramR package. walk key parameters control cram_policy() behaves, including choice model, learner type, baseline policy, batching strategy. parameters allow flexibility configuring learning process depending use case computational resources. model_params argument allows customize hyperparameters model used policy learning. left NULL, cram_policy() fall back sensible defaults depending model learner type: model_type = \"causal_forest\": method defaults grf::causal_forest() num.trees = 100. model_type = \"s_learner\" \"m_learner\" learner_type = \"ridge\": defaults glmnet::cv.glmnet() alpha = 1, corresponding Ridge regression. learner_type = \"fnn\" (Feedforward Neural Network): default Keras model built following architecture: can override defaults passing custom list model_params including parameter name defined underlying package, namely grf::causal_forest(), glmnet::cv.glmnet() keras.","code":"# Number of batches to split the data into batch <- 20    # Model type for estimating treatment effects # Options: \"causal_forest\", \"s_learner\", \"m_learner\", NULL (to use custom model) model_type <- \"causal_forest\"    # Learner type used inside s/m-learners # NULL is required for causal_forest; use \"ridge\" or \"fnn\" for s/m learners learner_type <- NULL    # Baseline policy to compare against (list of 0/1 for each individual) # Common options: # - All-control baseline: as.list(rep(0, nrow(X))) or NULL # - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE)) baseline_policy <- as.list(rep(0, nrow(X)))    # Whether to parallelize across batches (TRUE for faster but memory-heavy runs) parallelize_batch <- FALSE    # Model params  model_params <- NULL list(   input_layer = list(units = 64, activation = 'relu', input_shape = input_shape),   layers = list(     list(units = 32, activation = 'relu')   ),   output_layer = list(units = 1, activation = 'linear'),   compile_args = list(optimizer = 'adam', loss = 'mse'),   fit_params = list(epochs = 5, batch_size = 32, verbose = 0) ) # Significance level for confidence intervals (default = 95%) alpha <- 0.05    # Run the CRAM policy method result <- cram_policy(   X, D, Y,   batch = batch,   model_type = model_type,   learner_type = learner_type,   baseline_policy = baseline_policy,   parallelize_batch = parallelize_batch,   model_params = model_params,   alpha = alpha )  # Display the results print(result) #> $raw_results #>                        Metric   Value #> 1              Delta Estimate 0.23208 #> 2        Delta Standard Error 0.05862 #> 3              Delta CI Lower 0.11718 #> 4              Delta CI Upper 0.34697 #> 5       Policy Value Estimate 0.21751 #> 6 Policy Value Standard Error 0.05237 #> 7       Policy Value CI Lower 0.11486 #> 8       Policy Value CI Upper 0.32016 #> 9          Proportion Treated 0.60500 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.437 0.350 0.213"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom-model","dir":"Articles","previous_headings":"1. CRAM Policy","what":"Custom Model","title":"Quick Start with CRAM","text":"use model policy learning, can supply two user-defined functions:","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom_fitx-y-d----","dir":"Articles","previous_headings":"1. CRAM Policy > Custom Model","what":"1. custom_fit(X, Y, D, ...)","title":"Quick Start with CRAM","text":"function takes training data: covariates X, outcomes Y, binary treatment indicators D, returns fitted model object. may also define use additional parameters (e.g., number folds, regularization settings, etc.) within function body. X: matrix data frame features Y: numeric outcome vector D: binary vector indicating treatment assignment (0 1) Example: Custom X-learner Ridge regression 5-fold cross-validation","code":"custom_fit <- function(X, Y, D, n_folds = 5) {   treated_indices <- which(D == 1)   control_indices <- which(D == 0)   X_treated <- X[treated_indices, ]   Y_treated <- Y[treated_indices]   X_control <- X[control_indices, ]   Y_control <- Y[control_indices]   model_treated <- cv.glmnet(as.matrix(X_treated), Y_treated, alpha = 0, nfolds = n_folds)   model_control <- cv.glmnet(as.matrix(X_control), Y_control, alpha = 0, nfolds = n_folds)   tau_control <- Y_treated - predict(model_control, as.matrix(X_treated), s = \"lambda.min\")   tau_treated <- predict(model_treated, as.matrix(X_control), s = \"lambda.min\") - Y_control   X_combined <- rbind(X_treated, X_control)   tau_combined <- c(tau_control, tau_treated)   weights <- c(rep(1, length(tau_control)), rep(1, length(tau_treated)))   final_model <- cv.glmnet(as.matrix(X_combined), tau_combined, alpha = 0, weights = weights, nfolds = n_folds)   return(final_model) }"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom_predictmodel-x_new-d_new","dir":"Articles","previous_headings":"1. CRAM Policy > Custom Model","what":"2. custom_predict(model, X_new, D_new)","title":"Quick Start with CRAM","text":"function uses fitted model generate binary treatment decision individual X_new. return vector 0s 1s, indicating whether assign treatment (1) (0). may also incorporate custom threshold post-processing logic within function. Example: Apply decision rule ‚Äî treat estimated CATE greater 0","code":"custom_predict <- function(model, X_new, D_new) {   cate <- predict(model, as.matrix(X_new), s = \"lambda.min\")    # Apply decision rule: treat if CATE > 0   as.integer(cate > 0) }"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"use-cram_policy-with-custom_fit-and-custom_predict","dir":"Articles","previous_headings":"1. CRAM Policy > Custom Model","what":"3. Use cram_policy() with custom_fit() and custom_predict()","title":"Quick Start with CRAM","text":"custom_fit() custom_predict() defined, can integrate CRAM framework passing cram_policy() shown :","code":"experiment_results <- cram_policy(   X, D, Y,   batch = 20,   custom_fit = custom_fit,   custom_predict = custom_predict,   alpha = 0.05 ) print(experiment_results) #> $raw_results #>                        Metric    Value #> 1              Delta Estimate  0.13367 #> 2        Delta Standard Error  0.10053 #> 3              Delta CI Lower -0.06338 #> 4              Delta CI Upper  0.33071 #> 5       Policy Value Estimate  0.11910 #> 6 Policy Value Standard Error  0.10105 #> 7       Policy Value CI Lower -0.07897 #> 8       Policy Value CI Upper  0.31716 #> 9          Proportion Treated  0.57600 #>  #> $interactive_table #>  #> $final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 1000  #> Variable importance:  #>     1     2     3  #> 0.446 0.327 0.227"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"built-in-model-1","dir":"Articles","previous_headings":"2. CRAM ML","what":"Built-in Model","title":"Quick Start with CRAM","text":"section illustrates use cram_ml() built-modeling options available cramR package. function integrates caret framework, allowing users specify learning algorithm, loss function, batching strategy evaluate model performance. Beyond caret, cram_ml() also supports fully custom model training, prediction, loss functions, making suitable virtually machine learning task ‚Äî including regression, classification, clustering. illustrate use cram_ml(), begin generating synthetic dataset regression task. data consists three independent covariates continuous outcome. cram_ml() function offers extensive flexibility loss_name caret_params arguments. loss_name argument specifies performance metric used evaluate model batch. Available options include: \"se\" ‚Äì Squared Error (regression) \"ae\" ‚Äì Absolute Error \"logloss\" ‚Äì Logarithmic Loss (probabilistic classification) \"accuracy\" ‚Äì Classification Accuracy \"euclidean_distance\" ‚Äì Squared Euclidean Distance (clustering tasks) caret_params list defines model trained using caret package. can include argument supported caret::train(), allowing full control model specification tuning. Common components include: method: machine learning algorithm (e.g., \"lm\" linear regression, \"rf\" random forest, \"xgbTree\" XGBoost, \"svmLinear\" support vector machines) trControl: resampling strategy (e.g., trainControl(method = \"cv\", number = 5) 5-fold cross-validation, \"none\" training without resampling) tuneGrid: grid hyperparameters tuning (e.g., expand.grid(mtry = c(2, 3, 4))) metric: model selection metric used tuning (e.g., \"RMSE\" \"Accuracy\") preProcess: optional preprocessing steps (e.g., centering, scaling) importance: logical flag compute variable importance (useful tree-based models) Refer full documentation caret model training tuning complete list supported arguments options. Together, arguments allow users apply cram_ml() using wide variety built-machine learning models losses. users need go beyond built-choices, also provide next section friendly workflow specify custom models losses cram_ml().","code":"set.seed(42) X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100) data_df <- data.frame(X_data, Y = Y_data) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   loss_name = \"se\",   caret_params = caret_params_lm ) print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.86429 #> 2 Expected Loss Standard Error  0.73665 #> 3       Expected Loss CI Lower -0.57952 #> 4       Expected Loss CI Upper  2.30809 #>  #> $interactive_table #>  #> $final_ml_model #> Linear Regression  #>  #> 100 samples #>   3 predictor #>  #> No pre-processing #> Resampling: None"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom-model-1","dir":"Articles","previous_headings":"2. CRAM ML","what":"Custom Model","title":"Quick Start with CRAM","text":"addition using built-learners via caret, cram_ml() also supports fully custom model workflows. can specify : Model fitting function (custom_fit) Prediction function (custom_predict) Loss function (custom_loss) offers maximum flexibility, allowing CRAM evaluate learning model performance criterion, including regression, classification, even unsupervised losses clustering distance.","code":""},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom_fitdata----","dir":"Articles","previous_headings":"2. CRAM ML > Custom Model","what":"1. custom_fit(data, ...)","title":"Quick Start with CRAM","text":"function takes data frame returns fitted model. may define additional arguments hyperparameters training settings. data: data frame includes predictors outcome variable Y. Example: basic linear model fit three predictors:","code":"custom_fit <- function(data) {   lm(Y ~ x1 + x2 + x3, data = data) }"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom_predictmodel-data","dir":"Articles","previous_headings":"2. CRAM ML > Custom Model","what":"2. custom_predict(model, data)","title":"Quick Start with CRAM","text":"function generates predictions fitted model new data. returns numeric vector predicted outcomes. model: fitted model returned custom_fit() data: data frame new observations (typically including original predictors) Example: Extract predictors apply standard predict() call:","code":"custom_predict <- function(model, data) {   predictors_only <- data[, setdiff(names(data), \"Y\"), drop = FALSE]   predict(model, newdata = predictors_only) }"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"custom_losspredictions-data","dir":"Articles","previous_headings":"2. CRAM ML > Custom Model","what":"3. custom_loss(predictions, data)","title":"Quick Start with CRAM","text":"function defines loss metric used evaluate model predictions. return numeric vector individual losses, one per observation. internally aggregated cram_ml() compute overall performance. predictions: numeric vector predicted values model data: data frame containing true outcome values (Y) Example: Define custom loss function using Squared Error (SE)","code":"custom_loss <- function(predictions, data) {   actuals <- data$Y   se_loss <- (predictions - actuals)^2   return(se_loss) }"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"use-cram_ml-with-custom-functions","dir":"Articles","previous_headings":"2. CRAM ML > Custom Model","what":"4. Use cram_ml() with Custom Functions","title":"Quick Start with CRAM","text":"defined custom training, prediction, loss functions, can pass directly cram_ml() shown , note caret_params loss_name used built-functionalities now NULL:","code":"result <- cram_ml(   data = data_df,   formula = Y ~ .,   batch = 5,   custom_fit = custom_fit,   custom_predict = custom_predict,   custom_loss = custom_loss ) print(result) #> $raw_results #>                         Metric    Value #> 1       Expected Loss Estimate  0.59946 #> 2 Expected Loss Standard Error  0.39867 #> 3       Expected Loss CI Lower -0.18192 #> 4       Expected Loss CI Upper  1.38085 #>  #> $interactive_table #>  #> $final_ml_model #>  #> Call: #> lm(formula = Y ~ x1 + x2 + x3, data = data) #>  #> Coefficients: #> (Intercept)           x1           x2           x3   #>    0.031503     0.057754     0.008829    -0.031611"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"cram-bandit-evaluation","dir":"Articles","previous_headings":"","what":"3. CRAM Bandit Evaluation","title":"Quick Start with CRAM","text":"","code":"set.seed(42) T <- 100 K <- 4 pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K)) for (t in 1:T) {   for (j in 1:T) {     pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])   } } arm <- sample(1:K, T, replace = TRUE) reward <- rnorm(T, mean = 1, sd = 0.5) cram_results <- cram_bandit(pi, arm, reward) print(cram_results) #> $raw_results #>                        Metric   Value #> 1       Policy Value Estimate 0.67621 #> 2 Policy Value Standard Error 0.04394 #> 3       Policy Value CI Lower 0.59008 #> 4       Policy Value CI Upper 0.76234 #>  #> $interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"bandit-simulation","dir":"Articles","previous_headings":"3. CRAM Bandit Evaluation","what":"Bandit Simulation","title":"Quick Start with CRAM","text":"","code":"horizon     <- 500L simulations <- 100L k <- 4 d <- 3 list_betas <- cramR:::get_betas(simulations, d, k) bandit     <- cramR:::ContextualLinearBandit$new(k = k, d = d, list_betas = list_betas, sigma = 0.3) policy     <- cramR:::BatchContextualEpsilonGreedyPolicy$new(epsilon = 0.1, batch_size = 5)  sim <- cram_bandit_sim(   horizon, simulations,   bandit, policy,   alpha = 0.05, do_parallel = FALSE ) #> Simulation horizon: 500 #> Number of simulations: 101 #> Number of batches: 1 #> Starting main loop. #> Finished main loop. #> Completed simulation in 0:00:04.805 #> Computing statistics. print(sim) #> $estimates #>        sim  estimate variance_est  estimand prediction_error est_rel_error variance_prediction_error  std_error    ci_lower  ci_upper #>      <int>     <num>        <num>     <num>            <num>         <num>                     <num>      <num>       <num>     <num> #>   1:     1 0.5934946 0.0074853416 0.5008213     0.0926732980  0.1850426518               -0.15435762 0.08651787  0.42392267 0.7630665 #>   2:     2 0.5738572 0.0044886575 0.5472302     0.0266269364  0.0486576503               -0.49290236 0.06699744  0.44254457 0.7051697 #>   3:     3 0.3082747 0.0059714142 0.2685309     0.0397438543  0.1480047945               -0.32539072 0.07727493  0.15681864 0.4597308 #>   4:     4 0.4875583 0.0017858937 0.5025160    -0.0149577243 -0.0297656692               -0.79824202 0.04225984  0.40473050 0.5703860 #>   5:     5 0.6279068 0.0028121753 0.6234795     0.0044272785  0.0071009204               -0.68229979 0.05302995  0.52397002 0.7318436 #>   6:     6 0.6756592 0.0017886154 0.6278958     0.0477634808  0.0760691251               -0.79793454 0.04229202  0.59276839 0.7585501 #>   7:     7 0.5730742 0.0017161244 0.5853930    -0.0123188150 -0.0210436666               -0.80612408 0.04142613  0.49188046 0.6542679 #>   8:     8 0.5227079 0.0012413336 0.4893036     0.0334042998  0.0682690620               -0.85976267 0.03523256  0.45365337 0.5917625 #>   9:     9 0.5727205 0.0017862708 0.5531983     0.0195222438  0.0352897780               -0.79819942 0.04226430  0.48988400 0.6555570 #>  10:    10 0.5008949 0.0019120477 0.4505540     0.0503408845  0.1117310742               -0.78399001 0.04372697  0.41519162 0.5865982 #>  11:    11 0.5896355 0.0015834232 0.5646470     0.0249885711  0.0442552113               -0.82111574 0.03979225  0.51164416 0.6676269 #>  12:    12 0.6661715 0.0085479224 0.6730501    -0.0068785859 -0.0102200203               -0.03431456 0.09245497  0.48496311 0.8473800 #>  13:    13 0.4155410 0.0021417559 0.4353688    -0.0198278063 -0.0455425472               -0.75803916 0.04627911  0.32483566 0.5062464 #>  14:    14 0.5874491 0.0019488865 0.5881986    -0.0007495008 -0.0012742308               -0.77982821 0.04414620  0.50092415 0.6739741 #>  15:    15 0.5624189 0.0034441171 0.5775935    -0.0151745609 -0.0262720420               -0.61090735 0.05868660  0.44739532 0.6774426 #>  16:    16 0.3854339 0.0046490697 0.3881738    -0.0027398785 -0.0070583804               -0.47478010 0.06818409  0.25179558 0.5190723 #>  17:    17 0.5114540 0.0015462162 0.4738467     0.0376073533  0.0793660792               -0.82531914 0.03932196  0.43438442 0.5885237 #>  18:    18 0.5902585 0.0015424719 0.6093514    -0.0190929303 -0.0313332019               -0.82574215 0.03927432  0.51328222 0.6672347 #>  19:    19 0.7059122 0.0011704551 0.6840281     0.0218841422  0.0319930471               -0.86777004 0.03421191  0.63885809 0.7729663 #>  20:    20 0.6300472 0.0016527784 0.6004103     0.0296369256  0.0493611241               -0.81328047 0.04065438  0.55036608 0.7097283 #>  21:    21 0.8787457 0.0604291267 0.5725605     0.3061852009  0.5347648014                5.82686688 0.24582336  0.39694076 1.3605506 #>  22:    22 0.4966222 0.0015875680 0.4546451     0.0419771798  0.0923295626               -0.82064749 0.03984430  0.41852886 0.5747156 #>  23:    23 0.5641831 0.0134459969 0.6353099    -0.0711268351 -0.1119561254                0.51903620 0.11595688  0.33691177 0.7914544 #>  24:    24 0.7453828 0.0018363610 0.6813180     0.0640648238  0.0940307243               -0.79254057 0.04285278  0.66139290 0.8293727 #>  25:    25 0.5691530 0.0025291593 0.4830130     0.0861399897  0.1783388641               -0.71427299 0.05029075  0.47058494 0.6677210 #>  26:    26 0.3149706 0.0095690247 0.3623225    -0.0473518939 -0.1306899103                0.08104256 0.09782139  0.12324419 0.5066970 #>  27:    27 0.7145588 0.0021662946 0.6946689     0.0198899831  0.0286323228               -0.75526695 0.04654347  0.62333532 0.8057824 #>  28:    28 0.5874197 0.0011613517 0.5205859     0.0668337468  0.1283817829               -0.86879847 0.03407861  0.52062680 0.6542125 #>  29:    29 0.5283493 0.0011187357 0.5341385    -0.0057892396 -0.0108384617               -0.87361294 0.03344751  0.46279335 0.5939052 #>  30:    30 0.5795086 0.0027685835 0.5422025     0.0373060733  0.0688046861               -0.68722449 0.05261733  0.47638051 0.6826367 #>  31:    31 0.4414717 0.0015268933 0.4669405    -0.0254687762 -0.0545439480               -0.82750211 0.03907548  0.36488515 0.5180582 #>  32:    32 0.4766418 0.0021112230 0.4590270     0.0176147187  0.0383740323               -0.76148856 0.04594805  0.38658525 0.5666983 #>  33:    33 0.4410774 0.0036700149 0.5098778    -0.0688004595 -0.1349351833               -0.58538697 0.06058065  0.32234149 0.5598133 #>  34:    34 0.6196911 0.0121299017 0.6813577    -0.0616665915 -0.0905054657                0.37035282 0.11013583  0.40382879 0.8355533 #>  35:    35 0.5798466 0.0058569303 0.5980620    -0.0182154715 -0.0304574950               -0.33832432 0.07653058  0.42984938 0.7298438 #>  36:    36 0.6330162 0.0013438280 0.6223294     0.0106868119  0.0171722749               -0.84818356 0.03665826  0.56116736 0.7048651 #>  37:    37 0.4826806 0.0096985759 0.5041018    -0.0214212021 -0.0424937988                0.09567836 0.09848135  0.28966074 0.6757005 #>  38:    38 0.6297604 0.0013349188 0.6490027    -0.0192423404 -0.0296490898               -0.84919006 0.03653654  0.55815009 0.7013707 #>  39:    39 0.6207595 0.0020397236 0.6197995     0.0009600030  0.0015488928               -0.76956607 0.04516330  0.53224108 0.7092780 #>  40:    40 0.6472277 0.0068671224 0.7440085    -0.0967808618 -0.1300803149               -0.22419977 0.08286810  0.48480916 0.8096461 #>  41:    41 0.5725539 0.0023908102 0.5093588     0.0631951343  0.1240680174               -0.72990272 0.04889591  0.47671969 0.6683881 #>  42:    42 0.6392007 0.0013436991 0.6179314     0.0212693164  0.0344201885               -0.84819812 0.03665650  0.56735532 0.7110462 #>  43:    43 0.6178110 0.0069727988 0.6384992    -0.0206882362 -0.0324013479               -0.21226118 0.08350329  0.45414758 0.7814744 #>  44:    44 0.6078855 0.0009241534 0.5997307     0.0081548794  0.0135975695               -0.89559550 0.03039989  0.54830285 0.6674682 #>  45:    45 0.5727816 0.0102875044 0.6796075    -0.1068258610 -0.1571875865                0.16221145 0.10142734  0.37398770 0.7715756 #>  46:    46 0.4926007 0.0028690838 0.4808381     0.0117625096  0.0244625136               -0.67587065 0.05356383  0.38761748 0.5975838 #>  47:    47 0.5483488 0.0051467875 0.5236531     0.0246956742  0.0471603693               -0.41855137 0.07174111  0.40773879 0.6889588 #>  48:    48 0.6034589 0.0048266330 0.6381896    -0.0347307751 -0.0544207750               -0.45472022 0.06947397  0.46729238 0.7396254 #>  49:    49 0.5198579 0.0029179008 0.4351313     0.0847266509  0.1947151584               -0.67035565 0.05401760  0.41398536 0.6257304 #>  50:    50 0.7275270 0.0018915740 0.6891961     0.0383309062  0.0556168366               -0.78630299 0.04349223  0.64228379 0.8127702 #>  51:    51 0.4740533 0.0008184172 0.5132637    -0.0392103862 -0.0763942337               -0.90754086 0.02860799  0.41798267 0.5301239 #>  52:    52 0.5537644 0.0009899394 0.5203003     0.0334640587  0.0643168120               -0.88816346 0.03146330  0.49209745 0.6154313 #>  53:    53 0.7027147 0.0009531000 0.6735536     0.0291610542  0.0432943338               -0.89232532 0.03087232  0.64220602 0.7632233 #>  54:    54 0.6255499 0.0034253397 0.5774410     0.0481089078  0.0833139833               -0.61302869 0.05852640  0.51084024 0.7402595 #>  55:    55 0.3611252 0.0249086524 0.4948395    -0.1337143391 -0.2702175925                1.81400813 0.15782475  0.05179433 0.6704560 #>  56:    56 0.4769535 0.0009727451 0.4594448     0.0175086534  0.0381082811               -0.89010596 0.03118886  0.41582446 0.5380825 #>  57:    57 0.4879967 0.0033098639 0.5259849    -0.0379881886 -0.0722229703               -0.62607435 0.05753142  0.37523716 0.6007562 #>  58:    58 0.6215578 0.0021908236 0.6136904     0.0078673538  0.0128197431               -0.75249583 0.04680623  0.52981926 0.7132963 #>  59:    59 0.5461506 0.0015949373 0.4976941     0.0484565296  0.0973620737               -0.81981496 0.03993667  0.46787620 0.6244251 #>  60:    60 0.4702621 0.0058933050 0.5186381    -0.0483760528 -0.0932751608               -0.33421495 0.07676786  0.31979980 0.6207243 #>  61:    61 0.7637313 0.0009123368 0.7387288     0.0250024249  0.0338451998               -0.89693046 0.03020491  0.70453071 0.8229318 #>  62:    62 0.4716809 0.0030464720 0.4990114    -0.0273304964 -0.0547692834               -0.65583056 0.05519486  0.36350097 0.5798608 #>  63:    63 0.6490774 0.0007906656 0.6161465     0.0329308651  0.0534464831               -0.91067604 0.02811878  0.59396560 0.7041892 #>  64:    64 0.4493761 0.0031519295 0.4102655     0.0391105578  0.0953298721               -0.64391670 0.05614205  0.33933967 0.5594125 #>  65:    65 0.5955382 0.0129155241 0.7107474    -0.1152091858 -0.1620958344                0.45910703 0.11364649  0.37279515 0.8182812 #>  66:    66 0.5748850 0.0064759657 0.5248484     0.0500366203  0.0953353741               -0.26838997 0.08047338  0.41716010 0.7326100 #>  67:    67 0.5524612 0.0033530859 0.5486157     0.0038455232  0.0070095025               -0.62119143 0.05790584  0.43896787 0.6659546 #>  68:    68 0.5195672 0.0020548354 0.5665335    -0.0469663296 -0.0829012357               -0.76785884 0.04533029  0.43072146 0.6084129 #>  69:    69 0.4304486 0.0028164783 0.4738291    -0.0433805464 -0.0915531414               -0.68181367 0.05307050  0.32643232 0.5344649 #>  70:    70 0.5981785 0.0021411053 0.5800398     0.0181387398  0.0312715441               -0.75811266 0.04627208  0.50748692 0.6888701 #>  71:    71 0.3344481 0.0018145011 0.3679618    -0.0335137331 -0.0910793775               -0.79501016 0.04259696  0.25095957 0.4179366 #>  72:    72 0.4888112 0.0021549633 0.4693497     0.0194614845  0.0414647849               -0.75654708 0.04642158  0.39782656 0.5797958 #>  73:    73 0.5788351 0.0021856980 0.6128439    -0.0340087939 -0.0554934056               -0.75307488 0.04675145  0.48720393 0.6704662 #>  74:    74 0.5064910 0.0024730697 0.5187648    -0.0122737913 -0.0236596439               -0.72060960 0.04972997  0.40902210 0.6039600 #>  75:    75 0.5227681 0.0120303288 0.5790717    -0.0563035712 -0.0972307426                0.35910376 0.10968286  0.30779368 0.7377426 #>  76:    76 0.6457163 0.0094378094 0.5794493     0.0662670224  0.1143620706                0.06621876 0.09714839  0.45530898 0.8361237 #>  77:    77 0.6248139 0.0021666692 0.6158526     0.0089612869  0.0145510257               -0.75522463 0.04654749  0.53358246 0.7160453 #>  78:    78 0.1949192 0.1744233778 0.5537660    -0.3588467270 -0.6480115225               18.70515288 0.41764025 -0.62364061 1.0134791 #>  79:    79 0.7132369 0.0024377406 0.6504199     0.0628170200  0.0965791855               -0.72460084 0.04937348  0.61646663 0.8100071 #>  80:    80 0.5877896 0.0018526888 0.6340194    -0.0462297809 -0.0729154044               -0.79069596 0.04304287  0.50342714 0.6721521 #>  81:    81 0.5888747 0.0020205773 0.5348018     0.0540728760  0.1011082563               -0.77172908 0.04495083  0.50077265 0.6769767 #>  82:    82 0.7170877 0.0030761433 0.6748867     0.0422009724  0.0625304557               -0.65247850 0.05546299  0.60838220 0.8257931 #>  83:    83 0.6262660 0.0015754096 0.5506734     0.0755926267  0.1372730616               -0.82202106 0.03969143  0.54847228 0.7040598 #>  84:    84 0.5817337 0.0055618524 0.5291393     0.0525944086  0.0993961455               -0.37166019 0.07457783  0.43556387 0.7279036 #>  85:    85 0.3372241 0.0018526477 0.3982926    -0.0610684645 -0.1533256314               -0.79070061 0.04304239  0.25286259 0.4215857 #>  86:    86 0.6954308 0.0263460617 0.5984294     0.0970014525  0.1620934011                1.97639674 0.16231470  0.37729986 1.0135618 #>  87:    87 0.4766444 0.0065789900 0.4980668    -0.0214224033 -0.0430111074               -0.25675098 0.08111097  0.31766978 0.6356190 #>  88:    88 0.2370559 0.0092909719 0.2658272    -0.0287712412 -0.1082328789                0.04963006 0.09638969  0.04813562 0.4259763 #>  89:    89 0.6317936 0.0008887415 0.5639344     0.0678591133  0.1203315633               -0.89959610 0.02981177  0.57336357 0.6902235 #>  90:    90 0.6741108 0.0027238213 0.6531500     0.0209608302  0.0320919084               -0.69228141 0.05219024  0.57181984 0.7764018 #>  91:    91 0.5722533 0.0013877831 0.4958937     0.0763595741  0.1539837506               -0.84321782 0.03725296  0.49923883 0.6452678 #>  92:    92 0.6236436 0.0014768050 0.6160438     0.0075998926  0.0123366116               -0.83316073 0.03842922  0.54832376 0.6989635 #>  93:    93 0.5109169 0.0010766705 0.5391237    -0.0282067600 -0.0523196456               -0.87836517 0.03281266  0.44660529 0.5752286 #>  94:    94 0.6452847 0.0046669813 0.5782210     0.0670637244  0.1159828564               -0.47275657 0.06831531  0.51138919 0.7791803 #>  95:    95 0.3110406 0.0701107106 0.4641988    -0.1531582716 -0.3299410851                6.92062559 0.26478427 -0.20792706 0.8300082 #>  96:    96 0.4869943 0.0027929223 0.4804765     0.0065177422  0.0135651629               -0.68447486 0.05284811  0.38341387 0.5905746 #>  97:    97 0.8769112 0.4249120933 0.2277933     0.6491178219  2.8495909229               47.00364416 0.65185282 -0.40069688 2.1545192 #>  98:    98 0.4717461 0.0091029030 0.5374690    -0.0657228675 -0.1222821591                0.02838333 0.09540913  0.28474765 0.6587446 #>  99:    99 0.6002216 0.0014933388 0.5997550     0.0004666716  0.0007781039               -0.83129286 0.03864374  0.52448129 0.6759620 #> 100:   100 0.7499394 0.0018908801 0.6801481     0.0697912308  0.1026118113               -0.78638138 0.04348425  0.66471179 0.8351669 #>        sim  estimate variance_est  estimand prediction_error est_rel_error variance_prediction_error  std_error    ci_lower  ci_upper #>  #> $summary_table #>                                       Metric   Value #> 1             Empirical Bias on Policy Value 0.01049 #> 2     Average relative error on Policy Value 0.03564 #> 3             RMSE of errors on Policy Value 0.31144 #> 4 Empirical Coverage of Confidence Intervals 0.97000 #>  #> $interactive_table"},{"path":"https://yanisvdc.github.io/cramR/articles/quickstart.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Quick Start with CRAM","text":"cram_policy: Learn evaluate decision rules batch data. cram_ml: Evaluate ML models customizable losses. cram_bandit: Evaluate contextual bandits using -policy evaluation. flexible, unified framework supports standard modeling complex experimentation.","code":""},{"path":"https://yanisvdc.github.io/cramR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yanis Vandecasteele. Maintainer, author. Michael Lingzhi Li. Author. Kosuke Imai. Author. Zeyang Jia. Author. Longlin Wang. Author.","code":""},{"path":"https://yanisvdc.github.io/cramR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Vandecasteele Y, Li M, Imai K, Jia Z, Wang L (2025). cramR: Cram Method Efficient Simultaneous Learning Evaluation. R package version 0.1.0, https://yanisvdc.github.io/cramR , https://github.com/yanisvdc/cramR.","code":"@Manual{,   title = {cramR: The Cram Method for Efficient Simultaneous Learning and Evaluation},   author = {Yanis Vandecasteele and Michael Lingzhi Li and Kosuke Imai and Zeyang Jia and Longlin Wang},   year = {2025},   note = {R package version 0.1.0, https://yanisvdc.github.io/cramR },   url = {https://github.com/yanisvdc/cramR}, }"},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-what-is-cram","dir":"","previous_headings":"","what":"üìö What is CRAM?","title":"CRAM","text":"CRAM method powerful approach simultaneously learning evaluating decision rules‚Äîindividualized treatment rules (ITRs)‚Äîdata. particularly relevant high-stakes applications decisions must personalized justified. Common examples include: Healthcare: determining receive treatment based individual characteristics Advertising pricing: setting optimal prices maximize revenue Policy interventions: deciding individuals regions receive targeted support improve outcomes Unlike traditional approaches like sample splitting cross-validation, reserve portion data purely evaluation, CRAM reuses available data efficiently training evaluation. key distinction cross-validation CRAM evaluates final learned model directly, rather averaging performance multiple models trained different data subsets‚Äîresulting sharper inference better alignment real-world deployment.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-key-features","dir":"","previous_headings":"","what":"üéØ Key Features","title":"CRAM","text":"üß† Cram Policy (cram_policy): Learn evaluate individualized binary treatment rules using CRAM. Offers flexible model choices, including causal forests custom learners. üìà Cram ML (cram_ml): Learn evaluate ML models using CRAM. Supports flexible model training (via caret user-defined functions) custom loss functions. üé∞ Cram Bandit (cram_bandit): Learn perform -policy evaluation contextual bandit algorithms using CRAM. Supports real data simulation environments built-policies.","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-documentation","dir":"","previous_headings":"","what":"üìö Documentation","title":"CRAM","text":"Getting Started Function Reference ‚Äôs New","code":""},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_Ô∏è-installation","dir":"","previous_headings":"","what":"üõ†Ô∏è Installation","title":"CRAM","text":"install development version CRAM GitHub:","code":"# Install devtools if needed install.packages(\"devtools\")  # Install cramR from GitHub devtools::install_github(\"yanisvdc/cramR\")"},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-citation","dir":"","previous_headings":"üìÑ Citation & ü§ù Contributing","what":"üìö Citation","title":"CRAM","text":"use CRAM research, please cite following paper: can also cite R package:","code":"@article{jia2024cram,   title={The Cram Method for Efficient Simultaneous Learning and Evaluation},   author={Jia, Zeyang and Imai, Kosuke and Li, Michael Lingzhi},   journal={arXiv preprint arXiv:2403.07031},   year={2024} } @Manual{,   title  = {cramR: The Cram Method for Efficient Simultaneous Learning and Evaluation},   author = {Yanis Vandecasteele and Michael Lingzhi Li and Kosuke Imai and Zeyang Jia and Longlin Wang},   year   = {2025},   note   = {R package version 0.1.0},   url    = {https://github.com/yanisvdc/cramR} }"},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-how-to-contribute","dir":"","previous_headings":"üìÑ Citation & ü§ù Contributing","what":"ü§ù How to Contribute","title":"CRAM","text":"welcome contributions! contribute:","code":"# 1. Fork the repository.  # 2. Create a new branch git checkout -b feature/your-feature  # 3. Commit your changes git commit -am 'Add some feature'  # 4. Push to the branch git push origin feature/your-feature  # 5. Create a pull request.  # 6. Open an Issue or PR at: # https://github.com/yanisvdc/cramR/issues"},{"path":"https://yanisvdc.github.io/cramR/index.html","id":"id_-contact","dir":"","previous_headings":"","what":"üìß Contact","title":"CRAM","text":"questions issues, please open issue.","code":""},{"path":"https://yanisvdc.github.io/cramR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Yanis Vandecasteele Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":null,"dir":"Reference","previous_headings":"","what":"Averaged CRAM with Permutations ‚Äî averaged_cram","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"function implements Averaged CRAM randomly permuting batches (except last batch kept ) averaging performance results.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"","code":"averaged_cram(   X,   D,   Y,   batch,   model_type,   learner_type = NULL,   alpha = 0.05,   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   num_permutations = 10 )"},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"X matrix data frame covariates. D binary vector treatment indicators (0 1). Y vector outcomes. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". alpha Significance level confidence intervals. Default 0.05 (95% confidence). baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. num_permutations Number random permutations batches.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"list averaged performance variance estimates.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/averaged_cram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Averaged CRAM with Permutations ‚Äî averaged_cram","text":"","code":"X <- matrix(rnorm(1000), nrow = 100, ncol = 10) D <- sample(0:1, 100, replace = TRUE) Y <- rnorm(100) avg_cram_results <- averaged_cram(X, D, Y,                                   batch = 20,                                   model_type = \"m_learner\",                                   learner_type = \"ridge\",                                   num_permutations = 3) #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"Performs Cram method -policy Statistical Evaluation Contextual Bandit","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"","code":"cram_bandit(pi, arm, reward, alpha = 0.05)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"pi row j, column t, depth , gives pi_t(Xj, ) arm arms selected time step reward rewards time step alpha Confidence level intervals (default = 0.05).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Bandit - On-policy Statistical Evaluation in Contextual Bandit ‚Äî cram_bandit","text":"list containing: raw_results: Data frame performance metrics interactive_table: DT::datatable interactive view final_ml_model: Trained model object","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":null,"dir":"Reference","previous_headings":"","what":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"function implements armed bandit policy evaluation formula estimating \\(\\Delta(\\pi_T; \\pi_0)\\) given user-provided formula.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"","code":"cram_bandit_est(pi, reward, arm, batch = 1)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"pi 3-d array, row j, column t, depth , gives pi_t(Xj, ) reward vector rewards arm vector arms chosen batch vector integer. vector, gives batch assignment context. integer, interpreted batch size contexts assigned batch order dataset.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_est.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cramming Policy Evaluation for Multi-Armed Bandit ‚Äî cram_bandit_est","text":"estimated policy value difference \\(\\Delta(\\pi_T; \\pi_0)\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":null,"dir":"Reference","previous_headings":"","what":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"function performs simulation cram bandit policy learning evaluation.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"","code":"cram_bandit_sim(   horizon,   simulations,   bandit,   policy,   alpha = 0.05,   do_parallel = FALSE,   seed = 42 )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"horizon number timesteps simulations number simulations bandit bandit, generating contextual features observed rewards policy policy, choosing arm timestep alpha Significance level confidence intervals calculating empirical coverage. Default 0.05 (95% confidence). do_parallel Whether parallelize simulations. Default FALSE. seed Seed simulation","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_sim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"On-policy CRAM Bandit ‚Äî cram_bandit_sim","text":"**list** containing: raw_results data frame summarizing key metrics:   Average Prediction Error Policy Value Estimate,   Average Prediction Error Variance Estimate,   Empirical Coverage alpha level Confidence Interval. interactive_table interactive table summarizing key metrics detailed exploration.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Bandit Variance ‚Äî cram_bandit_var","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"function implements crammed variance estimator bandit policy","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"","code":"cram_bandit_var(pi, reward, arm, batch = 1)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"pi 3-d array, row j, column t, depth , gives pi_t(Xj, ) reward vector rewards arm vector arms chosen batch Batch size","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_bandit_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Bandit Variance ‚Äî cram_bandit_var","text":"crammed variance estimate bandit.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"function returns cram estimator policy value difference (delta).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"","code":"cram_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Estimator for Policy Value Difference (Delta) ‚Äî cram_estimator","text":"estimated policy value difference \\(\\(\\hat{\\Delta}\\)(\\(\\hat{\\pi}\\)_T; \\(\\pi\\)_0)\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"function computes Cram expected loss estimator \\(\\hat{R}_{\\mathrm{cram}}\\) based given loss matrix batch indices.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"","code":"cram_expected_loss(loss, batch_indices)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"loss matrix loss values \\(N\\) rows (data points) \\(K+1\\) columns (batches). assume first column loss matrix contains zeros. following nb_batch columns contain losses trained model individual. batch_indices list element vector indices corresponding batch. example: split(1:N, rep(1:nb_batch, = N / nb_batch)).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"estimated Cram expected loss \\(\\hat{R}_{\\mathrm{cram}}\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_expected_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Expected Loss Estimation ‚Äî cram_expected_loss","text":"","code":"# Example usage set.seed(123) N <- 100  # Number of data points K <- 10   # Number of batches  # Generate a loss matrix with K+1 columns, first column as zeros loss <- matrix(rnorm(N * (K+1)), nrow = N, ncol = K+1) loss[, 1] <- 0  # Ensure first column is zero  # Create batch indices dynamically batch_indices <- split(1:N, rep(1:K, length.out = N))  # Compute Cram Expected Loss cram_expected_loss(loss, batch_indices) #> [1] 0.2474586"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Learning ‚Äî cram_learning","title":"CRAM Learning ‚Äî cram_learning","text":"function performs learning part cram method.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Learning ‚Äî cram_learning","text":"","code":"cram_learning(   X,   D,   Y,   batch,   model_type = \"causal_forest\",   learner_type = \"ridge\",   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   n_cores = detectCores() - 1 )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Learning ‚Äî cram_learning","text":"X matrix data frame covariates sample. D vector binary treatment indicators (1 treated, 0 untreated). Y vector outcome values sample. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". baseline_policy list providing baseline policy (binary 0 1) sample. NULL, baseline policy defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. n_cores Number cores use parallelization parallelize_batch set TRUE. Defaults detectCores() - 1.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Learning ‚Äî cram_learning","text":"list containing: final_policy_model final fitted policy model, depending model_type learner_type. policies matrix learned policies, column represents batch's learned policy first column baseline policy. batch_indices indices batch, either generated (batch integer) provided user.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_learning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Learning ‚Äî cram_learning","text":"","code":"# Example usage X_data <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D_data <- sample(c(0, 1), 100, replace = TRUE) Y_data <- rnorm(100) nb_batch <- 5  # Perform CRAM learning result <- cram_learning(X = X_data, D = D_data, Y = Y_data, batch = nb_batch)  # Access the learned policies and final model policies_matrix <- result$policies final_model <- result$final_policy_model batch_indices <- result$batch_indices"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"Performs CRAM method (Causal Regularization via Approximate Models) simultaneous machine learning evaluation experimental observational studies unknown data generating processes.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"","code":"cram_ml(   data,   batch,   formula = NULL,   caret_params = NULL,   parallelize_batch = FALSE,   loss_name = NULL,   custom_fit = NULL,   custom_predict = NULL,   custom_loss = NULL,   alpha = 0.05 )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"data matrix data frame covariates. supervised learning, must include target variable specified `formula`. batch Integer specifying number batches vector pre-defined batch assignments. formula Optional formula supervised learning (e.g., y ~ .). Use NULL unsupervised methods like clustering. caret_params List parameters `caret::train()` containing: method: Model type (e.g., \"rf\", \"glm\", \"xgbTree\" supervised;         \"kmeans\" clustering) Additional method-specific parameters parallelize_batch Logical indicating whether parallelize batch processing (default = FALSE). loss_name Name loss metric (supported: \"se\", \"logloss\", \"accuracy\", \"euclidean_distance\"). custom_fit Optional custom model training function. custom_predict Optional custom prediction function. custom_loss Optional custom loss function. alpha Confidence level intervals (default = 0.05).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"list containing: raw_results: Data frame performance metrics interactive_table: DT::datatable interactive view final_ml_model: Trained model object","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"CRAM method implements novel approach simultaneous model training evaluation unknown data distributions. Key features: Automated batch-wise model training Cross-validation compatible Supports supervised unsupervised learning Provides confidence intervals loss estimates","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_ml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM ML - Simultaneous Machine Learning and Evaluation ‚Äî cram_ml","text":"","code":"# Load necessary libraries library(caret) #> Loading required package: ggplot2 #> Loading required package: lattice  # Set seed for reproducibility set.seed(42)  # Generate example dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100)  # Continuous target variable for regression data_df <- data.frame(X_data, Y = Y_data)  # Ensure target variable is included  # Define caret parameters for simple linear regression (no cross-validation) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  # Define the batch count (not used in this simple example) nb_batch <- 5  # Run ML learning function result <- cram_ml(   data = data_df,   formula = Y ~ .,  # Linear regression model   batch = nb_batch,   loss_name = 'se',   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Policy ‚Äî cram_policy","title":"CRAM Policy ‚Äî cram_policy","text":"function performs cram method (simultaneous policy learning evaluation) experimental observational data, data generation process unknown.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Policy ‚Äî cram_policy","text":"","code":"cram_policy(   X,   D,   Y,   batch,   model_type = \"causal_forest\",   learner_type = \"ridge\",   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   alpha = 0.05,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Policy ‚Äî cram_policy","text":"X matrix data frame covariates sample. D vector binary treatment indicators (1 treated, 0 untreated). Y vector outcome values sample. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. alpha Significance level confidence intervals. Default 0.05 (95% confidence). propensity propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Policy ‚Äî cram_policy","text":"list containing: raw_results: data frame summarizing key metrics truncated decimals: Delta Estimate: estimated treatment effect (delta). Delta Standard Error: standard error delta estimate. Delta CI Lower: lower bound confidence interval delta. Delta CI Upper: upper bound confidence interval delta. Policy Value Estimate: estimated policy value. Policy Value Standard Error: standard error policy value estimate. Policy Value CI Lower: lower bound confidence interval policy value. Policy Value CI Upper: upper bound confidence interval policy value. Proportion Treated: proportion individuals treated final policy. interactive_table: interactive table summarizing key metrics detailed exploration. final_policy_model: final fitted policy model based model_type learner_type custom_fit.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Policy ‚Äî cram_policy","text":"","code":"# Example data X_data <- matrix(rnorm(100 * 5), nrow = 100, ncol = 5) D_data <- D_data <- as.integer(sample(c(0, 1), 100, replace = TRUE)) Y_data <- rnorm(100) nb_batch <- 5  # Perform CRAM policy result <- cram_policy(X = X_data,                           D = D_data,                           Y = Y_data,                           batch = nb_batch)  # Access results result$raw_results #>                        Metric    Value #> 1              Delta Estimate  0.36852 #> 2        Delta Standard Error  0.26573 #> 3              Delta CI Lower -0.15230 #> 4              Delta CI Upper  0.88935 #> 5       Policy Value Estimate  0.53258 #> 6 Policy Value Standard Error  0.27544 #> 7       Policy Value CI Lower -0.00727 #> 8       Policy Value CI Upper  1.07243 #> 9          Proportion Treated  1.00000 result$interactive_table  {\"x\":{\"filter\":\"none\",\"vertical\":false,\"caption\":\"<caption>CRAM Experiment Results<\\/caption>\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],[\"Delta Estimate\",\"Delta Standard Error\",\"Delta CI Lower\",\"Delta CI Upper\",\"Policy Value Estimate\",\"Policy Value Standard Error\",\"Policy Value CI Lower\",\"Policy Value CI Upper\",\"Proportion Treated\"],[0.36852,0.26573,-0.1523,0.88935,0.5325800000000001,0.27544,-0.00727,1.07243,1]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Metric<\\/th>\\n      <th>Value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Metric\",\"targets\":1},{\"name\":\"Value\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}result$final_policy_model #> GRF forest object of type causal_forest  #> Number of trees: 100  #> Number of training samples: 100  #> Variable importance:  #>     1     2     3     4     5  #> 0.089 0.187 0.142 0.124 0.160"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"function returns cram estimator policy value (psi).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"","code":"cram_policy_value_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity Propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_policy_value_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Estimator for Policy Value (Psi) ‚Äî cram_policy_value_estimator","text":"estimated policy value.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":null,"dir":"Reference","previous_headings":"","what":"CRAM Simulation ‚Äî cram_simulation","title":"CRAM Simulation ‚Äî cram_simulation","text":"function performs cram method (simultaneous learning evaluation) simulation data, data generation process (DGP) known. data generation process X can given directly function induced provided dataset via row-wise bootstrapping. Results averaged across Monte Carlo replicates given DGP.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRAM Simulation ‚Äî cram_simulation","text":"","code":"cram_simulation(   X = NULL,   dgp_X = NULL,   dgp_D,   dgp_Y,   batch,   nb_simulations,   nb_simulations_truth = NULL,   sample_size,   model_type = \"causal_forest\",   learner_type = \"ridge\",   alpha = 0.05,   baseline_policy = NULL,   parallelize_batch = FALSE,   model_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CRAM Simulation ‚Äî cram_simulation","text":"X Optional. matrix data frame covariates sample inducing empirically DGP covariates. dgp_X Optional. function generate covariate data simulations. dgp_D vectorized function generate binary treatment assignments sample. dgp_Y vectorized function generate outcome variable sample given treatment covariates. batch Either integer specifying number batches (created random sampling) vector length equal sample size providing batch assignment (index) individual sample. nb_simulations number simulations (Monte Carlo replicates) run. nb_simulations_truth Optional. number additional simmulations (Monte Carlo replicates) beyond nb_simulations use calculating true policy value difference (delta) true policy value (psi) sample_size number samples simulation. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". alpha Significance level confidence intervals. Default 0.05 (95% confidence). baseline_policy list providing baseline policy (binary 0 1) sample. NULL, defaults list zeros length number rows X. parallelize_batch Logical. Whether parallelize batch processing (.e. cram method learns T policies, T number batches. learned parallel parallelize_batch TRUE vs. learned sequentially using efficient data.table structure parallelize_batch FALSE, recommended light weight training). Defaults FALSE. model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. custom_fit custom, user-defined, function outputs fitted model given training data (allows flexibility). Defaults NULL. custom_predict custom, user-defined, function making predictions given fitted model test data (allow flexibility). Defaults NULL. propensity propensity score model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CRAM Simulation ‚Äî cram_simulation","text":"list containing: avg_proportion_treated average proportion treated individuals across simulations. avg_delta_estimate average delta estimate across simulations. avg_delta_standard_error average standard error delta estimates. delta_empirical_bias empirical bias delta estimates. delta_empirical_coverage empirical coverage delta confidence intervals. avg_policy_value_estimate average policy value estimate across simulations. avg_policy_value_standard_error average standard error policy value estimates. policy_value_empirical_bias empirical bias policy value estimates. policy_value_empirical_coverage empirical coverage policy value confidence intervals.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/cram_simulation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CRAM Simulation ‚Äî cram_simulation","text":"","code":"# Define data generation process (DGP) functions X_data <- data.table::data.table(   binary = rbinom(100, 1, 0.5),                 # Binary variable (0 or 1)   discrete = sample(1:5, 100, replace = TRUE),  # Discrete variable (1 to 5)   continuous = rnorm(100)                       # Continuous variable ) dgp_D <- function(X) rbinom(nrow(X), 1, 0.5) dgp_Y <- function(D, X) { theta <- ifelse( X[, binary] == 1 & X[, discrete] <= 2,  # Group 1: High benefit 1, ifelse(X[, binary] == 0 & X[, discrete] >= 4,  # Group 3: High adverse effect -1, 0.1)  # Group 2: Neutral effect ) Y <- D * (theta + rnorm(length(D), mean = 0, sd = 1)) +   (1 - D) * rnorm(length(D))  # Outcome for untreated  return(Y) }  # Parameters: nb_simulations <- 10 nb_simulations_truth <- 2 batch <- 5  # Perform CRAM simulation result <- cram_simulation(X = X_data, dgp_D = dgp_D, dgp_Y = dgp_Y,                           batch = batch, nb_simulations = nb_simulations,                           nb_simulations_truth = nb_simulations_truth,                           sample_size=50)  # Access results result$avg_delta_estimate #> NULL result$delta_empirical_bias #> NULL"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"function estimates asymptotic variance cram estimator policy value difference (delta).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"","code":"cram_variance_estimator(X, Y, D, pi, batch_indices, propensity = NULL)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Variance Estimator for Policy Value Difference (Delta) ‚Äî cram_variance_estimator","text":"estimated variance \\(\\hat{v}^2_T\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"function estimates asymptotic variance cram estimator policy value (psi).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"","code":"cram_variance_estimator_policy_value(   X,   Y,   D,   pi,   batch_indices,   propensity = NULL )"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"X matrix data frame covariates sample. Y vector outcomes n individuals. D vector binary treatments n individuals. pi matrix n rows (nb_batch + 1) columns, n sample size nb_batch number batches, containing policy assignment individual policy. first column represents baseline policy. batch_indices list element vector indices corresponding individuals batch. propensity Propensity score function","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_variance_estimator_policy_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cram Variance Estimator for Policy Value (Psi) ‚Äî cram_variance_estimator_policy_value","text":"estimated variance \\(\\hat{w}^2_T\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"function computes variance estimator \\(\\hat{\\sigma}^2_2\\) based given loss matrix batch indices.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"","code":"cram_var_expected_loss(loss, batch_indices)"},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"loss matrix loss values \\(N\\) rows (data points) \\(K+1\\) columns (batches). assume first column loss matrix contains zeros. following nb_batch columns contain losses trained model individual. batch_indices list element vector indices corresponding batch. example: split(1:N, rep(1:nb_batch, = N / nb_batch)).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"estimated Cram expected loss \\(\\hat{R}_{\\mathrm{cram}}\\).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/cram_var_expected_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variance Estimation for Cram Estimator ‚Äî cram_var_expected_loss","text":"","code":"# Example usage set.seed(123) N <- 100  # Number of data points K <- 10   # Number of batches  # Generate a loss matrix with K+1 columns, first column as zeros loss <- matrix(rnorm(N * (K+1)), nrow = N, ncol = K+1) loss[, 1] <- 0  # Ensure first column is zero  # Create batch indices dynamically batch_indices <- split(1:N, rep(1:K, length.out = N))  # Compute Cram Expected Loss Variance cram_var_expected_loss(loss, batch_indices) #> [1] 19.95626"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Model ‚Äî fit_model","title":"Fit Model ‚Äî fit_model","text":"function trains given unfitted model provided data parameters, according model type learner type.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Model ‚Äî fit_model","text":"","code":"fit_model(model, X, Y, D, model_type, learner_type, model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Model ‚Äî fit_model","text":"model unfitted model object, returned `set_model`. X matrix data frame covariates samples. Y vector outcome values. D vector binary treatment indicators (1 treated, 0 untreated). model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Model ‚Äî fit_model","text":"fitted model object.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Model ‚Äî fit_model","text":"","code":"# Example usage for Ridge Regression S-learner set.seed(123) X <- matrix(rnorm(1000), nrow = 100, ncol = 10) D <- sample(0:1, 100, replace = TRUE) Y <- rnorm(100) # Set up the model model <- set_model(\"s_learner\", \"ridge\") # Define model parameters model_params <- list(alpha = 0) # Fit the model fitted_model <- fit_model(                         model, X, Y, D = D,                         model_type = \"s_learner\",                         learner_type = \"ridge\",                         model_params = model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Model ML ‚Äî fit_model_ml","title":"Fit Model ML ‚Äî fit_model_ml","text":"function trains given unfitted model provided data parameters, according model type learner type.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Model ML ‚Äî fit_model_ml","text":"","code":"fit_model_ml(data, formula, caret_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Model ML ‚Äî fit_model_ml","text":"data dataset formula formula caret_params parameters caret model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/fit_model_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Model ML ‚Äî fit_model_ml","text":"fitted model object.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Mock Dataset ‚Äî generate_data","title":"Generate Mock Dataset ‚Äî generate_data","text":"function generates simulated dataset covariates, treatment assignments, outcomes testing experimentation. dataset includes heterogeneous treatment effects across groups, mimicking realistic causal inference scenarios.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Mock Dataset ‚Äî generate_data","text":"","code":"generate_data(n)"},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Mock Dataset ‚Äî generate_data","text":"n Integer. number observations generate.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Mock Dataset ‚Äî generate_data","text":"list containing: X data.table three variables: D Binary treatment assignment (0 1). Y Numeric outcome based treatment effects covariates.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/generate_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Mock Dataset ‚Äî generate_data","text":"","code":"# Generate a dataset with 1000 observations data <- generate_data(1000) str(data) #> List of 3 #>  $ X:Classes 'data.table' and 'data.frame':\t1000 obs. of  3 variables: #>   ..$ binary    : int [1:1000] 1 0 0 0 1 1 0 0 1 1 ... #>   ..$ discrete  : int [1:1000] 3 4 4 4 1 4 1 1 3 2 ... #>   ..$ continuous: num [1:1000] 2.047 -0.485 0.363 1.911 -0.275 ... #>   ..- attr(*, \".internal.selfref\")=<externalptr>  #>  $ D: int [1:1000] 0 0 1 1 1 0 1 0 0 0 ... #>  $ Y: num [1:1000] -0.218 -0.2 -2.267 0.635 1.702 ... head(data$X) #>    binary discrete continuous #>     <int>    <int>      <num> #> 1:      1        3  2.0470005 #> 2:      0        4 -0.4845959 #> 3:      0        4  0.3625913 #> 4:      0        4  1.9112226 #> 5:      1        1 -0.2745191 #> 6:      1        4 -0.6519016 head(data$D) #> [1] 0 0 1 1 1 0 head(data$Y) #> [1] -0.2178099 -0.1997781 -2.2670366  0.6351589  1.7018075 -0.1513984"},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized ML Learning ‚Äî ml_learning","title":"Generalized ML Learning ‚Äî ml_learning","text":"function performs batch-wise learning **supervised** **unsupervised** machine learning models.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized ML Learning ‚Äî ml_learning","text":"","code":"ml_learning(   data,   formula = NULL,   batch,   parallelize_batch = FALSE,   loss_name = NULL,   caret_params = NULL,   custom_fit = NULL,   custom_predict = NULL,   custom_loss = NULL,   n_cores = detectCores() - 1 )"},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized ML Learning ‚Äî ml_learning","text":"data matrix data frame features. supervised model used, also include target variable. formula Optional formula specifying relationship target predictors **supervised learning** (use `NULL` unsupervised learning). batch Either integer specifying number batches (randomly sampled) vector length equal sample size indicating batch assignment observation. parallelize_batch Logical. Whether parallelize batch processing. Defaults `FALSE`. - `TRUE`, batch models trained parallel. - `FALSE`, training performed sequentially using `data.table` efficiency. loss_name name loss function used (e.g., `\"se\"`, `\"logloss\"`). caret_params **list** parameters pass `caret::train()` function. - Required: `method` (e.g., `\"glm\"`, `\"rf\"`, `\"kmeans\"`). - `method = \"kmeans\"`, function automatically return **cluster assignments**. custom_fit **custom function** training user-defined models. Defaults `NULL`. custom_predict **custom function** making predictions user-defined models. Defaults `NULL`. custom_loss Optional **custom function** computing loss trained model data. return **vector** containing per-instance losses. n_cores Number CPU cores use parallel processing (`parallelize_batch = TRUE`). Defaults `detectCores() - 1`.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized ML Learning ‚Äî ml_learning","text":"**list** containing: final_ml_model final trained ML model. losses matrix losses column represents batch's trained model. first column contains zeros (baseline model). batch_indices indices observations batch.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/ml_learning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generalized ML Learning ‚Äî ml_learning","text":"","code":"# Load necessary libraries library(caret)  # Set seed for reproducibility set.seed(42)  # Generate example dataset X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) Y_data <- rnorm(100)  # Continuous target variable for regression data_df <- data.frame(X_data, Y = Y_data)  # Ensure target variable is included  # Define caret parameters for simple linear regression (no cross-validation) caret_params_lm <- list(   method = \"lm\",   trControl = trainControl(method = \"none\") )  # Define the batch count (not used in this simple example) nb_batch <- 5  # Run ML learning function result_lm <- ml_learning(   data = data_df,   formula = Y ~ .,  # Linear regression model   batch = nb_batch,   loss_name = 'se',   caret_params = caret_params_lm )"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with the Specified Model ‚Äî model_predict","title":"Predict with the Specified Model ‚Äî model_predict","text":"function performs inference using trained model, providing flexibility different types models Causal Forest, Ridge Regression, Feedforward Neural Networks (FNNs).","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with the Specified Model ‚Äî model_predict","text":"","code":"model_predict(model, X, D = NULL, model_type, learner_type, model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with the Specified Model ‚Äî model_predict","text":"model trained model object returned `fit_model` function. X matrix data frame covariates predictions required. D vector binary treatment indicators (1 treated, 0 untreated). Optional, depending model type. model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with the Specified Model ‚Äî model_predict","text":"vector predictions CATE estimates, depending model_type learner_type.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict with the Specified Model ‚Äî model_predict","text":"","code":"# Load required library library(grf)  # Example: Predicting with a Causal Forest model set.seed(123) X <- matrix(rnorm(1000), nrow = 100, ncol = 10)  # Covariates Y <- rnorm(100)                                 # Outcomes D <- sample(0:1, 100, replace = TRUE)           # Treatment indicators cf_model <- causal_forest(X, Y, D)             # Train Causal Forest new_X <- matrix(rnorm(100), nrow = 10, ncol = 10) # New data for predictions predictions <- model_predict(model = cf_model, X = new_X, model_type = \"causal_forest\")"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with the Specified Model ‚Äî model_predict_ml","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"function performs inference using trained model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"","code":"model_predict_ml(model, data, formula, caret_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"model trained model object returned `fit_model_ml` function. data dataset formula formula caret_params parameters caret model","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/model_predict_ml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with the Specified Model ‚Äî model_predict_ml","text":"vector predictions CATE estimates, depending model_type learner_type.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Model ‚Äî set_model","title":"Set Model ‚Äî set_model","text":"function maps model type learner type corresponding model function.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Model ‚Äî set_model","text":"","code":"set_model(model_type, learner_type, model_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Model ‚Äî set_model","text":"model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params list additional parameters pass model, can parameter defined model reference package. Defaults NULL. FNNs, following elements defined model params list: input_layer list defining input layer. Must include: units Number units input layer. activation Activation function input layer. input_shape Input shape layer.  layers list lists, sublist specifies hidden layer : units Number units layer. activation Activation function layer.  output_layer list defining output layer. Must include: units Number units output layer. activation Activation function output layer (e.g., \"linear\" \"sigmoid\").  compile_args list arguments compiling model. Must include: optimizer Optimizer training (e.g., \"adam\" \"sgd\"). loss Loss function (e.g., \"mse\" \"binary_crossentropy\"). metrics Optional list metrics evaluation (e.g., c(\"accuracy\")).  learners (e.g., \"ridge\" \"causal_forest\"), model_params can include relevant hyperparameters.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Model ‚Äî set_model","text":"instantiated model object corresponding model function.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/set_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Model ‚Äî set_model","text":"","code":"# Example: Causal Forest with default parameters set_model(\"causal_forest\", NULL, model_params = list(num.trees = 100)) #> function (X, Y, W, Y.hat = NULL, W.hat = NULL, num.trees = 2000,  #>     sample.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,  #>     sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(X)) +  #>         20), ncol(X)), min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,  #>     honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,  #>     stabilize.splits = TRUE, ci.group.size = 2, tune.parameters = \"none\",  #>     tune.num.trees = 200, tune.num.reps = 50, tune.num.draws = 1000,  #>     compute.oob.predictions = TRUE, num.threads = NULL, seed = runif(1,  #>         0, .Machine$integer.max))  #> { #>     has.missing.values <- validate_X(X, allow.na = TRUE) #>     validate_sample_weights(sample.weights, X) #>     Y <- validate_observations(Y, X) #>     W <- validate_observations(W, X) #>     clusters <- validate_clusters(clusters, X) #>     samples.per.cluster <- validate_equalize_cluster_weights(equalize.cluster.weights,  #>         clusters, sample.weights) #>     num.threads <- validate_num_threads(num.threads) #>     all.tunable.params <- c(\"sample.fraction\", \"mtry\", \"min.node.size\",  #>         \"honesty.fraction\", \"honesty.prune.leaves\", \"alpha\",  #>         \"imbalance.penalty\") #>     default.parameters <- list(sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(X)) +  #>         20), ncol(X)), min.node.size = 5, honesty.fraction = 0.5,  #>         honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0) #>     args.orthog <- list(X = X, num.trees = max(50, num.trees/4),  #>         sample.weights = sample.weights, clusters = clusters,  #>         equalize.cluster.weights = equalize.cluster.weights,  #>         sample.fraction = sample.fraction, mtry = mtry, min.node.size = 5,  #>         honesty = TRUE, honesty.fraction = 0.5, honesty.prune.leaves = honesty.prune.leaves,  #>         alpha = alpha, imbalance.penalty = imbalance.penalty,  #>         ci.group.size = 1, tune.parameters = tune.parameters,  #>         num.threads = num.threads, seed = seed) #>     if (is.null(Y.hat)) { #>         forest.Y <- do.call(regression_forest, c(Y = list(Y),  #>             args.orthog)) #>         Y.hat <- predict(forest.Y)$predictions #>     } #>     else if (length(Y.hat) == 1) { #>         Y.hat <- rep(Y.hat, nrow(X)) #>     } #>     else if (length(Y.hat) != nrow(X)) { #>         stop(\"Y.hat has incorrect length.\") #>     } #>     if (is.null(W.hat)) { #>         forest.W <- do.call(regression_forest, c(Y = list(W),  #>             args.orthog)) #>         W.hat <- predict(forest.W)$predictions #>     } #>     else if (length(W.hat) == 1) { #>         W.hat <- rep(W.hat, nrow(X)) #>     } #>     else if (length(W.hat) != nrow(X)) { #>         stop(\"W.hat has incorrect length.\") #>     } #>     Y.centered <- Y - Y.hat #>     W.centered <- W - W.hat #>     data <- create_train_matrices(X, outcome = Y.centered, treatment = W.centered,  #>         sample.weights = sample.weights) #>     args <- list(num.trees = num.trees, clusters = clusters,  #>         samples.per.cluster = samples.per.cluster, sample.fraction = sample.fraction,  #>         mtry = mtry, min.node.size = min.node.size, honesty = honesty,  #>         honesty.fraction = honesty.fraction, honesty.prune.leaves = honesty.prune.leaves,  #>         alpha = alpha, imbalance.penalty = imbalance.penalty,  #>         stabilize.splits = stabilize.splits, ci.group.size = ci.group.size,  #>         compute.oob.predictions = compute.oob.predictions, num.threads = num.threads,  #>         seed = seed, reduced.form.weight = 0, legacy.seed = get_legacy_seed()) #>     tuning.output <- NULL #>     if (!identical(tune.parameters, \"none\")) { #>         if (identical(tune.parameters, \"all\")) { #>             tune.parameters <- all.tunable.params #>         } #>         else { #>             tune.parameters <- unique(match.arg(tune.parameters,  #>                 all.tunable.params, several.ok = TRUE)) #>         } #>         if (!honesty) { #>             tune.parameters <- tune.parameters[!grepl(\"honesty\",  #>                 tune.parameters)] #>         } #>         tune.parameters.defaults <- default.parameters[tune.parameters] #>         tuning.output <- tune_forest(data = data, nrow.X = nrow(X),  #>             ncol.X = ncol(X), args = args, tune.parameters = tune.parameters,  #>             tune.parameters.defaults = tune.parameters.defaults,  #>             tune.num.trees = tune.num.trees, tune.num.reps = tune.num.reps,  #>             tune.num.draws = tune.num.draws, train = causal_train) #>         args <- utils::modifyList(args, as.list(tuning.output[[\"params\"]])) #>     } #>     forest <- do.call.rcpp(causal_train, c(data, args)) #>     class(forest) <- c(\"causal_forest\", \"grf\") #>     forest[[\"seed\"]] <- seed #>     forest[[\"num.threads\"]] <- num.threads #>     forest[[\"ci.group.size\"]] <- ci.group.size #>     forest[[\"X.orig\"]] <- X #>     forest[[\"Y.orig\"]] <- Y #>     forest[[\"W.orig\"]] <- W #>     forest[[\"Y.hat\"]] <- Y.hat #>     forest[[\"W.hat\"]] <- W.hat #>     forest[[\"clusters\"]] <- clusters #>     forest[[\"equalize.cluster.weights\"]] <- equalize.cluster.weights #>     forest[[\"sample.weights\"]] <- sample.weights #>     forest[[\"tunable.params\"]] <- args[all.tunable.params] #>     forest[[\"tuning.output\"]] <- tuning.output #>     forest[[\"has.missing.values\"]] <- has.missing.values #>     forest #> } #> <bytecode: 0x000001fea93e5f70> #> <environment: namespace:grf>"},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"function validates provided baseline policy sets default baseline policy zeros individuals.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"","code":"test_baseline_policy(baseline_policy, n)"},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"baseline_policy list representing baseline policy individual. NULL, default baseline policy zeros created. n integer specifying number individuals population.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"validated default baseline policy list numeric values.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_baseline_policy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate or Set the Baseline Policy ‚Äî test_baseline_policy","text":"","code":"# Example: Default baseline policy baseline_policy <- test_baseline_policy(NULL, n = 10)  # Example: Valid baseline policy valid_policy <- as.list(rep(1, 10)) baseline_policy <- test_baseline_policy(valid_policy, n = 10)  # Example: Invalid baseline policy if (FALSE) { # \\dontrun{ invalid_policy <- c(1, 0, 1, 0) baseline_policy <- test_baseline_policy(invalid_policy, n = 10) } # }"},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate or Generate Batch Assignments ‚Äî test_batch","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"function validates provided batch assignment generates random batch assignments individuals.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"","code":"test_batch(batch, n)"},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"batch Either integer specifying number batches vector/list batch assignments individuals. n integer specifying number individuals population.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"list containing: batches list element contains indices individuals assigned specific batch. nb_batch total number batches.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/test_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate or Generate Batch Assignments ‚Äî test_batch","text":"","code":"# Example: Generate random batch assignments result <- test_batch(3, n = 9) print(result) #> $batches #> $batches$`1` #> [1] 3 1 5 #>  #> $batches$`2` #> [1] 8 6 7 #>  #> $batches$`3` #> [1] 2 9 4 #>  #>  #> $nb_batch #> [1] 3 #>   # Example: Validate a batch assignment vector batch_vector <- c(1, 1, 2, 2, 3, 3, 1, 2, 3) result <- test_batch(batch_vector, n = 9) print(result) #> $batches #> $batches$`1` #> [1] 1 2 7 #>  #> $batches$`2` #> [1] 3 4 8 #>  #> $batches$`3` #> [1] 5 6 9 #>  #>  #> $nb_batch #> [1] 3 #>   # Example: Invalid batch assignment if (FALSE) { # \\dontrun{ invalid_batch <- c(1, 1, 2) result <- test_batch(invalid_batch, n = 9) } # }"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate User-Provided Parameters for a Model ‚Äî validate_params","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"function validates user-provided parameters formal arguments specified model function. ensures user-specified parameters recognized model raises error invalid parameters.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"","code":"validate_params(model_function, model_type, learner_type, user_params)"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"model_function model function parameters validated (e.g., grf::causal_forest). model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". user_params named list parameters provided user.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"named list validated parameters safe pass model function.","code":""},{"path":[]},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate User-Provided Parameters for a Model ‚Äî validate_params","text":"","code":"# Example with causal_forest from grf library(grf) set.seed(123) my_X <- matrix(rnorm(1000), nrow = 100, ncol = 10)  # Covariates my_Y <- rnorm(100)                                  # Outcome variable my_W <- sample(0:1, 100, replace = TRUE)            # Binary treatment indicator # Define user parameters user_params <- list(num.trees = 100)  # Validate parameters valid_params <- validate_params(grf::causal_forest, \"causal_forest\", NULL, user_params)  # Use the validated parameters to call the model # X, Y, W must still be passed explicitly cf_model <- do.call(grf::causal_forest, c(list(X = my_X, Y = my_Y, W = my_W), valid_params))"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"function validates user-provided parameters Feedforward Neural Network (FNN) model. ensures correct structure input_layer, layers, output_layer, compile_args fit_params.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"","code":"validate_params_fnn(model_type, learner_type, model_params, X)"},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"model_type model type policy learning. Options include \"causal_forest\", \"s_learner\", \"m_learner\". Default \"causal_forest\". learner_type learner type chosen model. Options include \"ridge\" Ridge Regression \"fnn\" Feedforward Neural Network. Default \"ridge\". model_params named list parameters provided user configuring FNN model. X matrix data frame covariates parameters validated.","code":""},{"path":"https://yanisvdc.github.io/cramR/reference/validate_params_fnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Parameters for Feedforward Neural Networks (FNNs) ‚Äî validate_params_fnn","text":"named list validated parameters merged defaults missing values.","code":""},{"path":"https://yanisvdc.github.io/cramR/news/index.html","id":"cramr-010","dir":"Changelog","previous_headings":"","what":"cramR 0.1.0","title":"cramR 0.1.0","text":"Initial CRAN submission.","code":""}]

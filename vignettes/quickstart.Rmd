
---
title: "Quick Start with CRAM"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quick Start with CRAM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(pkgdown.max_print = Inf, width = 1000)
library(cramR)
library(data.table)
library(glmnet)
library(caret)
```

## Introduction

The **CRAM** package provides a unified framework for:

- ðŸ§  **Cram Policy (`cram_policy`)**: Learn and evaluate individualized binary treatment rules using flexible model choices, including causal forests and custom learners â€” all while efficiently reusing the entire dataset.

- ðŸ“ˆ **Cram ML (`cram_ml`)**: Assess ML models performance using CRAM. Supports flexible model training (via `caret` or user-defined functions) and custom loss functions.

- ðŸŽ° **Cram Bandit (`cram_bandit`)**: Perform on-policy evaluation of contextual bandit algorithms using CRAM. Supports both real data and simulation environments with built-in policies.

This vignette walks through these **three core modules**.

---

## 1. CRAM Policy

We begin by simulating a dataset consisting of covariates `X`, a binary treatment assignment `D`, and a continuous outcome `Y`, which we will use to demonstrate the `cram_policy()` function.

```{r}
library(data.table)
# Function to generate sample data with heterogeneous treatment effects:
# - Positive effect group
# - Neutral effect group
# - Adverse effect group
generate_data <- function(n) {
  X <- data.table(
    binary = rbinom(n, 1, 0.5),                 # Binary variable
    discrete = sample(1:5, n, replace = TRUE),  # Discrete variable
    continuous = rnorm(n)                       # Continuous variable
  )

  # Binary treatment assignment (50% treated)
  D <- rbinom(n, 1, 0.5)

  # Define heterogeneous treatment effects based on X
  treatment_effect <- ifelse(
    X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect
    1,
    ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect
           -1,
           0.1)                                   # Group 2: Neutral effect
  )

  # Outcome depends on treatment effect + noise
  Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +
    (1 - D) * rnorm(n)

  return(list(X = X, D = D, Y = Y))
}

# Generate a sample dataset
set.seed(123)
n <- 1000
data <- generate_data(n)
X <- data$X
D <- data$D
Y <- data$Y

```

### Package Model

In this example, we demonstrate how to use the **built-in models** and we walk through the key parameters that control how `cram_policy()` behaves, including the choice of model, learner type, baseline policy, and batching strategy. 

These parameters allow flexibility in configuring the learning process depending on your use case and computational resources. 

```{r}
# Number of batches to split the data into
batch <- 20  

# Model type for estimating treatment effects
# Options: "causal_forest", "s_learner", "m_learner", NULL (to use custom model)
model_type <- "causal_forest"  

# Learner type used inside s/m-learners
# NULL is required for causal_forest; use "ridge" or "fnn" for s/m learners
learner_type <- NULL  

# Baseline policy to compare against (list of 0/1 for each individual)
# Common options:
# - All-control baseline: as.list(rep(0, nrow(X)))
# - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE))
baseline_policy <- as.list(rep(0, nrow(X)))  

# Whether to parallelize across batches (TRUE for faster but memory-heavy runs)
parallelize_batch <- FALSE  

# Model-specific parameters
# Examples:
# - causal_forest: list(num.trees = 100)
# - ridge: list(alpha = 1)
# - fnn (Feedforward Neural Network): see below
model_params <- NULL  

# Advanced example for fnn (not used here):
# default_model_params <- list(
#   input_layer = list(units = 64, activation = 'relu', input_shape = input_shape),
#   layers = list(list(units = 32, activation = 'relu')),
#   output_layer = list(units = 1, activation = 'linear'),
#   compile_args = list(optimizer = 'adam', loss = 'mse'),
#   fit_params = list(epochs = 5, batch_size = 32, verbose = 0)
# )

# Significance level for confidence intervals (default = 95%)
alpha <- 0.05  

# Run the CRAM policy method
result <- cram_policy(
  X, D, Y,
  batch = batch,
  model_type = model_type,
  learner_type = learner_type,
  baseline_policy = baseline_policy,
  parallelize_batch = parallelize_batch,
  model_params = model_params,
  alpha = alpha
)

# Display the results
print(result)

```

### Custom Model

To use your own model for policy learning, you can supply two user-defined functions:

#### 1. `custom_fit(X, Y, D, ...)`  
This function takes the training data: covariates `X`, outcomes `Y`, and binary treatment indicators `D`, and returns a fitted model object.  
You may also define and use additional parameters (e.g., number of folds, regularization settings, etc.) within the function body.

- `X`: a matrix or data frame of features 
- `Y`: a numeric outcome vector  
- `D`: a binary vector indicating treatment assignment (0 or 1)

**Example**: Custom X-learner with Ridge regression and 5-fold cross-validation

```{r}
custom_fit <- function(X, Y, D, n_folds = 5) {
  treated_indices <- which(D == 1)
  control_indices <- which(D == 0)
  X_treated <- X[treated_indices, ]
  Y_treated <- Y[treated_indices]
  X_control <- X[control_indices, ]
  Y_control <- Y[control_indices]
  model_treated <- cv.glmnet(as.matrix(X_treated), Y_treated, alpha = 0, nfolds = n_folds)
  model_control <- cv.glmnet(as.matrix(X_control), Y_control, alpha = 0, nfolds = n_folds)
  tau_control <- Y_treated - predict(model_control, as.matrix(X_treated), s = "lambda.min")
  tau_treated <- predict(model_treated, as.matrix(X_control), s = "lambda.min") - Y_control
  X_combined <- rbind(X_treated, X_control)
  tau_combined <- c(tau_control, tau_treated)
  weights <- c(rep(1, length(tau_control)), rep(1, length(tau_treated)))
  final_model <- cv.glmnet(as.matrix(X_combined), tau_combined, alpha = 0, weights = weights, nfolds = n_folds)
  return(final_model)
}
```


#### 2. `custom_predict(model, X_new, D_new)`

This function uses the fitted model to generate a **binary treatment decision** for each individual in `X_new`.

It should return a vector of 0s and 1s, indicating whether to assign treatment (`1`) or not (`0`).  
You may also incorporate a custom threshold or post-processing logic within the function.

**Example**: Apply the decision rule â€” treat if the estimated CATE is greater than 0
```{r}
custom_predict <- function(model, X_new, D_new) {
  cate <- predict(model, as.matrix(X_new), s = "lambda.min")

  # Apply decision rule: treat if CATE > 0
  as.integer(cate > 0)
}
```


#### 3. Use `cram_policy()` with `custom_fit()` and `custom_predict()`

Once both `custom_fit()` and `custom_predict()` are defined, you can integrate them into the CRAM framework by passing them to `cram_policy()` as shown below:

```{r}
experiment_results <- cram_policy(
  X, D, Y,
  batch = 20,
  custom_fit = custom_fit,
  custom_predict = custom_predict,
  alpha = 0.05
)
print(experiment_results)
```
---

## 2. CRAM ML Evaluation

```{r}
set.seed(42)
X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100))
Y_data <- rnorm(100)
data_df <- data.frame(X_data, Y = Y_data)

caret_params_lm <- list(
  method = "lm",
  trControl = trainControl(method = "none")
)

result <- cram_ml(
  data = data_df,
  formula = Y ~ .,
  batch = 5,
  loss_name = "mse",
  caret_params = caret_params_lm
)
print(result)
```

### Custom Model

```{r}
custom_fit <- function(data) {
  lm(Y ~ x1 + x2 + x3, data = data)
}

custom_predict <- function(model, data) {
  predictors_only <- data[, setdiff(names(data), "Y"), drop = FALSE]
  predict(model, newdata = predictors_only)
}

custom_loss <- function(predictions, data) {
  actuals <- data$Y
  mse_loss <- (predictions - actuals)^2
  return(mse_loss)
}

result <- cram_ml(
  data = data_df,
  formula = Y ~ .,
  batch = 5,
  custom_fit = custom_fit,
  custom_predict = custom_predict,
  custom_loss = custom_loss
)
print(result)
```

---

## 3. CRAM Bandit Evaluation

```{r}
set.seed(42)
T <- 100
K <- 4
pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K))
for (t in 1:T) {
  for (j in 1:T) {
    pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])
  }
}
arm <- sample(1:K, T, replace = TRUE)
reward <- rnorm(T, mean = 1, sd = 0.5)
cram_results <- cram_bandit(pi, arm, reward)
print(cram_results)
```

### Bandit Simulation

```{r}
horizon     <- 500L
simulations <- 100L
k <- 4
d <- 3
list_betas <- cramR:::get_betas(simulations, d, k)
bandit     <- cramR:::ContextualLinearBandit$new(k = k, d = d, list_betas = list_betas, sigma = 0.3)
policy     <- cramR:::BatchContextualEpsilonGreedyPolicy$new(epsilon = 0.1, batch_size = 5)

sim <- cram_bandit_sim(
  horizon, simulations,
  bandit, policy,
  alpha = 0.05, do_parallel = FALSE
)
print(sim)
```

---

## Summary

- **cram_policy**: Learn and evaluate decision rules from batch data.
- **cram_ml**: Evaluate ML models with customizable losses.
- **cram_bandit**: Evaluate contextual bandits using on-policy evaluation.

This flexible, unified framework supports both standard modeling and complex experimentation.

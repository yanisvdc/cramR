
---
title: "Quick Start with CRAM"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quick Start with CRAM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(pkgdown.max_print = Inf, width = 1000)
library(cramR)
library(data.table)
library(glmnet)
library(caret)
```

## Introduction

The **Cram** package provides a unified framework for:

- 🧠 **Cram Policy (`cram_policy`)**: Learn and evaluate individualized binary treatment rules using CRAM. Offers flexible model choices, including causal forests and custom learners.

- 📈 **Cram ML (`cram_ml`)**: Learn and evaluate ML models using CRAM. Supports flexible model training (via `caret` or user-defined functions) and custom loss functions.

- 🎰 **Cram Bandit (`cram_bandit`)**: Learn and perform on-policy evaluation of contextual bandit algorithms using CRAM. Supports both real data and simulation environments with built-in policies.

This vignette walks through these **three core modules**.

---

## 3. CRAM Bandit

This section illustrates how to use the `cram_bandit()` function to **evaluate the final learned policy of a contextual bandit algorithm using the same data collected by the algorithm itself**. 

To demonstrate the method, we assume a contextual bandit sequence has been run and has produced:

- a sequence of learned policies (encoded as arrays of probabilities `π_t(x, a)`),
- the arms actually chosen (`arm`),
- the corresponding rewards observed (`reward`).

The `cram_bandit()` function requires a policy probability array `pi`, which captures how the learned policies assign probability mass across actions over time.

This array can be specified in one of two formats:

1. **3D Array**: An array of shape `(n, T, K)`, where:

   - `T` is the number of time steps (i.e., policy updates),
   - `K` is the number of available arms,
   - `n = T × batch`, where `batch` is the batch size i.e. the number of actions taken between each policy update.

Each element `pi[j, t, k]` represents the probability that the policy at time `t` assigns arm `k` to context `x_j`.

2. **2D Array**: An array of shape `(n, T)`:

   - Each entry represents the probability assigned by the policy at time `t` to the arm that was actually chosen under context `x_j`.

This compact form omits the full distribution over arms and assumes you are only tracking the realized action probabilities.

> 🛠️ If you need to compute this probability array from a trained policy or historical data, the `cramR` package provides helper utilities in the `cramR:::` namespace (see “Bandit Helpers” vignette). Note that the exact method may depend on how your bandit logs and models are structured.

```{r}
# Assume pi is a 3D array: observations x time x arms
# Assume arm and reward are vectors of length = nrow(pi)
set.seed(42)
T <- 100
K <- 4
pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K))
for (t in 1:T) {
  for (j in 1:T) {
    pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])
  }
}
arm <- sample(1:K, T, replace = TRUE)
reward <- rnorm(T, mean = 1, sd = 0.5)
cram_results <- cram_bandit(pi, arm, reward, batch=1)

# View summary of the evaluation
print(cram_results$raw_results)

# View interactive summary table
cram_results$interactive_table
```

The returned object includes both a raw numerical summary and an interactive table showing:

- the estimated policy value,
- its standard error,
- and the 95% confidence interval bounds.

---

## Summary

- **cram_policy**: Learn and evaluate decision rules from batch data.
- **cram_ml**: Evaluate ML models with customizable losses.
- **cram_bandit**: Evaluate contextual bandits using on-policy evaluation.

This flexible, unified framework supports both standard modeling and complex experimentation.

---
title: "Using cram_policy() for Policy Learning and Evaluation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using cram_policy() for Policy Learning and Evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(cramR)
library(DT)
```

## Introduction: What is the CRAM Method?

The **CRAM method** is a powerful approach for **simultaneously learning and evaluating decision rules**, such as individualized treatment rules (ITRs), from data.

Unlike traditional approaches like **sample splitting** or **cross-validation**, which waste part of the data on evaluation only, **CRAM reuses all available data** efficiently.

A key distinction from **cross-validation** is that CRAM evaluates the final learned model, rather than averaging performance across multiple models trained on different data splits.


CRAM:

- Sequentially trains and evaluates models on *batches* of data  
- Tracks model evolution with minimal assumptions  
- Estimates treatment effects and policy values using the **entire dataset**, increasing efficiency and statistical power

> Think of CRAM like a cram school: learn a bit, test a bit, repeat — getting better while constantly self-evaluating.

## The CRAM Workflow

Below is the core idea of the CRAM method visualized:

![ ](cram_visual_1.png)


This procedure ensures each update is backed by performance testing, enabling **both learning and evaluation in one pass** over the data.

![ ](cram_visual_3.png)


This schematic represents how CRAM averages over **many mini train-test steps**, ultimately estimating the improvement of the final learned rule over the baseline. 

Mathematically, the estimated policy value difference is:

\[
\hat{\Delta}(\pi_T; \pi_0) = \sum_{t=1}^{T-1} \hat{\Delta}(\pi_t, \pi_{t-1})
\]

Where:

- \(\pi_t\) is the policy learned after batch `t`
- Evaluation is done using the *unseen* remaining data

## The `cram_policy()` Function

The `cram_policy()` function in **cramR** implements the CRAM framework for binary treatment policy learning.

### 🔑 Key Features of `cram_policy()`

- **Model-Agnostic Flexibility**: Supports a variety of learning strategies, including `causal_forest`, `s_learner`, and `m_learner`, as well as fully customizable learners via user-defined fit and predict functions.

- **Efficient by Design**: Built on top of `data.table` for fast, memory-efficient computation, with optional support for parallel batch training to scale across larger datasets.
 

## Example: Running CRAM Policy on simulated data

```{r}
library(data.table)
# Function to generate sample data with heterogeneous treatment effects:
# - Positive effect group
# - Neutral effect group
# - Adverse effect group
generate_data <- function(n) {
  X <- data.table(
    binary = rbinom(n, 1, 0.5),                 # Binary variable
    discrete = sample(1:5, n, replace = TRUE),  # Discrete variable
    continuous = rnorm(n)                       # Continuous variable
  )

  # Binary treatment assignment (50% treated)
  D <- rbinom(n, 1, 0.5)

  # Define heterogeneous treatment effects based on X
  treatment_effect <- ifelse(
    X[, binary] == 1 & X[, discrete] <= 2,        # Group 1: Positive effect
    1,
    ifelse(X[, binary] == 0 & X[, discrete] >= 4, # Group 3: Adverse effect
           -1,
           0.1)                                   # Group 2: Neutral effect
  )

  # Outcome depends on treatment effect + noise
  Y <- D * (treatment_effect + rnorm(n, mean = 0, sd = 1)) +
    (1 - D) * rnorm(n)

  return(list(X = X, D = D, Y = Y))
}

# Generate a sample dataset
set.seed(123)
n <- 1000
data <- generate_data(n)
X <- data$X
D <- data$D
Y <- data$Y

```


```{r}
# Number of batches to split the data into
batch <- 20  

# Model type for estimating treatment effects
# Options: "causal_forest", "s_learner", "m_learner"
model_type <- "causal_forest"  

# Learner type used inside s/m-learners
# NULL is required for causal_forest; use "ridge" or "fnn" for s/m learners
learner_type <- NULL  

# Baseline policy to compare against (list of 0/1 for each individual)
# Common options:
# - All-control baseline: as.list(rep(0, nrow(X)))
# - Randomized baseline: as.list(sample(c(0, 1), nrow(X), replace = TRUE))
baseline_policy <- as.list(rep(0, nrow(X)))  

# Whether to parallelize across batches (TRUE for faster but memory-heavy runs)
parallelize_batch <- FALSE  

# Model-specific parameters
# Examples:
# - causal_forest: list(num.trees = 100)
# - ridge: list(alpha = 1)
# - fnn (Feedforward Neural Network): see below
model_params <- NULL  

# Advanced example for fnn (not used here):
# default_model_params <- list(
#   input_layer = list(units = 64, activation = 'relu', input_shape = input_shape),
#   layers = list(list(units = 32, activation = 'relu')),
#   output_layer = list(units = 1, activation = 'linear'),
#   compile_args = list(optimizer = 'adam', loss = 'mse'),
#   fit_params = list(epochs = 5, batch_size = 32, verbose = 0)
# )

# Significance level for confidence intervals (default = 95%)
alpha <- 0.05  

# Run the CRAM policy method
result <- cram_policy(
  X, D, Y,
  batch = batch,
  model_type = model_type,
  learner_type = learner_type,
  baseline_policy = baseline_policy,
  parallelize_batch = parallelize_batch,
  model_params = model_params,
  alpha = alpha
)

# Display the results
print(result)

```

## Interpreting Results

```{r}
result$raw_results
```

```{r}
result$interactive_table
```


The output of `cram_policy()` includes:

- `raw_results`: A data frame summarizing key metrics:
  - `Delta Estimate`, `Delta Standard Error`, and corresponding confidence interval bounds  
  - `Policy Value Estimate`, `Policy Value Standard Error`, and confidence interval bounds  
  - `Proportion Treated` under the final policy

- `interactive_table`: An interactive table view of the same results for exploration

- `final_policy_model`: The final fitted policy model based on the specified `model_type`, `learner_type`, or `custom_fit`


```{r}
class(result$final_policy_model)
summary(result$final_policy_model)

```

You can inspect or apply the learned model to new data.

## Visual Summary and Notes

![ ](cram_visual_2.png)


This visualization summarizes how multiple evaluations across iterations contribute to the full CRAM estimate.

Notes:

- **Batching**: You can pass a number (e.g., `batch = 5`) or a custom vector to control how data is split.  
- **Parallelization**: Enable with `parallelize_batch = TRUE`.  
- **Custom Learners**: Use `custom_fit` and `custom_predict` to plug in any estimator.  

## Why CRAM?

Compared to classic evaluation methods:

| Method            | Learning | Evaluation        | Data Use         |
|-------------------|----------|-------------------|------------------|
| Sample Splitting  | Partial  | Partial           | ❌ Inefficient    |
| Cross-Validation  | Partial  | Average performance over different models | ⚠️ Partial for each model  |
| **CRAM**          | ✅ Full  | ✅ Final model evaluated directly | ✅ Fully reused for both learning and evaluation |

In practice, CRAM:

- Reduces variance of policy evaluation  
- Produces tighter confidence intervals  
- Accommodates online learners and large-scale models  

## References

- Jia, Z., Imai, K., & Li, M. L. (2024). *The CRAM Method for Efficient Simultaneous Learning and Evaluation*. arXiv:2403.07031.  
- Wager & Athey (2018). *Causal Forests*.  
- Athey & Imbens (2016). *S/M-learner framework*.  

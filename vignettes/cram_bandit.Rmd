---
title: "Using cram_bandit() for On-Policy Evaluation in Contextual Bandits"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using cram_bandit() for On-Policy Evaluation in Contextual Bandits}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(cramR)
library(DT)
```

## What is `cram_bandit()`?

The `cram_bandit()` function performs **on-policy statistical evaluation** of contextual bandit policies using the **CRAM method**.

Given:
- `pi`: A 3D array where each entry `pi[j, t, a]` is the probability of choosing arm `a` at time `t` for context `j`
- `arm`: The vector of actions chosen at each timestep
- `reward`: The observed rewards
- `alpha`: Confidence level for inference

It returns:
- A point estimate of the policy value  
- Standard error and confidence interval  
- Results in both tabular and interactive format

---

## Example: Simulated Contextual Bandit Evaluation

```{r}
# Set random seed for reproducibility
set.seed(42)

# Define parameters
T <- 100  # Number of timesteps
K <- 4    # Number of arms

# Simulate a 3D array `pi` of shape (T, T, K)
# - First dimension: Individuals (context Xj)
# - Second dimension: Time steps (pi_t)
# - Third dimension: Arms (depth)
pi <- array(runif(T * T * K, 0.1, 1), dim = c(T, T, K))

# Normalize probabilities so that each row sums to 1 across arms
for (t in 1:T) {
  for (j in 1:T) {
    pi[j, t, ] <- pi[j, t, ] / sum(pi[j, t, ])  
  }
}

# Simulate arm selections (randomly choosing an arm)
arm <- sample(1:K, T, replace = TRUE)

# Simulate rewards (assume normally distributed rewards)
reward <- rnorm(T, mean = 1, sd = 0.5)
```

---

### Run the CRAM Bandit method

```{r}
result <- cram_bandit(pi, arm, reward)
```

---

## Results

### Summary Table

```{r}
result$raw_results
```

### Interactive Table

```{r}
result$interactive_table
```

---

## Notes

- The array `pi` should have shape **(T × T × K)**, with each slice along the third dimension representing the probability distribution over arms at a given time.
- The method assumes the policy used to collect the data is **the same** as the one being evaluated (*on-policy*).
- Confidence intervals are based on asymptotic theory using influence functions.

---

## References

This method builds on:

- On-policy evaluation in contextual bandits  
- Influence-function-based variance estimation  

See also:

- `DT::datatable()` for interactive result viewing

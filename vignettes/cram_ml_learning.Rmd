---
title: "Using ml_learning() for Batch-wise Machine Learning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using ml_learning() for Batch-wise Machine Learning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(cramR)
library(caret)
library(data.table)
```

## üîç What is `ml_learning()`?

The `ml_learning()` function implements **batch-wise machine learning** for both supervised and unsupervised tasks. It supports:

- Any `caret::train()` compatible model (regression, classification, clustering, etc.)
- Custom model training + prediction via `custom_fit`, `custom_predict`
- Custom or built-in loss functions (e.g., MSE, logloss, accuracy)
- Optional parallel processing (`parallelize_batch = TRUE`)

It‚Äôs a flexible utility that powers `cram_ml()` for general-purpose ML workflows, but is also useful on its own when customizing training pipelines.

---

## üß† When to use `ml_learning()`?

- You want to **evaluate a model‚Äôs performance** over multiple growing batches of data
- You want to track **loss evolution over time**
- You need **parallelized learning**, e.g., for neural nets or larger datasets
- You‚Äôre experimenting with **custom ML pipelines**, loss functions, or unsupervised tasks

---

## ‚öôÔ∏è Inputs

| Argument         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| `data`           | A data frame or matrix with predictors (and optionally a target column)    |
| `formula`        | Supervised learning formula (e.g., `Y ~ .`). Use `NULL` for unsupervised.  |
| `batch`          | Integer (number of batches) or custom vector of batch assignments          |
| `loss_name`      | Built-in loss type: `"mse"`, `"accuracy"`, `"logloss"`, etc.               |
| `caret_params`   | Model settings for `caret::train()` (must include `method`)                |
| `custom_fit`     | Optional custom training function                                           |
| `custom_predict` | Optional custom prediction function                                         |
| `custom_loss`    | Optional custom loss function (if not using `loss_name`)                   |
| `parallelize_batch` | Run batches in parallel (default `FALSE`)                               |
| `n_cores`        | Number of cores to use if parallelized                                     |

---

## üìò Example: Linear Regression with MSE

```{r}
# Simulate regression data
set.seed(42)
X_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100))
Y_data <- rnorm(100)
data_df <- data.frame(X_data, Y = Y_data)
```

```{r}
# Define model settings
caret_params_lm <- list(
  method = "lm",
  trControl = trainControl(method = "none")  # No cross-validation
)

# Define batch count
nb_batch <- 5
```

```{r}
# Run ML learning
result_lm <- ml_learning(
  data = data_df,
  formula = Y ~ .,
  batch = nb_batch,
  loss_name = "mse",
  caret_params = caret_params_lm
)
```

---

## üì¶ Output Structure

```{r}
str(result_lm)
```

The returned object is a **list** with:

| Element            | Description                                                             |
|--------------------|-------------------------------------------------------------------------|
| `final_ml_model`    | Trained model from the final batch                                     |
| `losses`            | Matrix with instance-wise losses for each batch (first column = 0s)    |
| `batch_indices`     | List of indices used in each cumulative batch                          |

---

## üîß Custom Models (Optional)

You can plug in your own models:

```r
custom_fit <- function(data) {
  model <- glm(Y ~ ., data = data)
  return(model)
}

custom_predict <- function(model, data) {
  return(predict(model, newdata = data))
}

custom_loss <- function(preds, data) {
  return((data$Y - preds)^2)
}
```

And then run:

```r
ml_learning(data_df, batch = nb_batch, custom_fit = custom_fit,
            custom_predict = custom_predict, custom_loss = custom_loss)
```

---

## ‚ö° Parallelization

To accelerate training, set:

```r
ml_learning(data_df,
            formula = Y ~ .,
            batch = nb_batch,
            caret_params = caret_params_lm,
            loss_name = "mse",
            parallelize_batch = TRUE,
            n_cores = 4)
```

---

## üìö See Also

- [`cram_ml()`](./cram_ml.html) ‚Äî wrapper for full pipeline (estimation + CI)
- [`cram_policy()`](./cram_policy.html) ‚Äî for causal policy learning
- [`caret::train()`](https://topepo.github.io/caret/train-models-by-tag.html) ‚Äî underlying training API
